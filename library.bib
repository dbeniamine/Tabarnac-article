Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Drongowski2008,
author = {Drongowski, Paul J.},
file = {:Users/mdiener/Downloads/Introduction\_to\_CodeAnalyst.pdf:pdf},
keywords = {Performance analysis,code analyst,codeanalyst,profiling},
mendeley-tags = {code analyst,codeanalyst},
title = {{An introduction to analysis and optimization with AMD CodeAnalyst™ Performance Analyzer}},
year = {2008}
}
@inproceedings{Cruz2014a,
author = {Cruz, Eduardo H.M. and Diener, Matthias and Alves, Marco A.Z. and Pilla, La\'{e}rcio L. and Navaux, Philippe O.A.},
booktitle = {International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
doi = {10.1109/SBAC-PAD.2014.22},
file = {:Users/mdiener/Dropbox/Papers/2014-SBAC-LAPT.pdf:pdf},
isbn = {978-1-4799-6905-0},
month = oct,
pages = {198--205},
title = {{Optimizing Memory Locality Using a Locality-Aware Page Table}},
year = {2014}
}
@techreport{MessagePassingInterfaceForum2012,
author = {{Message Passing Interface Forum}},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Message Passing Interface Forum - 2012 - MPI A Message-Passing Interface Standard.pdf:pdf},
title = {{MPI: A Message-Passing Interface Standard}},
year = {2012}
}
@inproceedings{Kale1993,
author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
booktitle = {Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kale, Krishnan - 1993 - CHARM A Portable Concurrent Object Oriented System Based On C.pdf:pdf},
pages = {91--108},
title = {{CHARM++: A Portable Concurrent Object Oriented System Based On C++}},
year = {1993}
}
@inproceedings{Sonnek2010,
abstract = {Virtualization is being widely used in large-scale computing environments, such as clouds, data centers, and grids, to provide application portability and facilitate resource multiplexing while retaining application isolation. In many existing virtualized platforms, it has been found that the network bandwidth often becomes the bottleneck resource, causing both high network contention and reduced performance for communication and data-intensive applications. In this paper, we present a decentralized affinity-aware migration technique that incorporates heterogeneity and dynamism in network topology and job communication patterns to allocate virtual machines on the available physical resources. Our technique monitors network affinity between pairs of VMs and uses a distributed bartering algorithm, coupled with migration, to dynamically adjust VM placement such that communication overhead is minimized. Our experimental results running the Intel MPI benchmark and a scientific application on a 7-node Xen cluster show that we can get up to 42\% improvement in the runtime of the application over a no-migration technique, while achieving up to 85\% reduction in network communication cost. In addition, our technique is able to adjust to dynamic variations in communication patterns and provides both good performance and low network contention with minimal overhead.},
author = {Sonnek, Jason and Greensky, James and Reutiman, Robert and Chandra, Abhishek},
booktitle = {International Conference on Parallel Processing (ICPP)},
doi = {10.1109/ICPP.2010.30},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Sonnek et al. - 2010 - Starling Minimizing Communication Overhead in Virtualized Computing Platforms Using Decentralized Affinity-Aware.pdf:pdf},
isbn = {978-1-4244-7913-9},
month = sep,
pages = {228--237},
title = {{Starling: Minimizing Communication Overhead in Virtualized Computing Platforms Using Decentralized Affinity-Aware Migration}},
year = {2010}
}
@article{Alameldeen2003,
abstract = {Commercial workloads are an important class of applications for multiprocessor servers, and their simulation is essential for computer architects to evaluate future server designs. However, simulating expensive servers running these large workloads on low-cost personal computers presents many challenges. The workloads must be scaled down and tuned to fit within our simulation environment. Simulation time must be made tractable, since simulations are much slower than native machine execution. Simulators should model a sufficient level of timing detail to allow eval- uating research ideas. We overcame these challenges by developing workloads and methods suitable for commercial workload simulation. TheWisconsin CommercialWorkload Suite currently includes four benchmarks that represent important commercial applications, which can be used for such purposes.We illustrate how commercial workloads can motivate and be used to evaluate innovations in server design.},
author = {a.R. Alameldeen and Martin, M.M.K. M K and Mauer, C.J. J and Moore, K.E. E and Hill, M.D. D and Wood, D.a. and Sorin, D.J. J and a.R. Alameldeen},
doi = {10.1109/MC.2003.1178046},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Alameldeen et al. - 2003 - Simulating a \$2M commercial server on a \$2K PC.pdf:pdf},
issn = {0018-9162},
journal = {IEEE Computer},
keywords = {cache coherence,commercial workloads,execution-driven simulation,full-system simulation,memory systems,multiprocessors,performance evaluation,simulation,variability},
month = feb,
number = {2},
pages = {50--57},
title = {{Simulating a \$2M commercial server on a \$2K PC}},
volume = {36},
year = {2003}
}
@inproceedings{Mavinakayanahalli2006,
abstract = {Kernel Probes (kprobes) can insert probes into a running kernel for purposes of debugging, tracing, performance evaluation, fault injection, etc. A user-defined handler is run when a probepoint is hit. From the barebones im- plementation in Linux 2.6.9, kprobes has un- dergone a number of improvements—support for colocated probes, function-return probes, reentrant probes, and the like. Handlers are now executed without any locks held, lead- ing to lower overhead compared to the earlier “single spinlock serialization” method. Other enhancements are on the anvil—the kprobe “booster” series, userspace probes and watch- point probes, to name a few. This paper will trace the developments in kprobes and also touch upon the current state of the aforemen- tioned enhancements.},
author = {Mavinakayanahalli, Ananth and Panchamukhi, Prasanna and Keniston, Jim and Keshavamurthy, Anil and Hiramatsu, Masami},
booktitle = {Linux Symposium},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mavinakayanahalli et al. - 2006 - Probing the Guts of Kprobes.pdf:pdf},
pages = {101--116},
title = {{Probing the Guts of Kprobes}},
year = {2006}
}
@article{Gropp1999,
abstract = {In this paper we describe the difficulties inherent in making accurate, reproducible measurements of message-passing performance. We describe some of the mistakes often made in attempting such mea- surements and the consequences of such mistakes.We describe mpptest, a suite of performance measurement programs developed at Argonne National Laboratory, that attempts to avoid such mistakes and obtain reproducible measures of MPI performance that can be useful to both MPI implementors and MPI application writers. We include a number of illustrative examples of its use.},
author = {Gropp, William},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Gropp - 1999 - Reproducible measurements of MPI performance characteristics.pdf:pdf},
journal = {Recent Advances in Parallel Virtual Machine and},
keywords = {mpi,mpptest},
mendeley-tags = {mpi,mpptest},
title = {{Reproducible measurements of MPI performance characteristics}},
url = {http://www.springerlink.com/index/PE2J40W56L9GM1Y9.pdf},
year = {1999}
}
@inproceedings{Li2005,
abstract = {Multimedia applications are becoming increasingly important for a large class of general-purpose processors. Contemporary media applications are highly complex and demand high performance. A distinctive feature of these applications is that they have significant parallelism, including thread- , data-, and instruction-level parallelism, that is potentially well-aligned with the increasing parallelism supported by emerging multi-core architectures. Designing systems to meet the demands of these applications therefore requires a benchmark suite comprising these complex applications and that exposes the parallelism present in them. This paper makes two contributions. First, it presents ALPBench, a publicly available benchmark suite that pulls together five complex media applications from various sources: speech recognition (CMU Sphinx 3), face recognition (CSU), ray tracing (Tachyon), MPEG-2 encode (MSSG), and MPEG-2 decode (MSSG). We have modified the original applications to expose thread-level and data-level parallelism using POSIX threads and sub-word SIMD (Inters SSE2) instructions respectively. Second, the paper provides a performance characterization of the ALPBench benchmarks, with a focus on parallelism. Such a characterization is useful for architects and compiler writers for designing systems and compiler optimizations for these applications.},
author = {Li, ML and Sasanka, R and Adve, SV},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Li, Sasanka, Adve - 2005 - The ALPBench benchmark suite for complex multimedia applications.pdf:pdf},
keywords = {alpbench},
mendeley-tags = {alpbench},
number = {October},
pages = {34--45},
title = {{The ALPBench benchmark suite for complex multimedia applications}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1525999},
year = {2005}
}
@inproceedings{Subramoni2012,
author = {Subramoni, H. and Potluri, S. and Kandalla, K. and Barth, B. and Vienne, J. and Keasler, J. and Tomko, K. and Schulz, K. and Moody, A. and Panda, D. K.},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
doi = {10.1109/SC.2012.47},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Subramoni et al. - 2012 - Design of a scalable InfiniBand topology service to enable network-topology-aware placement of processes.pdf:pdf},
isbn = {978-1-4673-0806-9},
keywords = {mpi},
mendeley-tags = {mpi},
month = nov,
title = {{Design of a scalable InfiniBand topology service to enable network-topology-aware placement of processes}},
year = {2012}
}
@inproceedings{Woo1995,
abstract = {The SPLASH-2 suite of parallel applications has recently been released to facilitate the study of centralized and distributed shared- address-space multiprocessors. In this context, this paper has two goals. One is to quantitatively characterize the SPLASH-2 programs in terms of fundamental properties and architectural interactions that are important to understand them well. The properties we study in- clude the computational load balance, communication to computa- tion ratio and traffic needs, important working set sizes, and issues related to spatial locality, as well as how these properties scale with problem size and the number of processors. The other, related goal is methodological: to assist people who will use the programs in ar- chitectural evaluations to prune the space of application and ma- chine parameters in an informed and meaningful way. For example, by characterizing the working sets of the applications, we describe which operating points in terms of cache size and problem size are representative of realistic situations, which are not, and which are re- dundant. Using SPLASH-2 as an example, we hope to convey the importance of understanding the interplay of problem size, number of processors, and working sets in designing experiments and inter- preting their results.},
author = {Woo, S.C. and Ohara, M. and Torrie, E. and Singh, J.P. and Gupta, A.},
booktitle = {International Symposium on Computer Architecture (ISCA)},
doi = {10.1109/ISCA.1995.524546},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Woo et al. - 1995 - The SPLASH-2 programs characterization and methodological considerations.pdf:pdf},
isbn = {0-89791-698-0},
number = {June},
pages = {24--36},
publisher = {Acm},
title = {{The SPLASH-2 programs: characterization and methodological considerations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=524546},
year = {1995}
}
@inproceedings{Morari2012,
author = {Morari, Alessandro and Gioiosa, Roberto and Wisniewski, Robert W. and Rosenburg, Bryan S. and Inglett, Todd a. and Valero, Mateo},
booktitle = {IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2012.94},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Morari et al. - 2012 - Evaluating the Impact of TLB Misses on Future HPC Systems.pdf:pdf},
isbn = {978-1-4673-0975-2},
keywords = {-exascale,bg,cnk,hpc,os noise,p,tlb miss},
month = may,
pages = {1010--1021},
title = {{Evaluating the Impact of TLB Misses on Future HPC Systems}},
year = {2012}
}
@inproceedings{Mucci1999,
author = {Mucci, P.J. and Browne, Shirley and Deane, Christine and Ho, George},
booktitle = {Proc. Dept. of Defense HPCMP Users Group Conference},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mucci et al. - 1999 - PAPI A portable interface to hardware performance counters.pdf:pdf},
number = {5},
pages = {7--10},
title = {{PAPI: A portable interface to hardware performance counters}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.6801\&rep=rep1\&type=pdf},
volume = {32},
year = {1999}
}
@inproceedings{Broquedis2010,
abstract = {The increasing numbers of cores, shared caches and memory nodes within machines introduces a complex hardware topology. High-performance computing applications now have to carefully adapt their placement and behavior according to the underlying hierarchy of hardware resources and their software affinities. We introduce the Hardware Locality (hwloc) software which gathers hardware information about processors, caches, memory nodes and more, and exposes it to applications and runtime systems in a abstracted and portable hierarchical manner. hwloc may significantly help performance by having runtime systems place their tasks or adapt their communication strategies de- pending on hardware affinities. We show that hwloc can already be used by popular high- performance OPENMP or MPI software. Indeed, scheduling OPENMP threads according to their affinities or placing MPI processes according to their communication patterns shows inter- esting performance improvement thanks to hwloc. An optimized MPI communication strategy may also be dynamically chosen according to the location of the communicating processes in the machine and its hardware characteristics.},
author = {Broquedis, Franois and Clet-Ortega, Jerome and Moreaud, Stephanie and Furmento, Nathalie and Goglin, Brice and Mercier, Guillaume and Thibault, Samuel and Namyst, Raymond},
booktitle = {Euromicro Conference on Parallel, Distributed and Network-based Processing (PDP)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Broquedis et al. - 2010 - hwloc A Generic Framework for Managing Hardware Affinities in HPC Applications.pdf:pdf},
pages = {180--186},
title = {{hwloc: A Generic Framework for Managing Hardware Affinities in HPC Applications}},
year = {2010}
}
@inproceedings{Li2010a,
author = {Li, Dong and de Supinski, Bronis R and Schulz, Martin and Cameron, Kirk and Nikolopoulos, Dimitrios S.},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2010.5470463},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2010 - Hybrid MPIOpenMP Power-Aware Computing.pdf:pdf},
isbn = {978-1-4244-6442-5},
keywords = {mpi,openmp,performance modeling,power-aware high-performance computing},
title = {{Hybrid MPI/OpenMP Power-Aware Computing}},
year = {2010}
}
@article{Murphy,
abstract = {This paper compares the system performance evaluation cooperative (SPEC) Integer and Floating-Point suites to a set of real-world applications for high-performance computing at Sandia National Laboratories. These applications focus on the high-end scientific and engineering domains; however, the techniques presented in this paper are applicable to any application domain. The applications are compared in terms of three memory properties: 1) temporal locality (or reuse over time), 2) spatial locality (or the use of data "near" data that has already been accessed), and 3) data intensiveness (or the number of unique bytes the application accesses). The results show that real-world applications exhibit significantly less spatial locality, often exhibit less temporal locality, and have much larger data sets than the SPEC benchmark suite. They further quantitatively demonstrate the memory properties of real supercomputing applications.},
author = {Murphy, Richard C. and Kogge, Peter M.},
doi = {http://doi.ieeecomputersociety.org/10.1109/TC.2007.1039},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Murphy, Kogge - 2007 - On The Memory Access Patterns of Supercomputer Applications Benchmark Selection and Its Implications.pdf:pdf},
journal = {IEEE Transactions on Computers},
number = {7},
pages = {937--945},
title = {{On The Memory Access Patterns of Supercomputer Applications: Benchmark Selection and Its Implications}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.5233\&rep=rep1\&type=pdf},
volume = {56},
year = {2007}
}
@techreport{Bailey1995,
author = {Bailey, David and Harris, Tim and Saphir, William and {Van Der Wijngaart}, R. and Woo, Alex and Yarrow, Maurice},
booktitle = {Contract},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bailey et al. - 1995 - The NAS parallel benchmarks 2.0.pdf:pdf},
institution = {Technical Report NAS-95-020, NASA Ames Research Center},
keywords = {nas},
mendeley-tags = {nas},
pages = {1--24},
title = {{The NAS parallel benchmarks 2.0}},
url = {http://www.nas.nasa.gov/assets/pdf/techreports/1995/nas-95-020.pdf},
year = {1995}
}
@article{Zhou2012,
abstract = {Como los procesadores de n\'{u}cleo cuenta evolucionan hacia mayores , arquitectos desarrollar\'{a}n sistemas de memoria m\'{a}s sofisticados para satisfacer el aumento de la sed de los n\'{u}cleos ' de ancho de banda de memoria . Dise\~{n}os temprana de m\'{u}ltiples n\'{u}cleos Processor sugieren que los sistemas de memoria futuras probablemente in-cluyen varios controladores y coherencia de cach\'{e} protocolos distribuidos. Procesadores de m\'{u}ltiples n\'{u}cleos que exponen las pol\'{\i}ticas de localidad de memoria para el sistema de software proporcionan oportunidades para la sintonizaci\'{o}n autom\'{a}tica que pueden lograr importantes beneficios de rendimiento. Lenguajes administrados normalmente proporcionan una sencilla pila de abstracci\'{o}n ci\'{o}n . Este art\'{\i}culo presenta las t\'{e}cnicas que tienden un puente entre la abstracci\'{o}n simple mont\'{o}n de lenguas modernas y los sistemas de memoria complicados de procesadores futuros . Presentamos un enfoque de NUMA para la recolecci\'{o}n de basura que equilibra los competidores preocupaciones de localidad de datos y la utilizaci\'{o}n del almacenamiento din\'{a}mico para mejorar el rendimiento . Combinamos un enfoque ligero formeasuring comportamiento de la memoria de una aplicaci\'{o}n con un algoritmo en l\'{\i}nea, de adaptaci\'{o}n para el ajuste de la memoria cach\'{e} para optimizarlo para los comportamientos de la aplicaci\'{o}n espec\'{\i}fica. Hemos implementado nuestro recolector de basura y el algoritmo de sintonizaci\'{o}n cach\'{e} y presentar los resultados en un procesador de 64 n\'{u}cleos TILEPro64 .},
author = {Zhou, Jin and Demsky, Brian},
doi = {10.1145/2426642.2259000},
institution = {ACM},
isbn = {978-1-4503-1350-6},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
number = {11},
pages = {3--14},
title = {{Memory management for many-core processors with software configurable locality policies}},
volume = {47},
year = {2012}
}
@inproceedings{Gropp2002,
abstract = {This talk will describe MPICH2, an all-new implementa- tion of MPI designed to support both MPI-1 and MPI-2 and to en- able further research into MPI implementation technology. To achieve high-performance and scalability and to encourage experimentation, the design of MPICH2 is strongly modular. For example, the MPI topol- ogy routines can easily be replaced by implementations tuned to a spe- cific environment, such as a geographically dispersed computational grid. The interface to the communication layers has been designed to exploit modern interconnects that are capable of remote memory access but can be implemented on older networks. An initial, TCP-based imple- mentation, will be described in detail, illustrating the use of a simple, communication-channel interface. A multi-method device that provides TCP, VIA, and shared memory communication will also be discussed. Performance results for point-to-point and collective communication will be presented. These illustrate the advantages of the new design: the point-to-point TCP performance is close to the raw achievable latency and bandwidth, and the collective routines are significantly faster than the “classic” MPICH versions (more than a factor of two in some cases). Performance issues that arise in supporting MPI THREAD MULTIPLE will be discussed, and the role of a proper choice of implementation abstrac- tion in achieving low-overhead will be illustrated with results from the MPICH2 implementation. Scalability to tens or hundreds of thousands of processors is another goal of the MPICH2 design. This talk will describe some of the features of MPICH2 that address scalability issues and current research targeting a system with 64K processing elements.},
author = {Gropp, William},
booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Gropp - 2002 - MPICH2 A new start for MPI implementations.pdf:pdf},
title = {{MPICH2: A new start for MPI implementations}},
year = {2002}
}
@article{Diener2015a,
author = {Diener, Matthias and Cruz, Eduardo H M and Navaux, Philippe O A and Busse, Anselm and Hei\ss, Hans-ulrich},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener et al. - 2015 - Communication-Aware Process and Thread Mapping Using Online Communication Detection.pdf:pdf},
journal = {Parallel Computing},
keywords = {communication optimization,mapping,parallel applications,shared memory},
number = {March},
pages = {43--63},
title = {{Communication-Aware Process and Thread Mapping Using Online Communication Detection}},
volume = {43},
year = {2015}
}
@inproceedings{Diener2015,
author = {Diener, Matthias and Cruz, Eduardo H M and Navaux, Philippe O A},
booktitle = {International Conference on Parallel, Distributed, and Network-Based Processing (PDP)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener, Cruz, Navaux - 2015 - Locality vs . Balance Exploring Data Mapping Policies on NUMA Systems.pdf:pdf},
keywords = {data mapping,load balance,locality,numa},
pages = {9--16},
title = {{Locality vs . Balance: Exploring Data Mapping Policies on NUMA Systems}},
year = {2015}
}
@misc{Verschelde2014,
annote = {- lots of OpenMP/MPI},
author = {Verschelde, Jan},
title = {{MCS 572: Introduction to Supercomputing}},
url = {http://homepages.math.uic.edu/~jan/mcs572/index.html},
year = {2014}
}
@techreport{Woodacre2005,
abstract = {This paper describes the global shared- memory architecture and benefits of the SGI Altix 3000 family of servers and superclusters. Memory size and scaling, cache organization and coherency, nodes and internode memory access, bandwidths and latencies, and RAS features are all discussed. The paper is written from a perspective that may be useful to an applications developer or a system administra- tor. It describes the current implementation of the Altix 3000 system communication infra- structure.},
author = {Woodacre, Michael and Robb, Derek and Roe, Dean and Feind, Karl},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Woodacre et al. - 2005 - The SGI Altix 3000 Global Shared-Memory Architecture.pdf:pdf},
title = {{The SGI Altix 3000 Global Shared-Memory Architecture}},
year = {2005}
}
@inproceedings{Terboven2008,
abstract = {The slogan of last year’s International Workshop on OpenMP was ”A Practical Programming Model for the Multi-Core Era”, although OpenMP still is fully hardware architecture agnostic. As a consequence the programmer is left alone with bad performance if threads and data happen to live apart. In this work we examine the programmer’s possibil- ities to improve data and thread affinity in OpenMP pro- grams for several toy applications and present how to apply the lessons learned on larger application codes. We filled a gap by implementing explicit data migration on Linux pro- viding a next touch mechanism.},
author = {Terboven, Christian and an Mey, Dieter and Schmidl, Dirk and Jin, Henry and Reichstein, Thomas},
booktitle = {Workshop on Memory Access on Future Processors: A Solved Problem? (MAW)},
doi = {10.1145/1366219.1366222},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Terboven et al. - 2008 - Data and Thread Affinity in OpenMP Programs.pdf:pdf},
isbn = {9781605580913},
keywords = {affinity,binding,ccnuma,migration,openmp},
pages = {377--384},
title = {{Data and Thread Affinity in OpenMP Programs}},
year = {2008}
}
@article{Loh2009,
author = {Loh, Gabriel H},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Loh - 2009 - Fundamental Latency Trade-offs in Architecting DRAM Caches.pdf:pdf},
number = {Section 5},
title = {{Fundamental Latency Trade-offs in Architecting DRAM Caches}},
year = {2009}
}
@inproceedings{Agarwal2006,
author = {Agarwal, Tarun and Sharma, Amit and Kal\'{e}, Laxmikant V.},
booktitle = {International Parallel and Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Agarwal, Sharma, Kal\'{e} - 2006 - Topology-aware task mapping for reducing communication contention on large parallel machines.pdf:pdf},
isbn = {1424400546},
keywords = {mpi},
mendeley-tags = {mpi},
title = {{Topology-aware task mapping for reducing communication contention on large parallel machines}},
year = {2006}
}
@misc{Petitet2012,
author = {Petitet, Antoine and Whaley, R. Clint and Dongarra, Jack and Cleary, Andy},
title = {{HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers}},
url = {http://www.netlib.org/benchmark/hpl/},
year = {2012}
}
@inproceedings{Diener2013,
abstract = {In current shared memory architectures, the com- plexity of the cache and memory hierarchies is increasing. Therefore, it is becoming more important to analyze the communication behavior of parallel applications when mapping threads to cores, to improve performance and energy efficiency. However, communication is implicit in most programming models for shared memory, which makes it difficult to detect the communication pattern between the threads in an accurate and low-overhead way. We propose a new mechanism to detect the communication pattern of shared memory applications by monitoring page table accesses. Combining this mechanism with a dynamic mi- gration algorithm allows mapping to be performed dynamically by the operating system. We implemented our mechanism in the Linux kernel and performed experiments with applications from the NAS Parallel Benchmarks. Results show a reduction of up to 16.7\% of the execution time and 63\% of the cache misses, compared to the original scheduler of the operating system. Furthermore, we decrease total processor and DRAM energy consumption by up to 14.7\% and 28.5\%, respectively.},
author = {Diener, Matthias and Cruz, Eduardo H. M. and Navaux, Philippe O. A.},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2013.57},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener, Cruz, Navaux - 2013 - Communication-Based Mapping Using Shared Pages.pdf:pdf},
keywords = {communication detection,mapping,page table,shared pages},
pages = {700--711},
title = {{Communication-Based Mapping Using Shared Pages}},
year = {2013}
}
@inproceedings{Bai2010,
address = {New York, New York, USA},
author = {Bai, Yuebin and Xu, Cong and Li, Zhi},
booktitle = {ACM Symposium on Applied Computing (SAC)},
doi = {10.1145/1774088.1774126},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bai, Xu, Li - 2010 - Task-aware based co-scheduling for virtual machine system.pdf:pdf},
isbn = {9781605586397},
keywords = {Task-Aware,VMM,Vcpu Scheduling,Xen},
pages = {181},
publisher = {ACM Press},
title = {{Task-aware based co-scheduling for virtual machine system}},
url = {http://portal.acm.org/citation.cfm?doid=1774088.1774126},
year = {2010}
}
@inproceedings{Kim1997,
author = {Kim, JS and Lilja, DJ},
booktitle = {International Workshop on Network-Based Parallel Computing: Communication, Architecture, and Applications},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Lilja - 1998 - Characterization of communication patterns in message-passing parallel scientific application programs.pdf:pdf},
title = {{Characterization of communication patterns in message-passing parallel scientific application programs}},
year = {1998}
}
@inproceedings{Amato2000,
author = {Amato, N.M. and Perdue, J. and Pietracaprina, A. and Pucci, G. and Mathis, M.},
booktitle = {International Parallel and Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2000.846058},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Amato et al. - 2000 - Predicting performance on SMPs. A case study the SGI Power Challenge.pdf:pdf},
isbn = {0-7695-0574-0},
pages = {729--737},
publisher = {IEEE Computer Society},
title = {{Predicting performance on SMPs. A case study: the SGI Power Challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=846058},
year = {2000}
}
@article{Magnusson2002,
author = {Magnusson, P.S. and Christensson, M. and Eskilson, J. and Forsgren, D. and Hallberg, G. and Hogberg, J. and Larsson, F. and Moestedt, A. and Werner, B.},
doi = {10.1109/2.982916},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Magnusson et al. - 2002 - Simics A Full System Simulation Platform.pdf:pdf},
issn = {00189162},
journal = {IEEE Computer},
number = {2},
pages = {50--58},
publisher = {IEEE Computer Society},
title = {{Simics: A Full System Simulation Platform}},
volume = {35},
year = {2002}
}
@inproceedings{Singh1994,
author = {Singh, Jaswinder Pal and Rothberg, Edward and Gupta, Anoop},
booktitle = {ACM Symposium on Parallel Algorithms and Architectures},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Rothberg, Gupta - 1994 - Modeling Communication in Parallel Algorithms A Fruitful Interaction between Theory and Systems.pdf:pdf},
title = {{Modeling Communication in Parallel Algorithms: A Fruitful Interaction between Theory and Systems?}},
year = {1994}
}
@article{Brandfass2012,
author = {Brandfass, B. and Alrutz, T. and Gerhold, T.},
doi = {10.1016/j.compfluid.2012.01.019},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Brandfass, Alrutz, Gerhold - 2012 - Rank reordering for MPI communication optimization.pdf:pdf},
issn = {00457930},
journal = {Computers \& Fluids},
month = jan,
pages = {372--380},
publisher = {Elsevier Ltd},
title = {{Rank reordering for MPI communication optimization}},
year = {2012}
}
@article{Bokhari1981,
abstract = {In array processors it is important to map problem modules onto processors such that modules that communicate with each other lie, as far as possible, on adjacent processors. This mapping problem is formulated in graph theoretic terms and shown to be equivalent, in its most general form, to the graph isomorphism problem. The problem is also very similar to the bandwidth reduction problem for sparse matrices and to the quadratic assignment problem.},
author = {Bokhari, SH},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bokhari - 1981 - On the Mapping Problem.pdf:pdf},
journal = {IEEE Transactions on Computers},
number = {3},
pages = {207--214},
title = {{On the Mapping Problem}},
volume = {C-30},
year = {1981}
}
@inproceedings{Che2010,
author = {Che, Shuai and Sheaffer, Jeremy W. and Boyer, Michael and Szafaryn, Lukasz G. and Skadron, Kevin},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2010.5650274},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Che et al. - 2010 - A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads.pdf:pdf},
isbn = {978-1-4244-9297-8},
month = dec,
publisher = {Ieee},
title = {{A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads}},
year = {2010}
}
@inproceedings{Pellegrini1994,
author = {Pellegrini, Fran\c{c}ois},
booktitle = {Scalable High-Performance Computing Conference (SHPCC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pellegrini - 1994 - Static Mapping by Dual Recursive Bipartitioning of Process and Architecture Graphs.pdf:pdf},
keywords = {scotch},
mendeley-tags = {scotch},
pages = {486--493},
title = {{Static Mapping by Dual Recursive Bipartitioning of Process and Architecture Graphs}},
year = {1994}
}
@techreport{Buckner1998,
abstract = {This documen ts the testing of the NASA Ames NAS Parallel Benchmarks on a network of Sun Ultra Workstations. Testing was performed using the CS Department Gemini lab and as many memory sizes and parallel grid configurations were used as w as possible.},
author = {Buckner, Kim},
booktitle = {Test},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Buckner - 1998 - Timings and memory usage for the NAS Parallel Benchmarks on a network of Sun Ultra Workstations.pdf:pdf},
institution = {Technical Report CS-98-408, University of Tennessee},
keywords = {memory usage,nas,sun},
mendeley-tags = {memory usage,nas,sun},
pages = {94--97},
title = {{Timings and memory usage for the NAS Parallel Benchmarks on a network of Sun Ultra Workstations}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Timings+and+memory+usage+for+the+NAS+Parallel+Benchmarks+on+a+network+of+Sun+Ultra+Workstations\#0},
year = {1998}
}
@techreport{Intel2008,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2008 - Quad-Core Intel ® Xeon ® Processor 5400 Series Datasheet.pdf:pdf},
keywords = {harpertown},
mendeley-tags = {harpertown},
number = {March},
title = {{Quad-Core Intel ® Xeon ® Processor 5400 Series Datasheet}},
url = {http://www.intel.com/assets/PDF/datasheet/318589.pdf},
year = {2008}
}
@book{MIPS1996,
author = {MIPS},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/MIPS - 1996 - MIPS R10000 Microprocessor User's Manual, Version 2.0.pdf:pdf},
title = {{MIPS R10000 Microprocessor User's Manual, Version 2.0}},
year = {1996}
}
@phdthesis{Ferreira2012,
author = {Ferreira, Manuela Klanovicz},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ferreira - 2012 - Mapeamento Est\'{a}tico de Processos MPI com Emparelhamento Perfeito de Custo M\'{a}ximo em Cluster Homog\^{e}neo de Multi-core.pdf:pdf},
title = {{Mapeamento Est\'{a}tico de Processos MPI com Emparelhamento Perfeito de Custo M\'{a}ximo em Cluster Homog\^{e}neo de Multi-cores}},
year = {2012}
}
@article{Thitikamol,
abstract = {Networks of workstations are characterized by dynamic resource capacities. Such environments can only be efficiently exploited by applications that are dynamically reconfigurable. The paper explores mechanisms and policies that enable online reconfiguration of shared memory applications through thread migration. We describe the design and preliminary performance of a distributed shared memory (DSM) system that performs online remappings of threads to nodes based on sharing behavior. Our system obtains complete sharing information through a novel correlation tracking phase that avoids the thread thrashing that characterizes previous approaches. This information is used to evaluate the communication required by a given thread mapping and to predict the resulting performance.},
author = {Thitikamol, Kritchalach and Keleher, Pete},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Thitikamol, Keleher - 1999 - Thread Migration and Communication Minimization in DSM Systems.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {dsm,load-balancing,thread migration},
number = {3},
pages = {487--497},
title = {{Thread Migration and Communication Minimization in DSM Systems}},
volume = {87},
year = {1999}
}
@article{Lameter2013,
author = {Lameter, Christoph},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lameter - 2013 - An overview of non-uniform memory access.pdf:pdf},
journal = {Communications of the ACM},
number = {9},
pages = {59},
title = {{An overview of non-uniform memory access}},
volume = {56},
year = {2013}
}
@article{James1990,
abstract = {The scalable coherent interface (SCI), a local or extended computer backplane interface being defined by an IEEE standard project (P1596), is discussed. the interconnection is scalable, meaning that up to 64 K processor, memory, or I/O nodes can effectively interface to a shared SCI interconnection. The SCI sharing-list structures are described, and sharing-list addition and removal are examined. Optimizations being considered to improve the performance of large system configurations are discussed. Request combining, a useful feature of linked-list coherence, is described. SCI's optional extensions, including synchronization using a queued-on-lock bit, are considered},
author = {James, David V and Laundrie, Anthony T and Gjessing, Stein and Sohi, Gurindar S},
doi = {10.1109/CMPEUR.1990.113656},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/James et al. - 1990 - Distributed-Directory Scheme Scalable Coherent Interface.pdf:pdf},
isbn = {0818620412},
issn = {00189162},
journal = {Computer},
number = {6},
pages = {74--77},
publisher = {IEEE Comput. Soc. Press},
title = {{Distributed-Directory Scheme: Scalable Coherent Interface}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=113656},
volume = {23},
year = {1990}
}
@misc{ArgonneNationalLaboratory2014,
abstract = {nemesis is default communication channel in mpich2},
author = {{Argonne National Laboratory}},
title = {{CH3 And Channels}},
url = {https://wiki.mpich.org/mpich/index.php/CH3\_And\_Channels},
year = {2014}
}
@article{Nitzberg1991,
abstract = {An overview of distributed shared memory (DSM) issues is presented. Memory coherence, design choices, and implementation methods are included. The discussion of design choices covers structure and granularity, coherence semantics, scalability, and heterogeneity. Implementation issues concern data location and access, the coherence protocol, replacement strategy, and thrashing. Algorithms that support process synchronization and memory management are discussed.},
author = {Nitzberg, Bill and Lo, Virginia},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Nitzberg, Lo - 1991 - Distributed Shared Memory A Survey of Issues and Algorithms.pdf:pdf},
journal = {IEEE Computer},
number = {8},
pages = {52--60},
title = {{Distributed Shared Memory: A Survey of Issues and Algorithms}},
volume = {24},
year = {1991}
}
@inproceedings{Awasthi2010,
abstract = {Modern processors such as Tilera’s Tile64, Intel’s Nehalem, and AMD’s Opteron are migrating memory controllers (MCs) on-chip, while maintaining a large, flat memory address space. This trend to utilize multiple MCs will likely continue and a core or socket will consequently need to route memory re- quests to the appropriate MC via an inter- or intra-socket ? interconnect fabric similar to AMD’s HyperTransport , or ? Intel’s Quick-Path Interconnect . Such systems are there- fore subject to non-uniform memory access (NUMA) laten- cies because of the time spent traveling to remote MCs. Each MC will act as the gateway to a particular piece of the physical memory. Data placement will therefore become increasingly critical in minimizing memory access latencies. To date, no prior work has examined the effects of data placement among multiple MCs in such systems. Future chip-multiprocessors are likely to comprise multiple MCs and an even larger number of cores. This trend will in- crease the memory access latency variation in these systems. Proper allocation of workload data to the appropriate MC will be important in reducing the latency of memory service requests. The allocation strategy will need to be aware of queuing delays, on-chip latencies, and row-buffer hit-rates for each MC. In this paper, we propose dynamic mech- anisms that take these factors into account when placing data in appropriate slices of the physical memory. We in- troduce adaptive first-touch page placement, and dynamic page-migration mechanisms to reduce DRAM access delays for multi-MC systems. These policies yield average perfor- mance improvements of 17\% for adaptive first-touch page- placement, and 35\% for a dynamic page-migration policy.},
author = {Awasthi, Manu and Nellans, David W. and Sudan, Kshitij and Balasubramonian, Rajeev and Davis, Al},
booktitle = {Parallel Architectures and Compilation Techniques (PACT)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Awasthi et al. - 2010 - Handling the Problems and Opportunities Posed by Multiple On-Chip Memory Controllers.pdf:pdf},
keywords = {data placement,dram management,memory controller},
pages = {319--330},
title = {{Handling the Problems and Opportunities Posed by Multiple On-Chip Memory Controllers}},
year = {2010}
}
@article{Karypis1998,
abstract = {Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [Bui and Jones, Proc. of the 6th SIAM Conference on Parallel Processing for Scientific Computing, 1993, 445–452; Hen- drickson and Leland, A Multilevel Algorithm for Partitioning Graphs, Tech. report SAND 93-1301, Sandia National Laboratories, Albuquerque, NM, 1993]. From the early work it was clear that multilevel techniques held great promise; however, it was not known if they can be made to con- sistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the effectiveness of many different choices for all three phases: coarsening, partition of the coarsest graph, and refinement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the final partition obtained after multilevel refinement. We also present a much faster variation of the Kernighan–Lin (KL) algorithm for refining during uncoarsening. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear pro- gramming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute fill-reducing orderings for sparse matrices, it produces orderings that have substantially smaller fill than the widely used multiple minimum degree algorithm.},
author = {Karypis, George and Kumar, Vipin},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Karypis, Kumar - 1998 - A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {fill-reducing orderings,finite element,graph partitioning,metis,parallel computations},
mendeley-tags = {metis},
month = jan,
number = {1},
pages = {359--392},
title = {{A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs}},
volume = {20},
year = {1998}
}
@inproceedings{Subramoni2013,
author = {Subramoni, H and Bureddy, D and Kandalla, K and Schulz, K and Barth, B and Perkins, J and Arnold, M and Panda, D K},
booktitle = {International Conference on Cluster Computing (CLUSTER)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Subramoni et al. - 2013 - Design of Network Topology Aware Scheduling Services for Large InfiniBand Clusters.pdf:pdf},
isbn = {9781479908981},
keywords = {-network topology,clus,cluster technology,infiniband,mpi,scheduling,ter resource management},
mendeley-tags = {mpi},
title = {{Design of Network Topology Aware Scheduling Services for Large InfiniBand Clusters}},
year = {2013}
}
@inproceedings{Roloff2012a,
abstract = {Using the Cloud Computing paradigm for High- Performance Computing (HPC) is currently a hot topic in the research community and the industry. The attractive- ness of Cloud Computing for HPC is the capability to run large applications on powerful, scalable hardware without needing to actually own or maintain this hardware. Most current research focuses on running HPC applications on the Amazon Cloud Computing platform, which is relatively easy because it supports environments that are similar to existing HPC solutions, such as clusters and supercomput- ers. In this paper, we present the concepts of Cloud Comput- ing and evaluate Microsoft Windows Azure PaaS as a plat- form for HPC applications. Since most HPC applications are based on the Unix programming model, their source code has to be ported to the Windows programming model in addition to porting it to the Azure platform. We outline the challenges we encountered during porting applications and their resolutions. Results show that Azure has a high performance and scales well. Furthermore, it is a viable alternative for running HPC applications.},
author = {Roloff, Eduardo and Birck, Francis and Diener, Matthias and Carissimi, Alexandre and Navaux, Philippe O. A.},
booktitle = {Latin American Conference on High-Performance Computing (CLCAR)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Roloff et al. - 2012 - Using Windows Azure for High Performance Computing.pdf:pdf},
title = {{Using Windows Azure for High Performance Computing}},
year = {2012}
}
@inproceedings{Hoefler2011,
abstract = {The steadily increasing number of nodes in high-performance computing systems and the technology and power constraints lead to sparse network topologies. Efficient mapping of ap- plication communication patterns to the network topology gains importance as systems grow to petascale and beyond. Such mapping is supported in parallel programming frame- works such as MPI, but is often not well implemented. We show that the topology mapping problem is NP-complete and analyze and compare different practical topology map- ping heuristics. We demonstrate an efficient and fast new heuristic which is based on graph similarity and show its util- ity with application communication patterns on real topolo- gies. Our mapping strategies support heterogeneous net- works and show significant reduction of congestion on torus, fat-tree, and the PERCS network topologies, for irregular communication patterns. We also demonstrate that the ben- efit of topology mapping grows with the network size and show how our algorithms can be used in a practical setting to optimize communication performance. Our efficient topol- ogy mapping strategies are shown to reduce network con- gestion by up to 80\%, reduce average dilation by up to 50\%, and improve benchmarked communication performance by 18\%.},
author = {Hoefler, Torsten and Snir, Marc},
booktitle = {International Conference on Supercomputing (ICS)},
doi = {10.1145/1995896.1995909},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Hoefler, Snir - 2011 - Generic topology mapping strategies for large-scale parallel architectures.pdf:pdf},
isbn = {9781450301022},
keywords = {complexity,mpi graph topologies,scotch,topology mapping},
mendeley-tags = {complexity,scotch},
title = {{Generic topology mapping strategies for large-scale parallel architectures}},
year = {2011}
}
@inproceedings{Alves2013,
author = {Alves, Marco A. Z. and Villavieja, Carlos and Diener, Matthias and Navaux, Philippe O.A.},
booktitle = {International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
doi = {10.1109/SBAC-PAD.2013.12},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Alves et al. - 2013 - Energy Efficient Last Level Caches via Last ReadWrite Prediction.pdf:pdf},
isbn = {978-1-4799-2928-3},
month = oct,
pages = {73--80},
title = {{Energy Efficient Last Level Caches via Last Read/Write Prediction}},
year = {2013}
}
@techreport{Intel2012,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2012 - 2nd Generation Intel Core Processor Family.pdf:pdf},
keywords = {sandy bridge,sandybridge},
mendeley-tags = {sandy bridge,sandybridge},
number = {September},
title = {{2nd Generation Intel Core Processor Family}},
volume = {1},
year = {2012}
}
@inproceedings{Diener2010,
abstract = {Process placement is a technique widely used on parallel machines with heterogeneous interconnects to reduce the overall communication time. For instance, two processes which communicate frequently are mapped close to each other. Finding the optimal mapping between threads and cores in a shared-memory environment (for example, OpenMP and Pthreads) is an even more complex task due to implicit communication. In this work, we examine data sharing patterns between threads in different workloads and use those patterns in a similar way as messages are used to map processes in cluster computers. We evaluated our technique on a state-of-the-art multi- core processor and achieved moderate improvements in the common case and considerable improvements in some cases, reducing execution time by up to 45\%.},
author = {Diener, Matthias and Madruga, Felipe L. and Rodrigues, Eduardo R. and Alves, Marco A. Z. and Navaux, Philippe O. A.},
booktitle = {IEEE International Conference on High Performance Computing and Communications (HPCC)},
doi = {http://doi.ieeecomputersociety.org/10.1109/HPCC.2010.114},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener et al. - 2010 - Evaluating Thread Placement Based on Memory Access Patterns for Multi-core Processors.pdf:pdf},
keywords = {cache,memory access patterns,multi-core processor,process mapping,shared,thread placement},
title = {{Evaluating Thread Placement Based on Memory Access Patterns for Multi-core Processors}},
year = {2010}
}
@inproceedings{Ma2009,
abstract = {Interprocessor communication is an important factor in determining the performance scalability of parallel systems. The communication requirements of a parallel application can be quantified to understand its communication pattern and communication pattern similarities among applications can be determined. This is essential for the efficient mapping of applications on parallel systems and leads to better interprocessor communication implementation among others. This paper proposes a methodology to compare the communication pattern of distributed-memory programs. Communication correlation coefficient quantifies the degree of similarity between two applications based on the communication metrics selected to characterize the applications. To capture the network topology requirements, we extract the communication graph of each applications and quantities this similarity. We apply this methodology to four applications in the NAS parallel benchmark suite and evaluate the communication patterns by studying the effects of varying problem size and the number of logical processes (LPs).},
author = {Ma, Chao and Teo, Yong Meng and March, Verdi and Xiong, Naixue and Pop, Ioana Romelia and He, Yan Xiang and See, Simon},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2009 - An Approach for Matching Communication Patterns in Parallel Applications.pdf:pdf;:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2009 - An Approach for Matching Communication Patterns in Parallel Applications(2).pdf:pdf},
title = {{An Approach for Matching Communication Patterns in Parallel Applications}},
year = {2009}
}
@book{Weaver2000,
author = {Weaver, David L and Germond, Tom},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Weaver, Germond - 2000 - The SPARC Architecture Manual, Version 9.pdf:pdf},
isbn = {0138250014},
title = {{The SPARC Architecture Manual, Version 9}},
year = {2000}
}
@inproceedings{Saphir1997,
author = {Saphir, William and {Van der Wijngaart}, R. and Woo, A. and Yarrow, M.},
booktitle = {8th SIAM Conference on Parallel Processing for Scientific Computing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Saphir et al. - 1997 - New implementations and results for the NAS parallel benchmarks 2.pdf:pdf},
keywords = {nas},
mendeley-tags = {nas},
pages = {14--17},
title = {{New implementations and results for the NAS parallel benchmarks 2}},
url = {http://www.nas.nasa.gov/Resources/Software/npb\_2.2.pdf},
year = {1997}
}
@inproceedings{Jeannot2010,
abstract = {MPI process placement can play a deterministic role concerning the application performance. This is especially true with nowadays architecture (heterogenous, multicore with different level of caches, etc.). In this paper, we will describe a novel algorithm called TreeMatch that maps processes to resources in order to reduce the communication cost of the whole application. We have implemented this algorithm and will discuss its performance using simulation and on the NAS benchmarks.},
author = {Jeannot, Emmanuel and Mercier, Guillaume},
booktitle = {Euro-Par Parallel Processing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jeannot, Mercier - 2010 - Near-optimal placement of MPI processes on hierarchical NUMA architectures.pdf:pdf;:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jeannot, Mercier - 2010 - Near-optimal placement of MPI processes on hierarchical NUMA architectures(2).pdf:pdf},
keywords = {treematch},
mendeley-tags = {treematch},
pages = {199--210},
title = {{Near-optimal placement of MPI processes on hierarchical NUMA architectures}},
year = {2010}
}
@article{Barham2003,
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating sys- tems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most pro- vide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sac- rificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which oper- ating systems such as Linux, BSD andWindows XP, can be ported with minimal effort. Our design is targeted at hosting up to 100 virtual machine in- stances simultaneously on a modern server. The virtualization ap- proach taken by Xen is extremely efficient: we allowoperating sys- tems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead — at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Barham et al. - 2003 - Xen and the art of virtualization.pdf:pdf},
isbn = {1581137575},
journal = {ACM SIGOPS Operating Systems Review},
keywords = {hypervisors,paravirtualization,virtual machine monitors,xen},
mendeley-tags = {xen},
number = {5},
pages = {164--177},
title = {{Xen and the art of virtualization}},
url = {http://dl.acm.org/citation.cfm?id=945462},
volume = {37},
year = {2003}
}
@inproceedings{Hursey2011,
abstract = {High Performance Computing (HPC) systems are composed of servers containing an ever-increasing number of cores. With such high processor core counts, non-uniform mem- ory access (NUMA) architectures are almost universally used to reduce inter-processor and memory communication bottlenecks by distributing processors and memory throughout a server- internal networking topology. Application studies have shown that the tuning of processes placement in a server’s NUMA networking topology to the application can have a dramatic impact on performance. The performance implications are mag- nified when running a parallel job across multiple server nodes, especially with large scale HPC applications. This paper presents the Locality-Aware Mapping Algorithm (LAMA) for distributing the individual processes of a parallel application across processing resources in an HPC system, paying particular attention to the internal server NUMA topologies. The algorithm is able to support both homogeneous and het- erogeneous hardware systems, and dynamically adapts to the available hardware and user-specified process layout at run-time. As implemented in Open MPI, the LAMA provides 362,880 mapping permutations and is able to naturally scale out to additional hardware resources as they become available in future architectures.},
author = {Hursey, Joshua and Squyres, JM and Dontje, Terry},
booktitle = {IEEE International Conference on Cluster Computing (CLUSTER)},
doi = {10.1109/CLUSTER.2011.59},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Hursey, Squyres, Dontje - 2011 - Locality-Aware Parallel Process Mapping for Multi-Core HPC Systems.pdf:pdf},
isbn = {978-1-4577-1355-2},
keywords = {locality,mpi,numa,process affinity,resource management},
month = sep,
title = {{Locality-Aware Parallel Process Mapping for Multi-Core HPC Systems}},
year = {2011}
}
@techreport{ParsecGroup2011,
abstract = {This memo presents the study of the exploration of input sets for SPLASH-2. Based on experimental data, we generate a modernized SPLASH-2, a.k.a., SPLASH-2x, by selecting multiple scales of input sets. SPLASH-2x will be integrated into PARSEC framework.},
author = {{PARSEC Group}},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/PARSEC Group - 2011 - A Memo on Exploration of SPLASH-2 Input Sets.pdf:pdf},
keywords = {input,parsec,splash2,splash2x},
mendeley-tags = {input,parsec,splash2,splash2x},
number = {June},
title = {{A Memo on Exploration of SPLASH-2 Input Sets}},
year = {2011}
}
@techreport{AMD2012,
author = {AMD},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/AMD - 2012 - AMD Opteron™ 6300 Series processor Quick Reference Guide.pdf:pdf},
keywords = {abu dhabi,abudhabi,kbs,opteron},
mendeley-tags = {abu dhabi,abudhabi,kbs,opteron},
number = {August},
title = {{AMD Opteron™ 6300 Series processor Quick Reference Guide}},
year = {2012}
}
@inproceedings{Noordergraaf1999,
abstract = {This paper presents performance results from work done on Sun\&amp;\#146;s WildFire system. WildFire is a codename for a prototype shared memory multiprocessor developed by Sun MicrosystemsTM consisting of up to four unmodified Sun EnterpriseTM x000 series symmetric multiprocessors (SMPs). A goal of the WildFire system is to evaluate the effectiveness of leveraging large SMPs in the construction of even larger systems. We have conducted several performance experiments with a shared memory parallelized finite difference solver. Our work demonstrates the key features of the WildFire system, including automatic page migration and read/write replication. Our results show that the dynamic page migration algorithms used by the WildFire system are effective in automatically optimizing data placement at runtime. Performance comparisons between the WildFire system and currently available SMPs show that the system exhibits good scalability characteristics, and actually outperforms SMPs on this particular application.},
author = {Noordergraaf, L. and Pas, R. Van Der},
booktitle = {Supercomputing (SC)},
doi = {10.1109/SC.1999.10052},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Noordergraaf, Pas - 1999 - Performance Experiences on Sun's WildFire Prototype.pdf:pdf},
isbn = {1-58113-091-0},
pages = {1--16},
title = {{Performance Experiences on Sun's WildFire Prototype}},
year = {1999}
}
@misc{Mattson2008,
author = {Mattson, Tim and Meadows, Larry},
booktitle = {Intel Corporation},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mattson, Meadows - 2008 - A “Hands-on” Introduction to OpenMP.pdf:pdf},
title = {{A “Hands-on” Introduction to OpenMP}},
year = {2008}
}
@inproceedings{Dupros2010,
author = {Dupros, Fabrice and Pousa, Christiane and Carissimi, Alexandre and M\'{e}haut, Jean-Fran\c{c}ois},
booktitle = {Parallel Computing: From Multicores and GPU's to Petascale},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Dupros et al. - 2010 - Parallel simulations of seismic wave propagation on NUMA architectures.pdf:pdf},
pages = {67--74},
title = {{Parallel simulations of seismic wave propagation on NUMA architectures}},
year = {2010}
}
@techreport{Bhadauria2008,
abstract = {The shared-memory, multi-threaded PARSEC benchmark suite is intended to represent emerging software work- loads for future systems. It is specifically intended for use by both industry and academia as a tool for testing new Chip Multiprocessor (CMP) designs. We analyze the suite in detail and identify bottlenecks using hardware per- formance counters. We take a systems-level approach, with an emphasis on architectural design space features that vary with CMPs. We narrow the architectural design space, and list several areas worth investigating for improved benchmark performance.},
author = {Bhadauria, Major and Weaver, Vince and McKee, Sally A.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bhadauria, Weaver, McKee - 2008 - A Charecterization of the PARSEC Benchmark Suite for CMP Design.pdf:pdf},
keywords = {Benchmark,Parsec},
mendeley-tags = {Benchmark,Parsec},
title = {{A Charecterization of the PARSEC Benchmark Suite for CMP Design}},
year = {2008}
}
@inproceedings{Deveci2014,
author = {Deveci, Mehmet and Rajamanickam, Sivasankaran and Leung, Vitus J. and Pedretti, Kevin and Olivier, Stephen L. and Bunde, David P. and Catalyurek, Umit V. and Devine, Karen},
booktitle = {International Parallel and Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Deveci et al. - 2014 - Exploiting Geometric Partitioning in Task Mapping for Parallel Computers.pdf:pdf},
isbn = {978-1-4799-3800-1},
month = may,
pages = {27--36},
title = {{Exploiting Geometric Partitioning in Task Mapping for Parallel Computers}},
year = {2014}
}
@misc{Intel2012b,
author = {Intel},
title = {{Intel Performance Counter Monitor - A better way to measure CPU utilization}},
url = {http://www.intel.com/software/pcm},
year = {2012}
}
@inproceedings{Karlsson2012,
author = {Karlsson, C and Davies, T and Chen, Zizhong},
booktitle = {IEEE International Conference on Cluster Computing (CLUSTER)},
doi = {10.1109/CLUSTER.2012.47},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Karlsson, Davies, Chen - 2012 - Optimizing Process-to-Core Mappings for Application Level Multi-dimensional MPI Communications.pdf:pdf},
keywords = {Cartesian communicator,Jaguar supercomputer,MPT 3.},
pages = {486--494},
title = {{Optimizing Process-to-Core Mappings for Application Level Multi-dimensional MPI Communications}},
year = {2012}
}
@misc{Luszczek2005,
author = {Luszczek, Piotr and Dongarra, Jack J. and Koester, David and Rabenseifer, Rolf and Lucas, Bob and Kepner, Jeremy and Mccalpin, John and Bailey, David and Takahashi, Daisuke and Jack, J and Rabenseifner, Rolf},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Luszczek et al. - 2005 - Introduction to the HPC challenge benchmark suite.pdf:pdf},
title = {{Introduction to the HPC Challenge Benchmark Suite}},
year = {2005}
}
@techreport{Corporation2011,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2011 - Intel ® Xeon ® Processor 5600 Series.pdf:pdf},
keywords = {nehalem},
mendeley-tags = {nehalem},
number = {June},
title = {{Intel® Xeon® Processor 5600 Series}},
year = {2011}
}
@inproceedings{Marathe2006,
author = {Marathe, Jaydeep and Mueller, Frank},
booktitle = {ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Marathe, Mueller - 2006 - Hardware Profile-guided Automatic Page Placement for ccNUMA Systems.pdf:pdf},
pages = {90--99},
title = {{Hardware Profile-guided Automatic Page Placement for ccNUMA Systems}},
year = {2006}
}
@article{Moh2009,
author = {Moh, Sangman},
doi = {10.1109/HPCC.2009.72},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Moh - 2009 - Adjacency-Based Mesh Process Mapping for Irregular Cluster Systems.pdf:pdf},
isbn = {978-1-4244-4600-1},
journal = {2009 11th IEEE International Conference on High Performance Computing and Communications},
pages = {500--505},
publisher = {Ieee},
title = {{Adjacency-Based Mesh Process Mapping for Irregular Cluster Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5167035},
year = {2009}
}
@inproceedings{Chen2006,
abstract = {SMP clusters and multiclusters are widely used to execute message-passing parallel applications. The ways to map par- allel processes to processors (or cores) could affect the ap- plication performance significantly due to the non-uniform communicating cost in such systems. It is desired to have a tool to map parallel processes to processors (or cores) auto- matically. Although there have been various efforts to address this issue, the existing solutions either require intensive user in- tervention, or can not be able to handle the situation of multiclusters well. In this paper, we propose a profile-guided approach to find the optimized mapping automatically to minimize the cost of point-to-point communications for arbitrary mes- sage passing applications. The implemented toolset is called MPIPP (MPI Process Placement toolset), and it includes several components: 1) A tool to get the communication profile of MPI applica- tions 2) A tool to get the network topology of target clusters 3) An algorithm to find optimized mapping, which is espe- cially more effective than existing graph partition algorithms for multiclusters. We evaluated the performance of our tool with the NPB benchmarks and three other applications in several clus- ters. Experimental results show that the optimized pro- cess placement generated by our tools can achieve significant speedup.},
author = {Chen, Hu and Chen, Wenguang and Huang, Jian and Robert, Bob and Kuhn, H},
booktitle = {International Conference on Supercomputing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2006 - MPIPP An Automatic Profile-guided Parallel Process Placement Toolset for SMP Clusters and Multiclusters.pdf:pdf},
keywords = {clus-,graph mapping,graph partitioning,parallel computing,process placement,ter},
pages = {353--360},
title = {{MPIPP: An Automatic Profile-guided Parallel Process Placement Toolset for SMP Clusters and Multiclusters}},
year = {2006}
}
@article{Gibson2000,
abstract = {Simulation is the primary method for evaluating computer systems during all phases of the design process. One signif- icant problem with simulation is that it rarely models the system exactly, and quantifying the resulting simulator er- ror can be difficult. More importantly, architects often as- sume without proof that although their simulator may make inaccurate absolute performance predictions, it will still ac- curately predict architectural trends. This paper studies the source and magnitude of error in a range of architectural simulators by comparing the simu- lated execution time of several applications and microbench- marks to their execution time on the actual hardware being modeled. The existence of a hardware gold standard al- lows us to find, quantify, and fix simulator inaccuracies. We then use the simulators to predict architectural trends and analyze the sensitivity of the results to the simulator config- uration. We find that most of our simulators predict trends accurately, as long as they model all of the important perfor- mance effects for the application in question. Unfortunately, it is difficult to know what these effects are without having a a hardware reference, as they can be quite subtle. This calls into question the value, for architectural studies, of highly detailed simulators whose characteristics are not carefully validated against a real hardware design.},
author = {Gibson, Jeff and Kunz, Robert and Ofelt, David and Horowitz, Mark and Hennessy, John and Heinrich, Mark},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Gibson et al. - 2000 - FLASH vs. (Simulated) FLASH Closing the Simulation Loop.pdf:pdf},
journal = {ACM SIGPLAN Notices},
number = {11},
pages = {49--58},
title = {{FLASH vs. (Simulated) FLASH: Closing the Simulation Loop}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.368\&rep=rep1\&type=pdf},
volume = {35},
year = {2000}
}
@article{Foster1996,
abstract = {Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems.},
author = {Foster, Ian and Kesselman, Carl and Tuecke, Steven},
doi = {10.1006/jpdc.1996.0108},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Foster, Kesselman, Tuecke - 1996 - The Nexus Approach to Integrating Multithreading and Communication.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = aug,
number = {1},
pages = {70--82},
title = {{The Nexus Approach to Integrating Multithreading and Communication}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0743731596901082},
volume = {37},
year = {1996}
}
@inproceedings{McCurdy2010,
author = {McCurdy, Collin and Vetter, Jeffrey},
booktitle = {IEEE International Symposium on Performance Analysis of Systems \& Software (ISPASS)},
doi = {10.1109/ISPASS.2010.5452060},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/McCurdy, Vetter - 2010 - Memphis Finding and fixing NUMA-related performance problems on multi-core platforms.pdf:pdf},
isbn = {978-1-4244-6023-6},
pages = {87--96},
title = {{Memphis: Finding and fixing NUMA-related performance problems on multi-core platforms}},
year = {2010}
}
@phdthesis{Erlandsson2004,
abstract = {TSP (or Telecommunication Server Platform) is a scalable, high availability cluster operating system developed by Ericsson for use in the telecommuni- cations industry. This thesis describes an attempt to simulate a TSP cluster in the full system simulator Simics, and talks about some of the possibilities offered by such a setup and full system simulation in general. This attempt to simulate TSP was unsuccessful in completely booting the cluster in Simics, but some of the experiences and problems encountered are described. A proposed development environment for working with TSP in Simics is also presented, along with scripts that were created during this thesis to alleviate the working process.},
author = {Erlandsson, Emil and Eriksson, Olle},
booktitle = {Engineering},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Erlandsson, Eriksson - 2004 - Experiences from Simulating TSP Clusters in the Simics Full System Simulator.pdf:pdf},
keywords = {Full system simulation,Simics,TSP,TelORB},
number = {June},
pages = {52},
title = {{Experiences from Simulating TSP Clusters in the Simics Full System Simulator}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.103.4598\&rep=rep1\&type=pdf},
year = {2004}
}
@inproceedings{Ivanov2001,
abstract = {A cache coherence protocol is a set of rules, which cache controllers in a system with multiple cache memories must follow to maintain the consistency of data stored in the local cache memories as well as in main memory. MESI is a popular cache coherence protocol used to synchronize the operation of cache controllers in many Shared Memory MIMD systems. MESI is also used to maintain the consistency between the level-1 and level-2 caches of the Intel Pentium® microprocessor. I this paper we present a model of the MESI protocol based on the recently introduced series-parallel poset modeling and verification methodology. We illustrate the use of the new methodology by verifying a few properties of the MESI protocol.},
author = {Ivanov, L. and Nunna, R.},
booktitle = {IEEE International Symposium on Circuits and Systems (ISCAS)},
doi = {10.1109/ISCAS.2001.922002},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ivanov, Nunna - 2001 - Modeling and Verification of Cache Coherence Protocols.pdf:pdf},
isbn = {0-7803-6685-9},
keywords = {mesi},
mendeley-tags = {mesi},
pages = {129--132},
publisher = {IEEE},
title = {{Modeling and Verification of Cache Coherence Protocols}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=922002},
volume = {5},
year = {2001}
}
@inproceedings{Mercier2011,
author = {Mercier, Guillaume and Jeannot, Emmanuel},
booktitle = {European MPI Users' Group conference on Recent advances in the message passing interface (EuroMPI)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mercier, Jeannot - 2011 - Improving MPI Applications Performance on Multicore Clusters with Rank Reordering.pdf:pdf},
keywords = {communication pattern,message-passing,multicore architectures,process placement,rank reordering},
title = {{Improving MPI Applications Performance on Multicore Clusters with Rank Reordering}},
year = {2011}
}
@misc{Corbet2012,
author = {Corbet, Jonathan},
title = {{AutoNUMA: the other approach to NUMA scheduling}},
url = {http://lwn.net/Articles/488709/},
year = {2012}
}
@inproceedings{Lof2005,
abstract = {To achieve close to optimal performance on cc-NUMA systems for shared memory parallel applications with com- plex data access patterns, a mechanism for co-locating threads and the data during the execution of the program is needed. The affinity-on-next-touch procedure studied in this paper is based on re-doing the standard first-touch al- location at explicitly given locations in the code. We study the performance of a parallelized scientific computing ap- plication for which thread-data affinity can not be created by standard methods. We observe a performance improve- ment of 64\% if the affinity-on-next-touch procedure is in- voked once at a critical location in the code. We also per- form experiments that show that the overhead connected to creating the affinity can be almost fully attributed the han- dling of page entries in the TLBs. The cost for actually mi- grating data is negligible. Using larger but fewer pages we measure a performance improvement of 127\% compared to the original code.},
author = {L\"{o}f, Henrik and Holmgren, Sverker},
booktitle = {International Conference on Supercomputing (SC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/L\"{o}f, Holmgren - 2005 - affinity-on-next-touch Increasing the Performance of an Industrial PDE Solver on a cc-NUMA System.pdf:pdf},
pages = {387--392},
title = {{affinity-on-next-touch: Increasing the Performance of an Industrial PDE Solver on a cc-NUMA System}},
year = {2005}
}
@inproceedings{Nikolopoulos2000a,
abstract = {We present the design and implementation Of UPMLIB, a runtime system$\backslash$nthat provides transparent facilities for dynamically tuning the memory$\backslash$nperformance of OpenMP programs on scalable shared-memory$\backslash$nmultiprocessors with hardware cache-coherence, UPMLIB integrates$\backslash$ninformation from the compiler and the operating system, to implement$\backslash$nalgorithms that perform accurate and timely page migrations. The$\backslash$nalgorithms and the associated mechanisms correlate memory reference$\backslash$ninformation with the semantics of parallel programs and scheduling$\backslash$nevents that break the association between threads and data for which$\backslash$nthreads have memory affinity at runtime. Our experimental evidence$\backslash$nshows that UPMLIB makes OpenMP programs immune to the page placement$\backslash$nstrategy of the operating system, thus obviating the need for$\backslash$nintroducing data placement directives in OpenMP. Furthermore, UPMlib$\backslash$nprovides solid improvements of throughput in multiprogrammed execution$\backslash$nenvironments.},
author = {Nikolopoulos, D S and Papatheodorou, T S and Polychronopoulos, C D and Labarta, J and Ayguade, E},
booktitle = {Languages, Compilers, and Run-Time Systems for Scalable Computers},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Nikolopoulos et al. - 2000 - UPMLIB A runtime system for Turning the memory performance of OpenMP programs on scalable shared-memory mul.pdf:pdf},
keywords = {memory management,openmp,oper-,processors,runtime systems,scalable shared-memory multi-},
pages = {85--99},
title = {{UPMLIB: A runtime system for Turning the memory performance of OpenMP programs on scalable shared-memory multiprocessors}},
year = {2000}
}
@article{Edmonds1965,
abstract = {A matching in a graph G is a subset of edges in G such that no two meet the same node in G. The convex polyhedron C is characterized, where the extreme points of C correspond to the matchings in G. Where each edge of G carries a real numerical weight, an efficient algorithm is described for finding a matching in G with maximum weight-sum.},
author = {Edmonds, Jack},
doi = {10.6028/jres.069B.013},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Edmonds - 1965 - Maximum matching and a polyhedron with 0,1-vertices.pdf:pdf},
issn = {0022-4340},
journal = {Journal of Research of the National Bureau of Standards -- Section B. Mathematics and Mathematical Physics},
keywords = {"Graph G,0,1-vertices",edmonds,maximum matching,polyhedron},
mendeley-tags = {edmonds},
number = {1 and 2},
pages = {125},
title = {{Maximum matching and a polyhedron with 0,1-vertices}},
volume = {69B},
year = {1965}
}
@inproceedings{Buntinas2006a,
author = {Buntinas, Darius and Mercier, Guillaume and Gropp, William},
booktitle = {International Symposium on Cluster Computing and the Grid (CCGRID)},
doi = {10.1109/CCGRID.2006.31},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Buntinas, Mercier, Gropp - 2006 - Design and evaluation of Nemesis, a scalable, low-latency, message-passing communication subsystem.pdf:pdf},
isbn = {0-7695-2585-7},
pages = {521--530},
title = {{Design and evaluation of Nemesis, a scalable, low-latency, message-passing communication subsystem}},
year = {2006}
}
@article{Aochi2013,
author = {Aochi, Hideo and Ulrich, Thomas and Ducellier, Ariane and Dupros, Fabrice and Michea, David},
doi = {10.1088/1742-6596/454/1/012010},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Aochi et al. - 2013 - Finite difference simulations of seismic wave propagation for understanding earthquake physics and predicting grou.pdf:pdf},
issn = {1742-6596},
journal = {Journal of Physics: Conference Series},
month = aug,
number = {1},
pages = {012010},
title = {{Finite difference simulations of seismic wave propagation for understanding earthquake physics and predicting ground motions: Advances and challenges}},
volume = {454},
year = {2013}
}
@inproceedings{Blagodurov2010,
author = {Blagodurov, Sergey and Zhuravlev, Sergey and Dashti, Mohammad and Fedorova, Alexandra},
booktitle = {USENIX Annual Technical Conference},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Blagodurov et al. - 2010 - A Case for NUMA-aware Contention Management on Multicore Systems.pdf:pdf},
pages = {557--571},
title = {{A Case for NUMA-aware Contention Management on Multicore Systems}},
year = {2010}
}
@article{Torrellas2009,
abstract = {Extreme-scale computers promise orders-of-magnitude improvement in performance over current high-end machines for the same machine power consumption and physical footprint. They also bring some important architectural challenges.},
author = {Torrellas, Josep},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Torrellas - 2009 - Architectures for extreme-scale computing.pdf:pdf},
journal = {IEEE Computer},
number = {11},
pages = {28--35},
title = {{Architectures for extreme-scale computing}},
volume = {42},
year = {2009}
}
@article{Lin2012,
author = {Lin, Yufei and Tang, Yuhua and Xu, Xinhai},
doi = {10.1109/ICAICT.2012.6398498},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Tang, Xu - 2012 - SaPM Switch-aware process mapping model for parallel computing.pdf:pdf},
isbn = {978-1-4673-1740-5},
journal = {2012 6th International Conference on Application of Information and Communication Technologies (AICT)},
month = oct,
pages = {1--4},
publisher = {Ieee},
title = {{SaPM: Switch-aware process mapping model for parallel computing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6398498},
year = {2012}
}
@article{Tikir2008,
abstract = {In this paper, we first introduce a profile-driven online page migration scheme and investigate its impact on the performance of multithreaded applications. We use centralized lightweight, inexpensive plug- in hardware monitors to profile the memory access behavior of an application, and then migrate pages to memory local to the most frequently accessing processor. We also investigate the use of several other potential sources of data gathered from hardware monitors and compare their effectiveness to using data from centralized hardware monitors. In particular, we investigate the effectiveness of using cache miss profiles, Translation Lookaside Buffer (TLB) miss profiles and the content of the on-chip TLBs using the valid bit information. Moreover,wealso introduce a modest hardware feature, called Address Translation Counters (ATC), and compare its effectiveness with other sources of hardware profiles. Using the Dyninst runtime instrumentation combined with hardware monitors, we were able to add page migration capabilities to a Sun Fire 6800 server without having to modify the operating system kernel, or to re-compile application programs. Our dynamic page migration scheme reduced the total number of non-local memory accesses of applications by up to 90\% and improved the execution times up to 16\%.Wealso conducted a simulation based study and demonstrated that cache miss profiles gathered from on-chip CPU monitors, which are typically available in current microprocessors, can be effectively used to guide dynamic page migrations in applications.},
author = {Tikir, Mustafa M. and Hollingsworth, Jeffrey K.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Tikir, Hollingsworth - 2008 - Hardware monitors for dynamic page migration.pdf:pdf},
journal = {Journal of Parallel and Distributed Computing (JPDC)},
keywords = {dynamic page migration,hardware performance monitors},
month = sep,
number = {9},
pages = {1186--1200},
title = {{Hardware monitors for dynamic page migration}},
volume = {68},
year = {2008}
}
@misc{Intel2012a,
author = {Intel},
keywords = {compact},
mendeley-tags = {compact},
title = {{Using KMP\_AFFINITY to create OpenMP thread mapping to OS proc IDs}},
url = {https://software.intel.com/sites/products/documentation/doclib/iss/2013/compiler/cpp-lin/GUID-8BA55F4A-D5AE-4E27-8C25-058B68D280A4.htm},
year = {2012}
}
@article{Su2012,
author = {Su, ChunYi and Li, Dong and Nikolopoulos, Dimitrios S. and Grove, Matthew and Cameron, Kirk and {De Supinski}, Bronis R.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Su et al. - 2012 - Critical Path-Based Thread Placement for NUMA systems.pdf:pdf},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {critical path,multicore processors,numa,openmp,shared resource contention,thread placement},
number = {2},
pages = {106--112},
title = {{Critical Path-Based Thread Placement for NUMA systems}},
volume = {40},
year = {2012}
}
@inproceedings{Cuesta2011a,
author = {Cuesta, Blas and Ros, Alberto and G\'{o}mez, Mar\'{\i}a E and Robles, Antonio and Duato, Jos\'{e}},
booktitle = {ISCA},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cuesta et al. - 2011 - Increasing the Effectiveness of Directory Caches by Deactivating Coherence for Private Memory Blocks Categories a.pdf:pdf},
isbn = {9781450304726},
keywords = {cache coherence,coherence deactivation,directory cache,efficiency,multiprocessor,operating,private block,system},
pages = {93--103},
title = {{Increasing the Effectiveness of Directory Caches by Deactivating Coherence for Private Memory Blocks Categories and Subject Descriptors}},
year = {2011}
}
@article{Wulf1995,
author = {Wulf, WA},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Wulf - 1995 - Hitting the memory wall Implications of the obvious.pdf:pdf},
journal = {ACM SIGARCH Computer Architecture News},
pages = {20--24},
title = {{Hitting the memory wall: Implications of the obvious}},
year = {1995}
}
@inproceedings{Feliu2012,
abstract = {In order to improve CMP performance, recent research has focused on scheduling to mitigate contention produced by the limited memory bandwidth. Nowadays, com- mercial CMPs implement multi-level cache hierarchies where last level caches are shared by at least two cache structures located at the immediately lower cache level. In turn, these caches can be shared by several multithreaded cores. In this microprocessor design, contention points may appear along the whole memory hierarchy. Moreover, this problem is expected to aggravate in future technologies, since the number of cores and hardware threads, and consequently the size of the shared caches increases with each microprocessor generation. In this paper we characterize the impact on performance of the different contention points that appear along the memory subsystem. Then, we propose a generic scheduling strategy for CMPs that takes into account the available bandwidth at each level of the cache hierarchy. The proposed strategy selects the processes to be co-scheduled and allocates them to cores in order to minimize contention effects. The proposal has been implemented and evaluated in a commercial single-threaded quad-core processor with a rel- atively small two-level cache hierarchy. Despite these potential contention limitations are less than in recent processor designs, compared to the Linux scheduler, the proposal reaches perfor- mance improvements up to 9\% while these benefits (across the studied benchmark mixes) are always lower than 6\% for a memory-aware scheduler that does not take into account the cache hierarchy. Moreover, in some cases the proposal doubles the speedup achieved by the memory-aware scheduler.},
author = {Feliu, Josue and Sahuquillo, Julio and Petit, Salvador and Duato, Jose},
booktitle = {International Parallel and Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2012.54},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Feliu et al. - 2012 - Understanding Cache Hierarchy Contention in CMPs to Improve Job Scheduling.pdf:pdf},
isbn = {978-1-4673-0975-2},
keywords = {cache hierarchy,memory-aware scheduling,shared caches},
title = {{Understanding Cache Hierarchy Contention in CMPs to Improve Job Scheduling}},
year = {2012}
}
@article{Thekkath1994,
abstract = {Multithreaded amhitectures context switch between in- struction streams to hide memory access latency. Although this impmves processor utilization, it can increase cache intetfenmce and\&grade overall performance. One tech- nique to reduce the interconnect tram is to co-locate threads that sham data on the same pmcessoz Multiple threads sharing in the cache should reduce compulso\~{} and invalidation misses, thereby impmving execution time. Totest this hypothesis, we compared a variety of thread placement algorithms via trace-driven simulation of four- teen coarse- and medium-grain parallel applications on several multithnwded architectures. Our results contra- dict the hypothesis. Rather than \&creasing, compulsory and invalidation misses nvnained nearly constant across all placement algorithms, for allpmcessor configurations, even with an injinite cache. That is, sharing-based place- nu?nthad no (positive) effect on mzcutwn time. Instead load balancing was the critical factor that agectedperjor- mance. Our results were due to one or both of thefollowing reasons: (1) the sequential and uniform access of shared data by the application’s threads and (2) the insignificant number of data references that require interconnect access, relative to the total number of instructions.},
author = {Thekkath, R. and Eggers, S. J.},
doi = {10.1145/192007.192027},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Thekkath, Eggers - 1994 - Impact of sharing-based thread placement on multithreaded architectures.pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
month = apr,
number = {2},
pages = {176--186},
title = {{Impact of sharing-based thread placement on multithreaded architectures}},
url = {http://portal.acm.org/citation.cfm?doid=192007.192027},
volume = {22},
year = {1994}
}
@inproceedings{Sistare1999,
author = {Sistare, Steve and Vandevaart, R and Loh, Eugene},
booktitle = {ACM/IEEE Conference on Supercomputing (SC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Sistare, Vandevaart, Loh - 1999 - Optimization of MPI collectives on clusters of large-scale SMP's.pdf:pdf},
isbn = {1581130910},
keywords = {clustering,collective,mpi,mpich,shared memory,smp},
title = {{Optimization of MPI collectives on clusters of large-scale SMP's}},
url = {http://dl.acm.org/citation.cfm?id=331555},
year = {1999}
}
@inproceedings{Ribeiro2009,
author = {Ribeiro, Christiane P and Mehaut, Jean-Francois and Carissimi, Alexandre and Castro, Marcio and Fernandes, Luiz G},
booktitle = {International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
doi = {10.1109/SBAC-PAD.2009.16},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro et al. - 2009 - Memory Affinity for Hierarchical Shared Memory Multiprocessors.pdf:pdf},
keywords = {2009,mai,nas,numa,sbac,scheduling},
mendeley-tags = {mai},
pages = {59--66},
title = {{Memory Affinity for Hierarchical Shared Memory Multiprocessors}},
year = {2009}
}
@article{Pielke1992,
author = {Pielke, R A and Cotton, W R and Walko, R L and Trembaek, C J and Lyons, W A and Grasso, L D and Nieholls, M E and Moran, M D and Wesley, D A and Lee, T J and Copeland, J H},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pielke et al. - 1992 - A Comprehensive Meteorological Modeling System- RAMS.pdf:pdf},
journal = {Meteorology and Atmospheric Physics},
keywords = {rams},
mendeley-tags = {rams},
number = {1-4},
pages = {69--91},
title = {{A Comprehensive Meteorological Modeling System- RAMS}},
volume = {91},
year = {1992}
}
@inproceedings{Cho2006,
author = {Cho, Sangyeun and Jin, Lei},
booktitle = {Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cho, Jin - 2006 - Managing Distributed, Shared L2 Caches through OS-Level Page Allocation.pdf:pdf},
isbn = {0769527329},
pages = {455--468},
publisher = {IEEE Computer Society},
title = {{Managing Distributed, Shared L2 Caches through OS-Level Page Allocation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.3652\&rep=rep1\&type=pdf},
year = {2006}
}
@techreport{Pilla2011,
author = {Pilla, L and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Bhatele, Abhinav and Navaux, Philippe O A},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pilla et al. - 2011 - Improving Parallel System Performance with a NUMA-aware Load Balancer.pdf:pdf},
keywords = {-load balancing,mem-,non-uniform memory access,numa,object migration,ory contention,performance},
mendeley-tags = {numa},
title = {{Improving Parallel System Performance with a NUMA-aware Load Balancer}},
year = {2011}
}
@article{Li2013,
abstract = {This article introduces McPAT, an integrated power, area, and timing modeling framework that supports comprehensive design space exploration for multicore and manycore processor configurations ranging from 90nm to 22nm and beyond. At microarchitectural level, McPAT includes models for the fundamental components of a complete chip multiprocessor, including in-order and out-of-order processor cores, networks-on-chip, shared caches, and integrated system components such as memory controllers and Ethernet controllers. At circuit level, McPAT supports detailed modeling of critical-path timing, area, and power. At technology level, McPAT models timing, area, and power for the device types forecast in the ITRS roadmap. McPAT has a flexible XML interface to facilitate its use with many performance simulators. Combined with a performance simulator, McPAT enables architects to accurately quantify the cost of new ideas and assess trade-offs of different architectures using new metrics such as Energy-Delay-Area2 Product (EDA2P) and Energy-Delay-Area Product (EDAP). This article explores the interconnect options of future manycore processors by varying the degree of clustering over generations of process technologies. Clustering will bring interesting trade-offs between area and performance because the interconnects needed to group cores into clusters incur area overhead, but many applications can make good use of them due to synergies from cache sharing. Combining power, area, and timing results of McPAT with performance simulation of PARSEC benchmarks for manycore designs at the 22nm technology shows that 8-core clustering gives the best energy-delay product, whereas when die area is taken into account, 4-core clustering gives the best EDA2P and EDAP.},
author = {Li, Sheng and Ahn, Jung Ho and Strong, Richard D. and Brockman, Jay B. and Tullsen, Dean M. and Jouppi, Norman P.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2013 - The McPAT Framework for Multicore and Manycore Architectures Simultaneously Modeling Power, Area, and Timing.pdf:pdf},
issn = {15443566},
journal = {ACM Transactions on Architecture and Code Optimization},
number = {1},
pages = {1--29},
title = {{The McPAT Framework for Multicore and Manycore Architectures: Simultaneously Modeling Power, Area, and Timing}},
volume = {10},
year = {2013}
}
@article{Jovanovic2001,
author = {Jovanovic, Zoran and Maric, Slavko},
doi = {10.1016/S0167-739X(00)00055-8},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jovanovic, Maric - 2001 - A heuristic algorithm for dynamic task scheduling in highly parallel computing systems.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {communication,dynamic task scheduling,multiple processor computing systems,parallel processing,precedence constraints},
month = apr,
number = {6},
pages = {721--732},
title = {{A heuristic algorithm for dynamic task scheduling in highly parallel computing systems}},
volume = {17},
year = {2001}
}
@inproceedings{Barrow-Williams2009,
abstract = {Recent benchmark suite releases such as Parsec specifically utilise the tightly coupled cores available in chip- multiprocessors to allow the use of newer, high perfor- mance, models of parallelisation. However, these tech- niques introduce additional irregularity and complexity to data sharing and are entirely dependent on efficient com- munication performance between processors. This paper thoroughly examines the crucial communication and shar- ing behaviour of these future applications. The infrastructure used allows both accurate and com- prehensive program analysis, employing a full Linux OS running on a simulated 32-core x86 machine. Experiments use full programruns,with communication classified at both core and thread granularities. Migratory, read-only and producer-consumer sharing patterns are observed and their behaviour characterised. The temporal and spatial charac- teristics of communication are presented for the full collec- tion of Splash-2 and Parsec benchmarks. Our results aim to support the design of future communication systems for CMPs, encompassing coherence protocols, network-on-chip and thread mapping.},
author = {Barrow-Williams, Nick and Fensch, Christian and Moore, Simon},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2009.5306792},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Barrow-Williams, Fensch, Moore - 2009 - A Communication Characterisation of Splash-2 and Parsec.pdf:pdf},
isbn = {978-1-4244-5156-2},
keywords = {Benchmark,Parsec,Splash-2},
mendeley-tags = {Benchmark,Parsec,Splash-2},
title = {{A Communication Characterisation of Splash-2 and Parsec}},
year = {2009}
}
@article{Jeannot2014,
author = {Jeannot, Emmanuel and Mercier, Guillaume and Tessier, Francois},
doi = {10.1109/TPDS.2013.104},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jeannot, Mercier, Tessier - 2014 - Process Placement in Multicore ClustersAlgorithmic Issues and Practical Techniques.pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
month = apr,
number = {4},
pages = {993--1002},
title = {{Process Placement in Multicore Clusters: Algorithmic Issues and Practical Techniques}},
volume = {25},
year = {2014}
}
@inproceedings{Bienia2008a,
abstract = {The PARSEC benchmark suite was recently released and has been adopted by a significant number of users within a short amount of time. This new collection of workloads is not yet fully understood by researchers. In this study we compare the SPLASH-2 and PARSEC benchmark suites with each other to gain insights into differences and similarities between the two program collections. We use standard statistical methods and machine learning to analyze the suites for redundancy and overlap on chip-multiprocessors (CMPs). Our analysis shows that PARSEC workloads are fundamentally different from SPLASH-2 benchmarks. The observed differences can be explained with two technology trends, the proliferation of CMPs and the accelerating growth of world data.},
author = {Bienia, Christian and Kumar, Sanjeev and Li, Kai},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2008.4636090},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bienia, Kumar, Li - 2008 - PARSEC vs. SPLASH-2 A quantitative comparison of two multithreaded benchmark suites on Chip-Multiprocessors.pdf:pdf},
isbn = {978-1-4244-2777-2},
keywords = {benchmark suite,multithreading,performance measurement,shared-memory computers},
month = oct,
pages = {47--56},
title = {{PARSEC vs. SPLASH-2: A quantitative comparison of two multithreaded benchmark suites on Chip-Multiprocessors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4636090},
year = {2008}
}
@misc{JEDEC2012,
author = {JEDEC},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/JEDEC - 2012 - DDR3 SDRAM Standard.pdf:pdf},
title = {{DDR3 SDRAM Standard}},
year = {2012}
}
@misc{Kleen2004,
author = {Kleen, Andi},
booktitle = {Novel Inc},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kleen - 2004 - An NUMA API for Linux.pdf:pdf},
keywords = {libnuma,numactl},
mendeley-tags = {libnuma,numactl},
number = {2},
title = {{An NUMA API for Linux}},
url = {http://andikleen.de/numaapi3.pdf},
year = {2004}
}
@article{Ito2013,
author = {Ito, Satoshi and Goto, Kazuya and Ono, Kenji},
doi = {10.1016/j.compfluid.2012.04.024},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ito, Goto, Ono - 2013 - Automatically optimized core mapping to subdomains of domain decomposition method on multicore parallel environm.pdf:pdf},
issn = {00457930},
journal = {Computers \& Fluids},
keywords = {Computational fluid dynamics,High performance computing,Multicore processor,Parallelization,application middleware},
month = jul,
pages = {88--93},
publisher = {Elsevier Ltd},
title = {{Automatically optimized core mapping to subdomains of domain decomposition method on multicore parallel environments}},
volume = {80},
year = {2013}
}
@article{Bolosky1993a,
abstract = {False sharing occurs when processors in a shared-memory parallel system make references to different data objects within the same coherence block (cache line or page), thereby inducing "unnecessary" coherence operations. False sharing is widely believed to be a serious problem for parallel program performance, but a precise definition and quantification of the problem has proven to be elusive. We explain why. In the process, we present a variety of possible definitions for false sharing, and discuss the merits and drawbacks of each. Our discussion is based on experience gained during a fouryear study of multiprocessor memory architecture and its effect on the behavior of applications in a sixteen-program suite.},
author = {Bolosky, William J and Scott, Michael L},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bolosky, Scott - 1993 - False sharing and its effect on shared memory performance.pdf:pdf},
journal = {USENIX Systems on USENIX Experiences},
number = {14520052},
pages = {1--15},
publisher = {USENIX Association},
title = {{False sharing and its effect on shared memory performance}},
volume = {1801},
year = {1993}
}
@phdthesis{mdiener-thesis10,
abstract = {Process placement is a technique widely used on parallel machines with het- erogeneous interconnections to reduce the overall communication time. For in- stance, two processes which communicate frequently are mapped close to each other. Finding the optimal mapping between threads and cores in a shared-memory environment (for example, OpenMP and Pthreads) is a more complex task due to implicit communication. In this work, we examine data sharing patterns between threads in different workloads and use those patterns in a similar way as messages are used to map processes in cluster computers. We evaluated our technique on three state-of-the-art multi-core processors and achieved moderate improvements in the common case and considerable improvements in some cases, reducing exe- cution time by up to 45\%. 8},
author = {Diener, Matthias},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener - 2010 - Evaluating Thread Placement Improvements in Multi-core Architectures.pdf:pdf},
keywords = {cache,memory access patterns,multi-core processor,process mapping,shared,thread placement},
title = {{Evaluating Thread Placement Improvements in Multi-core Architectures}},
year = {2010}
}
@techreport{Pellegrini2010,
author = {Pellegrini, Fran\c{c}ois},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pellegrini - 2010 - Scotch and libScotch 5.1 User's Guide.pdf:pdf},
title = {{Scotch and libScotch 5.1 User's Guide}},
year = {2010}
}
@inproceedings{Jin2004,
abstract = {We describe a new suite of computational benchmarks that models applications featuring multiple levels of par- allelism. Such parallelism is often available in realistic flow computations on systems of meshes, but had not previously been captured in benchmarks. The new suite, named NPB (NAS Parallel Benchmarks) Multi-Zone, is ex- tended from the NPB suite, and involves solving the ap- plication benchmarks LU, BT and SP on collections of loosely coupled discretization meshes. The solutions on the meshes are updated independently, but after each time step they exchange boundary value information. This strat- egy provides relatively easily exploitable coarse-grain parallelism between meshes. Three reference implementations are available: one serial, one hybrid using the Message Passing Interface (MPI) and OpenMP, and another hybrid using a shared memory multi-level programming model (SMP+OpenMP). We examine the effectiveness of hybrid parallelization paradigms in these implementations on three different parallel computers. We also use an empir- ical formula to investigate the performance characteristics of the hybrid parallel codes.},
author = {Jin, Haoqiang and Van derWijngaart, Rob F.},
booktitle = {International Parallel and Distributed Processing Symposium},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jin, Van derWijngaart - 2004 - Performance characteristics of the multi-zone NAS parallel benchmarks.pdf:pdf},
isbn = {0769521320},
keywords = {nas},
mendeley-tags = {nas},
number = {C},
pages = {674--685},
title = {{Performance characteristics of the multi-zone NAS parallel benchmarks}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731505001644},
volume = {00},
year = {2004}
}
@inproceedings{Lai1999,
author = {Lai, An-Chow and Falsafi, Babak},
booktitle = {International Symposium on Computer Architecture (ISCA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lai, Falsafi - 1999 - Memory sharing predictor The key to a speculative coherent DSM.pdf:pdf},
title = {{Memory sharing predictor: The key to a speculative coherent DSM}},
year = {1999}
}
@article{Hamerly2005,
abstract = {This paper describes the new features available in the SimPoint 3.0 release. The release provides two techniques for drastically reducing the run-time of SimPoint: faster searching to find the best clustering, and efficiently clustering large numbers of intervals. SimPoint 3.0 also provides an option to output only the simulation points that represent the majority of execution, which can reduce simulation time without much increase in error. Finally, this release provides support for correctly clustering variable length intervals, taking into consideration the weight of each interval during clustering. This paper describes SimPoint 3.0’s new features, how to use them, and points out some common pitfalls.},
author = {Hamerly, Greg and Perelman, Erez and Lau, Jeremy and Calder, Brad},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Hamerly et al. - 2005 - SimPoint 3.0 Faster and More Flexible Program Phase Analysis.pdf:pdf},
journal = {Journal of Instruction Level Parallelism},
keywords = {Simpoint},
mendeley-tags = {Simpoint},
number = {4},
pages = {1--28},
title = {{SimPoint 3.0: Faster and More Flexible Program Phase Analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.9689\&rep=rep1\&type=pdf},
volume = {7},
year = {2005}
}
@phdthesis{Ribeiro2011,
abstract = {Multi-core platforms with non-uniform memory access (NUMA) design are now a common resource in High Performance Computing. In such platforms, the shared memory is organized in an hierarchical memory subsystem in which the shared memory is physically distributed into several memory banks. Additionally, these platforms feature several levels of cache memories. Because of such hierarchy, memory access latencies may vary depending on the distance between cores and memories. Furthermore, since the number of cores is considerably high in these machines, concurrent accesses to the same memory banks are performed, degrading bandwidth usage. Therefore, a key element in improving the application performance on these machines is dealing with memory affinity. Memory affinity is a relationship between threads and data of application that describes how threads access data. In order to keep memory affinity a compromise between data and thread placement is then necessary. In this context, the main objective of this thesis is to attain scalable performances on multi-core NUMA machines by reducing latencies and increasing memory bandwidth. The first goal of this thesis is to investigate which characteristics of the NUMA platform and the application have an important impact on the memory affinity control and propose mechanisms to deal with them on multi-core machines with NUMA design. We focus on High Performance Scientific Numerical workloads with regular and irregular memory access characteristics. The study of memory affinity aims at the proposal of an environment to manage memory affinity on Multi-core Platforms with NUMA design. This environment provides fine grained mechanisms to manage data placement for an application using the application compile time information, runtime information and architecture characteristics. The second goal is to provide solutions that show performance portability. We mean by performance portability, solutions that are capable of providing similar performances on different NUMA platforms. To do so, we propose mechanisms that are independent of machine architecture and compiler. The portability of the proposed environment is evaluated through the performance analysis of several benchmarks and applications over different platforms. Finally, the third goal of this thesis is to implement memory affinity mechanisms that can be easily adapted and used in different parallel systems. Our approach takes into account the different data structures used in High Performance Scientific Numerical workloads, in order to provide solutions that can be used in different contexts. All the ideas developed in this research work are implemented within a Framework named Minas (Memory affInity maNAgement Software). We evaluate the applicability of such mechanisms in three parallel programming systems, OpenMP, Charm++ and OpenSkel. Additionally, we evaluated Minas performance using several benchmarks and two real world applications from geophysics.},
author = {Ribeiro, Christiane Pousa},
booktitle = {System},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro - 2011 - Contributions on Memory Affinity Management for Hierarchical Shared Memory Multi-core Platforms.pdf:pdf},
keywords = {Hierarchical multiprocessor architectures,affinity,memory management,memory policy,preprocessor$\backslash$},
pages = {183},
title = {{Contributions on Memory Affinity Management for Hierarchical Shared Memory Multi-core Platforms}},
year = {2011}
}
@article{Martin2005,
abstract = {The Wisconsin Multifacet Project has created a sim- ulation toolset to characterize and evaluate the per- formance of multiprocessor hardware systems commonly used as database and web servers. We leverage an existing full-system functional simula- tion infrastructure (Simics [14]) as the basis around which to build a set of timing simulator modules for modeling the timing of the memory system and microprocessors. This simulator infrastructure enables us to run architectural experiments using a suite of scaled-down commercial workloads [3]. To enable other researchers to more easily perform such research, we have released these timing simulator modules as the Multifacet General Execution-driven Multiprocessor Simulator (GEMS) Toolset, release 1.0, under GNU GPL [9].},
author = {Martin, M.M.K. and Sorin, D.J. and Beckmann, B.M. and Marty, M.R. and Xu, Min and Alameldeen, A.R. and Moore, K.E. and Hill, M.D. and Wood, D.A.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Martin et al. - 2005 - Multifacet's general execution-driven multiprocessor simulator (GEMS) toolset.pdf:pdf},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {gem5,gems,simics},
mendeley-tags = {gem5,gems,simics},
number = {4},
pages = {92--99},
publisher = {ACM},
title = {{Multifacet's general execution-driven multiprocessor simulator (GEMS) toolset}},
volume = {33},
year = {2005}
}
@inproceedings{Agarwal2009,
author = {Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K.},
booktitle = {IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
doi = {10.1109/ISPASS.2009.4919636},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Agarwal et al. - 2009 - GARNET A Detailed On-Chip Network Model inside a Full-System Simulator.pdf:pdf},
isbn = {978-1-4244-4184-6},
month = apr,
pages = {33--42},
title = {{GARNET: A Detailed On-Chip Network Model inside a Full-System Simulator}},
year = {2009}
}
@techreport{Verghese1996,
author = {Verghese, Ben},
booktitle = {Young},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Verghese - 1996 - OS Support for Improving Data Locality on CC-NUMA Compute Servers Anoop Gupta Mendel Rosenblum Technical Report CSL-T.pdf:pdf},
number = {February},
title = {{OS Support for Improving Data Locality on CC-NUMA Compute Servers Anoop Gupta Mendel Rosenblum Technical Report : CSL-TR-96-688 February 1996}},
year = {1996}
}
@inproceedings{Grove2001,
author = {Grove, Duncan and Coddington, Paul},
booktitle = {Proceedings of HPC Asia},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Grove, Coddington - 2001 - Precise MPI performance measurement using MPIBench.pdf:pdf},
pages = {24--28},
title = {{Precise MPI performance measurement using MPIBench}},
url = {http://www.dhpc.adelaide.edu.au/reports/104/dhpc-104.pdf},
year = {2001}
}
@inproceedings{Gabriel2004,
abstract = {A large number of MPI implementations are currently avail- able, each of which emphasize different aspects of high-performance com- puting or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and in- fluenced by experience gained from the code bases of the LAM/MPI, LA-MPI, and FT-MPI projects, Open MPI is an all-new, production- quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality im- plementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time compo- sition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI.},
author = {Gabriel, Edgar and Fagg, Graham E. and Bosilca, George and Angskun, Thara and Dongarra, Jack J. and Squyres, Jeffrey M. and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew},
booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Gabriel et al. - 2004 - Open MPI Goals, concept, and design of a next generation MPI implementation.pdf:pdf},
title = {{Open MPI: Goals, concept, and design of a next generation MPI implementation}},
year = {2004}
}
@inproceedings{Lattner2004,
abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
author = {Lattner, Chris and Adve, Vikram},
booktitle = {International Symposium on Code Generation and Optimization (CGO)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lattner, Adve - 2004 - LLVM A compilation framework for lifelong program analysis \& transformation.pdf:pdf},
pages = {75--86},
title = {{LLVM: A compilation framework for lifelong program analysis \& transformation}},
year = {2004}
}
@inproceedings{Broquedis2010a,
abstract = {The now commonplace multi-core chips have introduced, by design, a deep hierarchy of memory and cache banks within parallel computers as a tradeoff between the user friendliness of shared memory on the one side, and memory access scalability and efficiency on the other side. However, to get high performance out of such machines requires a dynamic mapping of application tasks and data onto the underlying architecture. Moreover, depending on the application behavior, this mapping should favor cache affinity, memory bandwidth, computation synchrony, or a combination of these. The great challenge is then to perform this hardware-dependent mapping in a portable, abstract way. To meet this need, we propose a new, hierarchical approach to the execution of OpenMP threads onto multicore machines. Our ForestGOMP runtime system dynamically generates struc- tured trees out of OpenMP programs. It collects relationship information about threads and data as well. This information is used together with scheduling hints and hardware counter feedback by the scheduler to select the most appropriate threads and data distribution. ForestGOMP features a high- level platform for developing and tuning portable threads schedulers. We present several applications for which we developed specific scheduling policies that achieve excellent speedups on 16-core machines.},
annote = {- NAS, Stream Benchmarks},
author = {Broquedis, Fran\c{c}ois and Aumage, Olivier and Goglin, Brice and Thibault, Samuel and Wacrenier, Pierre-Andr and Namyst, Raymond},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Broquedis et al. - 2010 - Structuring the execution of OpenMP applications for multicore architectures.pdf:pdf},
keywords = {mapping,openmp},
mendeley-tags = {mapping,openmp},
pages = {1--10},
title = {{Structuring the execution of OpenMP applications for multicore architectures}},
year = {2010}
}
@misc{Intel2013a,
author = {Intel},
title = {{Intel Trace Analyzer and Collector}},
url = {http://software.intel.com/en-us/intel-trace-analyzer},
year = {2013}
}
@inproceedings{Gaud2014,
author = {Gaud, Fabien and Lepers, Baptiste and Decouchant, Jeremie and Funston, Justin and Fedorova, Alexandra and Quema, Vivien},
booktitle = {Usenix Atc},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Gaud et al. - 2014 - Large Pages May Be Harmful on NUMA Systems.pdf:pdf},
pages = {231--242},
title = {{Large Pages May Be Harmful on NUMA Systems}},
year = {2014}
}
@inproceedings{Moreira2014,
author = {Moreira, Francis B. and Alves, Marco A.Z. and Diener, Matthias and Navaux, Philippe O.A. and Koren, Israel},
booktitle = {International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
doi = {10.1109/SBAC-PAD.2014.19},
file = {:Users/mdiener/Dropbox/Papers/2014-SBAC-BLAP.pdf:pdf},
isbn = {978-1-4799-6905-0},
month = oct,
pages = {222--229},
title = {{Profiling and Reducing Micro-Architecture Bottlenecks at the Hardware Level}},
year = {2014}
}
@inproceedings{Dupros2008,
author = {Dupros, Fabrice and Aochi, Hideo and Ducellier, Ariane and Komatitsch, Dimitri and Roman, Jean},
booktitle = {IEEE International Conference on Computational Science and Engineering (CSE)},
doi = {10.1109/CSE.2008.51},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Dupros et al. - 2008 - Exploiting Intensive Multithreading for the Efficient Simulation of 3D Seismic Wave Propagation.pdf:pdf},
isbn = {978-0-7695-3193-9},
pages = {253--260},
title = {{Exploiting Intensive Multithreading for the Efficient Simulation of 3D Seismic Wave Propagation}},
year = {2008}
}
@inproceedings{Boneti2008,
abstract = {Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mech- anisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior.We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mech- anism that allows us to bias the processor resource alloca- tion. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16\%). Moreover, our solution reduces the application’s execution time combining effect of load balance and high responsive scheduling.},
author = {Boneti, C. and Gioiosa, R. and Cazorla, F.J. and Valero, M.},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/SC.2008.5217785},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Boneti et al. - 2008 - A dynamic scheduler for balancing HPC applications.pdf:pdf},
isbn = {978-1-4244-2834-2},
month = nov,
number = {November},
pages = {1--12},
publisher = {Ieee},
title = {{A dynamic scheduler for balancing HPC applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5217785},
year = {2008}
}
@inproceedings{Thoziyoor2008a,
abstract = {In this paper we introduce CACTI-D, a significant enhancement of CACTI 5.0. CACTI-D adds support for modeling of commodity DRAM technology and support for main memory DRAM chip organization. CACTI-D enables modeling of the complete memory hierarchy with consistent models all the way from SRAM based L1 caches through main memory DRAMs on DIMMs. We illustrate the potential applicability of CACTI-D in the design and analysis of future memory hierarchies by carrying out a last level cache study for a multicore multithreaded architecture at the 32nm technology node. In this study we use CACTI-D to model all components of the memory hierarchy including L1, L2, last level SRAM, logic process based DRAM or commodity DRAM L3 caches, and main memory DRAM chips. We carry out architectural simulation using benchmarks with large data sets and present results of their execution time, breakdown of power in the memory hierarchy, and system energy-delay product for the different system configurations. We find that commodity DRAM technology is most attractive for stacked last level caches, with significantly lower energy-delay products.},
author = {Thoziyoor, Shyamkumar and Ahn, Jung Ho and Monchiero, Matteo and Brockman, Jay B. and Jouppi, Norman P.},
booktitle = {International Symposium on Computer Architecture (ISCA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Thoziyoor et al. - 2008 - A Comprehensive Memory Modeling Tool and Its Application to the Design and Analysis of Future Memory Hierarchi.pdf:pdf},
keywords = {cacti},
mendeley-tags = {cacti},
pages = {51--62},
title = {{A Comprehensive Memory Modeling Tool and Its Application to the Design and Analysis of Future Memory Hierarchies}},
year = {2008}
}
@article{Sumant2010,
abstract = {The 2.6 Linux kernel employs a number of techniques to improve the use of large amounts of memory, making Linux more enterprise-ready than ever before. This article outlines a few of the more important changes, including reverse mapping, how reverse mapping is used for page reclaim, the use of larger memory pages, storage of page-table entries in high memory,kernel shared memory increases efficiency and flexibility and greater stability of the memory manager.},
annote = {bad paper},
author = {Sumant, Archana S and Chawan, Pramila M},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Sumant, Chawan - 2010 - Virtual Memory Management Techniques in 2.6 Kernel and Challenges.pdf:pdf},
journal = {IACSIT International Journal of Engineering and Technology},
keywords = {Copy on Write (CoW),page reclaiming,page-direct approach,page-table entries (PTEs),reverse mapping(RMAP),translation lookaside buffer (TLB)},
number = {2},
pages = {157--160},
title = {{Virtual Memory Management Techniques in 2.6 Kernel and Challenges}},
volume = {2},
year = {2010}
}
@inproceedings{Buntinas2006,
abstract = {This paper presents the implementation of MPICH2 over the Nemesis communication subsystem and the evaluation of its shared-memory performance. We describe design issues as well as some of the optimization techniques we employed. We conducted a performance eval- uation over shared memory using microbenchmarks as well as application benchmarks. The evaluation shows that MPICH2 Nemesis has very low communication overhead, making it suitable for smaller-grained applications.},
author = {Buntinas, Darius and Mercier, Guillaume and Gropp, William},
booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Buntinas, Mercier, Gropp - 2006 - Implementation and shared-memory evaluation of MPICH2 over the Nemesis communication subsystem.pdf:pdf},
title = {{Implementation and shared-memory evaluation of MPICH2 over the Nemesis communication subsystem}},
year = {2006}
}
@inproceedings{Cappello2000,
abstract = {The hybrid memory model of clusters of multiprocessors raises two issues: programming model and performance. Many parallel programs have been written by using the MPI standard. To evaluate the pertinence of hybrid models for existing MPI codes, we compare a unified model (MPI) and a hybrid one (OpenMP fine grain parallelization after profiling) for the NAS 2.3 benchmarks on two IBM SP sys- tems. The superiority of one model depends on 1) the level of shared memory model parallelization, 2) the communi- cation patterns and 3) the memory access patterns. The relative speeds of the main architecture components (CPU, memory, and network) are of tremendous importance for selecting one model. With the used hybrid model, our re- sults show that a unified MPI approach is better for most of the benchmarks. The hybrid approach becomes better only when fast processors make the communication performance significant and the level of parallelization is sufficient},
author = {Cappello, Franck and Etiemble, Daniel},
booktitle = {ACM/IEEE conference on Supercomputing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cappello, Etiemble - 2000 - MPI versus MPI OpenMP on the IBM SP for the NAS Benchmarks.pdf:pdf},
isbn = {0780398025},
title = {{MPI versus MPI+ OpenMP on the IBM SP for the NAS Benchmarks}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1592725},
year = {2000}
}
@inproceedings{Ziakas2010,
author = {Ziakas, Dimitrios and Baum, Allen and Maddox, Robert a. and Safranek, Robert J.},
booktitle = {Symposium on High Performance Interconnects},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ziakas et al. - 2010 - Intel® QuickPath Interconnect Architectural Features Supporting Scalable System Architectures.pdf:pdf},
keywords = {intel,nehalem,qpi,quickpath,scalable,xeon},
month = aug,
pages = {1--6},
title = {{Intel QuickPath Interconnect - Architectural Features Supporting Scalable System Architectures}},
year = {2010}
}
@inproceedings{Traff2002,
abstract = {The topology functionality of the Message Passing Interface (MPI) provides a portable, architecture-independent means for adapting application programs to the communication architecture of the target hardware. However, current MPI implemen- tations rarely go beyond the most trivial implementation, and simply performs no process remapping. We discuss the potential of the topology mechanism for systems with a hierarchical communication architecture like clusters of SMP nodes. The MPI topology functional- ity is a weak mechanism, and we argue about some of its shortcomings. We formulate the topology optimization problem as a graph embedding problem, and show that for hierarchical systems it can be solved by graph partitioning. We state the properties of a new heuristic for solving both the embedding problem and the “easier” graph partitioning problem. The graph partitioning based framework has been fully implemented in MPI/SX for the NEC SX-series of parallel vector computers. MPI/SX is thus one of very few MPI implementations with a non-trivial topology functionality. On a 4 node NEC SX-6 significant communication performance improvements are achieved with synthetic MPI benchmarks.},
author = {Tr\"{a}ff, Jesper Larsson},
booktitle = {ACM/IEEE Conference on Supercomputing (SC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Tr\"{a}ff - 2002 - Implementing the MPI Process Topology Mechanism(2).pdf:pdf},
isbn = {076951524X},
keywords = {graph partitioning},
mendeley-tags = {graph partitioning},
pages = {1--14},
title = {{Implementing the MPI Process Topology Mechanism}},
year = {2002}
}
@inproceedings{Bienia2008,
abstract = {This paper presents and characterizes the Princeton Ap- plication Repository for Shared-Memory Computers (PAR- SEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiproces- sors have focused on high-performance computing applica- tions and used a limited number of synchronization meth- ods. PARSEC includes emerging applications in recogni- tion, mining and synthesis (RMS) as well as systems appli- cations which mimic large-scale multi-threaded commercial programs. Our characterization shows that the benchmark suite is diverse in working set, locality, data sharing, syn- chronization, and off-chip traffic. The benchmark suite has been made available to the public.},
author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bienia et al. - 2008 - The PARSEC Benchmark Suite Characterization and Architectural Implications.pdf:pdf},
keywords = {benchmark suite,multi-threading,performance measurement,shared-memory computers},
pages = {72--81},
title = {{The PARSEC Benchmark Suite: Characterization and Architectural Implications}},
year = {2008}
}
@article{Ogasawara2009,
abstract = {We propose a novel online method of identifying the pre- ferred NUMA nodes for objects with negligible overhead during the garbage collection time as well as object allo- cation time. Since the number of CPUs (or NUMA nodes) is increasing recently, it is critical for the memory manager of the runtime environment of an object-oriented language to exploit the low latency of local memory for high perfor- mance. To locate the CPU of a thread that frequently ac- cesses an object, prior research uses the runtime information about memory accesses as sampled by the hardware. How- ever, the overhead of this approach is high for a garbage col- lector. Our approach uses the information about which thread can exclusively access an object, or the Dominant Thread (DoT). The dominant thread of an object is the thread that often most accesses an object so that we do not require memory access samples. Our NUMA-aware GC performs DoT based object copying, which copies each live object to the CPU where the dominant thread was last dispatched before GC. The dominant thread information is known from the thread stack and from objects that are locked or reserved by threads and is propagated in the object reference graph. We demonstrate that our approach can improve the per- formance of benchmark programs such as SPECpower ssj2008, SPECjbb2005, and SPECjvm2008.We prototyped aNUMA- aware memory manager on a modified version of IBM Java VM and tested it on a cc-NUMA POWER6 machine with eight NUMA nodes. Our NUMA-aware GC achieved per- formance improvements up to 14.3\% and 2.0\% on average over a JVM that only used the NUMA-aware allocator. The Permission},
author = {Ogasawara, Takeshi},
doi = {10.1145/1639949.1640117},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ogasawara - 2009 - NUMA-Aware Memory Manager with Dominant-Thread-Based Copying GC.pdf:pdf},
isbn = {9781605587349},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
month = oct,
number = {10},
pages = {377--389},
title = {{NUMA-Aware Memory Manager with Dominant-Thread-Based Copying GC}},
volume = {44},
year = {2009}
}
@misc{Argonne2014,
author = {{Argonne National Laboratory}},
keywords = {binding,round robin},
mendeley-tags = {binding,round robin},
title = {{Using the Hydra Process Manager}},
url = {http://wiki.mpich.org/mpich/index.php/Using\_the\_Hydra\_Process\_Manager},
year = {2014}
}
@article{Snavely2000,
abstract = {Simultaneous Multithreading machines fetch and execute instructions from multiple instruction streams to increase system utilization and speedup the execution of jobs. When there are more jobs in the system than there is hardware to support simultaneous execution, the operating system scheduler must choose the set of jobs to coscheduleThis paper demonstrates that performance on a hardware multithreaded processor is sensitive to the set of jobs that are coscheduled by the operating system jobscheduler. Thus, the full benefits of SMT hardware can only be achieved if the scheduler is aware of thread interactions. Here, a mechanism is presented that allows the scheduler to significantly raise the performance of SMT architectures. This is done without any advance knowledge of a workload's characteristics, using sampling to identify jobs which run well together.We demonstrate an SMT jobscheduler called SOS. SOS combines an overhead-free sample phase which collects information about various possible schedules, and a symbiosis phase which uses that information to predict which schedule will provide the best performance. We show that a small sample of the possible schedules is sufficient to identify a good schedule quickly. On a system with random job arrivals and departures, response time is improved as much as 17\% over a schedule which does not incorporate symbiosis.},
author = {Snavely, Allan and Tullsen, D.M.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Snavely, Tullsen - 2000 - Symbiotic jobscheduling for a simultaneous mutlithreading processor.pdf:pdf},
journal = {ACM SIGPLAN Notices},
number = {11},
pages = {234--244},
publisher = {ACM},
title = {{Symbiotic jobscheduling for a simultaneous mutlithreading processor}},
url = {http://dl.acm.org/citation.cfm?id=357011},
volume = {35},
year = {2000}
}
@inproceedings{Rodrigues2009a,
abstract = {We propose an approach to reduce the execution time of applications with a steady communication pattern on clusters of multi-core processors by leveraging the asymmetry of core communication speeds. In addition to the well known fact that communication link speeds on a fixed cluster vary with processor selection, we consider one effect of multicore processor chips: link speeds vary with core selection within a single processor chip. The approach requires measuring link speeds among cluster cores as well as communication volumes and computational loads of the selected application processes. This data is fed into the dual recursive bipartitioning method to obtain close to optimal application process placement on cluster cores. We apply this approach to a real world application achieving sensible execution time reduction without even recompiling source code.},
author = {Rodrigues, Eduardo R. and Madruga, Felipe L. and Navaux, Philippe O. A. and Panetta, Jairo},
booktitle = {IEEE Symposium on Computers and Communications (ISCC)},
doi = {10.1109/ISCC.2009.5202271},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Rodrigues et al. - 2009 - Multi-core aware process mapping and its impact on communication overhead of parallel applications.pdf:pdf},
isbn = {978-1-4244-4672-8},
keywords = {process mapping},
mendeley-tags = {process mapping},
title = {{Multi-core aware process mapping and its impact on communication overhead of parallel applications}},
year = {2009}
}
@phdthesis{Cruz2012a,
abstract = {communication between them. In parallel applications based on the shared memory paradigm, the communication is implicit and occurs through accesses to shared variables, which makes difficult to detect the communication pattern between the threads. Traditional approaches use simulation to monitor the memory accesses performed by the application, requiring modifications to the source code and drastically increasing the overhead. In this master thesis, we introduce two novel light-weight mechanisms to find the communication pattern of threads. The first mechanism makes use of the information about shared cache lines provided by cache coherence protocols. The second mechanism makes use of the Translation Lookaside Buffer (TLB) to detect which memory pages each core is accessing. Both our mechanisms rely entirely on hardware features, which makes the thread mapping transparent to the programmer and allows it to be performed dynami- cally by the operating system. Moreover, no time consuming task, such as simulation, is required. We evaluated our mechanisms with theNAS Parallel Benchmarks (NPB) and obtained accurate representations of the communication patterns. We generated thread mappings from the detected communication patterns using a mapping algorithm. Mapping is a NP-Hard problem. Therefore, in order to achieve a polynomial complexity, we designed a heuristic method based on the Edmonds graph matching algorithm. Running the ap- plications with these mappings resulted in performance improvements of up to 15.3\% compared to the original scheduler of the operating system. The number of cache misses, cache line invalidations and snoop transactions were reduced by up to 31.9\%, 41\% and 65.4\%, respectively.},
author = {Cruz, Eduardo H.M.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz - 2012 - Dynamic Detection of the Communication Pattern in Shared Memory Environments for Thread Mapping.pdf:pdf},
title = {{Dynamic Detection of the Communication Pattern in Shared Memory Environments for Thread Mapping}},
year = {2012}
}
@inproceedings{Hamerly2003,
abstract = {Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of a single industry standard benchmark at this level of detail takes on the order of months to complete. This problem is exacerbated by the fact that to properly perform an architectural evaluation requires multiple benchmarks to be evaluated across many separate runs. To ad- dress this issue we recently created a tool called SimPoint that automatically finds a small set of Simulation Points to represent the complete execution of a program for efficient and accurate simulation. In this paper we describe how to use the SimPoint tool, and introduce an improved SimPoint algorithm designed to significantly reduce the simulation time required when the simu- lation environment relies upon fast-forwarding.},
author = {Perelman, Erez and Hamerly, Greg and {Van Biesbrouck}, Michael and Sherwood, Timothy and Calder, Brad},
booktitle = {ACM SIGMETRICS Performance Evaluation Review},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Perelman et al. - 2003 - Using SimPoint for Accurate and Efficient Simulation Erez Perelman.pdf:pdf},
keywords = {Simpoint,clustering,fast-forwarding,sampling,simpoint,simulation},
mendeley-tags = {Simpoint},
pages = {318--319},
title = {{Using SimPoint for Accurate and Efficient Simulation Erez Perelman}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.6942\&rep=rep1\&type=pdf},
year = {2003}
}
@article{Klug2008,
abstract = {In this paper we present a framework for automatic detection and application of the best binding between threads of a running paral- lel application and processor cores in a shared memory system, by mak- ing use of hardware performance counters. This is especially important within the scope of multicore architectures with shared cache levels. We demonstrate that many applications from the SPEC OMP benchmark show quite sensitive runtime behavior depending on the thread/core binding used. In our tests, the proposed framework is able to find the best binding in nearly all cases. The proposed framework is intended to supplement job scheduling systems for better automatic exploitation of systems with multicore processors, as well as making programmers aware of this issue by providing measurement logs.},
author = {Klug, Tobias and Ott, Michael and Weidendorfer, Josef and Trinitis, Carsten},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Klug et al. - 2008 - autopin -- Automated Optimization of Thread-to-Core Pinning on Multicore Systems.pdf:pdf},
journal = {High Performance Embedded Architectures and Compilers},
keywords = {automatic performance optimization,cmp,cpu binding,hardware performance counters,multicore,thread placement},
number = {4},
pages = {219--235},
title = {{autopin -- Automated Optimization of Thread-to-Core Pinning on Multicore Systems}},
volume = {3},
year = {2008}
}
@techreport{Intel2010,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2010 - Intel® Xeon® Processor 7500 Series.pdf:pdf},
keywords = {nehalem,nehalem-ex,turing,x7550},
mendeley-tags = {nehalem,nehalem-ex,turing,x7550},
number = {March},
title = {{Intel® Xeon® Processor 7500 Series}},
year = {2010}
}
@inproceedings{Desikan2001,
abstract = {We measure the experimental error that arises from the use of non-validated simulators in computer architecture research, with the goal of increasing the rigor of simula- tion-based studies. We describe the methodology that we used to validate a microprocessor simulator against a Compaq DS-10L workstation, which contains an Alpha 21264 processor. Our evaluation suite consists of a set of 21 microbenchmarks that stress different aspects of the 21264 microarchitecture. Using the microbenchmark suite as the set of workloads, we describe how we reduced our simulator error to an arithmetic mean of 2\%, and include details about the specific aspects of the pipeline that required extra care to reduce the error.We show how these low-level optimizations reduce average error from 40\% to less than 20\% on macrobenchmarks drawn from the SPEC2000 suite. Finally, we examine the degree to which performance optimizations are stable across different sim- ulators, showing that researchers would draw different conclusions, in some cases, if using validated simulators.},
author = {Desikan, R. and Burger, D. and Keckler, S.W.},
booktitle = {28th Annual International Symposium on Computer Architecture},
doi = {10.1109/ISCA.2001.937455},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Desikan, Burger, Keckler - 2001 - Measuring experimental error in microprocessor simulation.pdf:pdf},
isbn = {0-7695-1162-7},
pages = {266--277},
title = {{Measuring experimental error in microprocessor simulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=937455},
year = {2001}
}
@inproceedings{Hore2010,
author = {Hore, Alain and Ziou, Djemel},
booktitle = {International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.579},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Hore, Ziou - 2010 - Image Quality Metrics PSNR vs. SSIM.pdf:pdf},
isbn = {978-1-4244-7542-1},
keywords = {image quality metrics,mse,psnr,ssim},
mendeley-tags = {mse},
month = aug,
pages = {2366--2369},
title = {{Image Quality Metrics: PSNR vs. SSIM}},
year = {2010}
}
@inproceedings{Heiss1992,
abstract = {We consider the task allocation problem for a homogeneous, multiprogranuned MIMD multicomputer system. Programs arrive in a Poisson stream, and are given as so-eaUed phase graphs. Each phase is described by a task interaction graph. We propose an O(n 2) heuristic allocation algorithm where n is the number of tasks. The algorithm has two parts, the first independent of, the second dependent on the topology of the multicomputer. The first part performs a linear hierarchic clustering of the tasks which is used by the second part to map clusters of suitable size onto free parts of the processor graph. The algorithm is evaluated by simulation for a binary tree topology.},
author = {Heiss, Hans-Ulrich and Wiesenfarth, Rainer},
booktitle = {International ACPC Conference on Parallel Computation},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Heiss, Wiesenfarth - 1991 - A Heuristic Algorithm for Dynamic Task Allocation in Highly Parallel Systems.pdf:pdf},
keywords = {distributed systems,highly parallel systems,mapping problem,task allocation,task interaction graph,tig},
mendeley-tags = {task interaction graph,tig},
pages = {252--265},
title = {{A Heuristic Algorithm for Dynamic Task Allocation in Highly Parallel Systems}},
year = {1991}
}
@inproceedings{Majo2013,
author = {Majo, Zoltan and Gross, Thomas R.},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2013.6704666},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Majo, Gross - 2013 - (Mis)Understanding the NUMA Memory System Performance of Multithreaded Workloads(2).pdf:pdf},
isbn = {978-1-4799-0555-3},
keywords = {dedup,ferret,parsec,streamcluster},
mendeley-tags = {dedup,ferret,parsec,streamcluster},
pages = {11--22},
title = {{(Mis)Understanding the NUMA Memory System Performance of Multithreaded Workloads}},
year = {2013}
}
@inproceedings{Diener2014,
author = {Diener, Matthias and Cruz, Eduardo H. M. and Navaux, Philippe O. A. and Busse, Anselm and Hei\ss, Hans-Ulrich},
booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Diener et al. - 2014 - kMAF Automatic Kernel-Level Management of Thread and Data Affinity.pdf:pdf},
keywords = {cache hierarchies,data affinity,numa,thread affinity},
title = {{kMAF: Automatic Kernel-Level Management of Thread and Data Affinity}},
year = {2014}
}
@article{Goglin2013,
abstract = {The multiplication of cores in today’s architectures raises the importance of intra-node communication in modern clusters and their impact on the overall parallel application performance. Although several proposals focused on this issue in the past, there is still a need for a portable and hardware-independent solution that addresses the requirements of both point-to-point and collective MPIoperations inside shared-memory computing nodes. This paper presents the KNEM module for the Linux kernel that provides MPI implementations with a flexible and scalable interface for performing kernel-assisted single-copy data transfers between local processes. It enables high-performance communication within most existing MPI implementations and brings significant application performance improvements thanks to more efficient point-to-point and collective operations.},
author = {Goglin, Brice and Moreaud, St\'{e}phanie},
doi = {10.1016/j.jpdc.2012.09.016},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Goglin, Moreaud - 2013 - KNEM A generic and scalable kernel-assisted intra-node MPI communication framework.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = feb,
number = {2},
pages = {176--188},
publisher = {Elsevier Inc.},
title = {{KNEM: A generic and scalable kernel-assisted intra-node MPI communication framework}},
volume = {73},
year = {2013}
}
@inproceedings{Song2009,
author = {Song, Fengguang and Moore, Shirley and Dongarra, Jack},
booktitle = {International Conference on Cluster Computing and Workshops (CLUSTER)},
doi = {10.1109/CLUSTR.2009.5289173},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Song, Moore, Dongarra - 2009 - Analytical modeling and optimization for affinity based thread scheduling on multicore systems.pdf:pdf},
isbn = {978-1-4244-5011-4},
title = {{Analytical modeling and optimization for affinity based thread scheduling on multicore systems}},
year = {2009}
}
@article{Marathe2010,
abstract = {Non-uniform memory architectures with cache coherence (ccNUMA) are becoming in- creasingly common, not just for large-scale high performance platforms but also in the context of multi-cores architectures. Under ccNUMA, data placement may influence over- all application performance significantly as references resolved locally to a processor/core impose lower latencies than remote ones. This work develops a novel hardware-assisted page placement paradigm based on auto- mated tracing of the memory references made by application threads. Two placement schemes, modeling both single-level and multi-level latencies, allocate pages near proces- sors that most frequently access that memory page. These schemes leverage performance monitoring capabilities of contemporary microprocessors to efficiently extract an approx- imate trace of memory accesses. This information is used to decide page affinity, i.e., the node to which the page is bound. The method operates entirely in user space, is widely automated, and handles not only static but also dynamic memory allocation. Experiments show that this method, although based on lossy tracing, can efficiently and effectively improve page placement, leading to an average wall-clock execution time saving of over 20\%for the tested benchmarks on the SGIAltixwith a 2x remote access penalty and 12\% on AMD Opterons with a 1.3-2.0x access penalty. This is accompanied by a one-time tracing overhead of 2.7\% over the overall original program wallclock time.},
author = {Marathe, Jaydeep and Thakkar, Vivek and Mueller, Frank},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Marathe, Thakkar, Mueller - 2010 - Feedback-Directed Page Placement for ccNUMA via Hardware-generated Memory Traces.pdf:pdf},
journal = {Journal of Parallel and Distributed Computing},
keywords = {hardware performance monitoring,numa,page,trace-guided optimization},
number = {12},
pages = {1204--1219},
title = {{Feedback-Directed Page Placement for ccNUMA via Hardware-generated Memory Traces}},
volume = {70},
year = {2010}
}
@misc{Meter2009,
author = {Meter, Rodney Van},
keywords = {memory wall},
mendeley-tags = {memory wall},
title = {{Memory: Caching and Memory Hierarchy}},
url = {http://web.sfc.keio.ac.jp/~rdv/keio/sfc/teaching/architecture/architecture-2009/lec08-cache.html},
year = {2009}
}
@techreport{Jin1999,
author = {Jin, H and Frumkin, M and Yan, J},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jin, Frumkin, Yan - 1999 - The OpenMP implementation of NAS Parallel Benchmarks and Its Performance.pdf:pdf},
keywords = {nas,nas parallel benchmarks,openmp program,shared memory system},
mendeley-tags = {nas},
number = {October},
title = {{The OpenMP implementation of NAS Parallel Benchmarks and Its Performance}},
year = {1999}
}
@inproceedings{Trahay2011,
abstract = {Modern supercomputers with multi-core nodes en- hanced by accelerators, as well as hybrid programming models, introduce more complexity in modern applications. Exploiting efficiently all the resources requires a complex analysis of the performance of applications in order to detect time-consuming or idle sections. This paper presents EZTRACE, a generic trace generation framework that aims at providing a simple way to analyze applications. EZTRACE is based on plugins that allow it to trace different programming models such as MPI, pthread or OpenMP as well as user-defined libraries or application. This framework uses two steps: one to collect the basic information during execution and one post-mortem analysis. This permits tracing the execution of applications with low overhead while allowing to refine the analysis after the execution of the program. We also present a simple script language for EZTRACE that gives the user the capability to easily define the functions to instrument without modifying the source code of the application. The evaluation of EZTRACE shows that the framework offers a convenient way to analyze applications without impacting the execution.},
author = {Trahay, Fran\c{c}ois and Rue, Fran\c{c}ois and Faverge, Mathieu and Ishikawa, Yutaka and Namyst, Raymond and Dongarra, Jack},
booktitle = {International Symposium on Cluster, Cloud and Grid Computing (CCGrid)},
doi = {10.1109/CCGrid.2011.83},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Trahay et al. - 2011 - EZTrace a generic framework for performance analysis.pdf:pdf},
isbn = {978-1-4577-0129-0},
month = may,
title = {{EZTrace: a generic framework for performance analysis}},
year = {2011}
}
@article{Soryani2013,
author = {Soryani, Mohsen and Analoui, Morteza and Zarrinchian, Ghobad},
doi = {10.1007/s11227-013-0918-7},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Soryani, Analoui, Zarrinchian - 2013 - Improving inter-node communications in multi-core clusters using a contention-free process mappin.pdf:pdf},
issn = {0920-8542},
journal = {The Journal of Supercomputing},
keywords = {communications,high performance clusters,inter-node,network interface,process mapping algorithm},
month = apr,
number = {1},
pages = {488--513},
title = {{Improving inter-node communications in multi-core clusters using a contention-free process mapping algorithm}},
volume = {66},
year = {2013}
}
@article{Cuesta2013,
author = {Cuesta, Blas and Ros, Alberto and Gomez, Maria E. and Robles, Antonio and Duato, Jose},
doi = {10.1109/TC.2011.241},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cuesta et al. - 2013 - Increasing the Effectiveness of Directory Caches by Avoiding the Tracking of Non-Coherent Memory Blocks.pdf:pdf},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
number = {3},
pages = {482--495},
title = {{Increasing the Effectiveness of Directory Caches by Avoiding the Tracking of Non-Coherent Memory Blocks}},
volume = {62},
year = {2013}
}
@inproceedings{Mazouz2011,
abstract = {With the introduction of multi-core processors, thread affinity has quickly appeared to be one of the most important factors to accelerate program execution times. The current article presents a complete experimental study on the per- formance of various thread pinning strategies. We investi- gate four application independent thread pinning strategies and five application sensitive ones based on cache sharing. We made extensive performance evaluation on three different multi-core machines reflecting three usual utilisa- tion: workstation machine, server machine and high per- formance machine. In overall, we show that fixing thread affinities (whatever the tested strategy) is a better choice for improving program performance on HPC ccNUMA machines compared to OS-based thread placement. This means that the current Linux OS scheduling strategy is not necessarily the best choice in terms of performance on ccNUMA machines, even if it is a good choice in terms of cores usage ratio and work balancing. On smaller Core2 and Nehalem machines, we show that the benefit of thread pinning is not satisfactory in terms of speedups versus OS- based scheduling, but the performance stability is much better.},
author = {Mazouz, Abdelhafid and Touati, Sid-Ahmed-Ali and Barthou, Denis},
booktitle = {International Conference on High Performance Computing Simulation},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mazouz, Touati, Barthou - 2011 - Performance Evaluation and Analysis of Thread Pinning Strategies on Multi-Core Platforms Case Study of.pdf:pdf},
isbn = {9781612843834},
keywords = {multi-cores,openmp,operating systems,thread affinity,thread level parallelism},
pages = {273--279},
title = {{Performance Evaluation and Analysis of Thread Pinning Strategies on Multi-Core Platforms: Case Study of SPEC OMP Applications on Intel Architectures}},
year = {2011}
}
@article{Villa2005,
abstract = {Nowadays, the use of multiprocessor systems is not just limited to typical scientific applications, but these systems are increasingly being used for executing commercial applications, such as databases and web servers. Therefore, it becomes essential to study the behavior of multiprocessor architectures under commercial workloads. To accomplish this, we need simulators able to model not only the CPU, memory and interconnection network but also other aspects that are critical in the execution of commercial workloads, such as I/O subsystem and operating system. In this paper, we present our first experiences using Simics, a simulator which allows full-system simulation of multiprocessor archi- tectures covering all the topics previously mentioned. Using Simics we carry out a detailed performance study of a static web content server, showing how changes in some architectural parameters, such as number of processors and cache size, affect final performance. The results we have obtained corroborate the intuition of increasing performance of a dual-processor web server opposite to a single-processor one, and at the same time, allow us to check out Simics lim- itations. Finally, we compare these results with those that are obtained on real machines.},
author = {Villa, F and Acacio, M and Garcia, J},
doi = {10.1016/j.sysarc.2004.09.003},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Villa, Acacio, Garcia - 2005 - Evaluating IA-32 web servers through simics a practical experience.pdf:pdf},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {commercial applications,full system simulators,multiprocessor systems,simics},
month = apr,
number = {4},
pages = {251--264},
title = {{Evaluating IA-32 web servers through simics: a practical experience}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1383762104001225},
volume = {51},
year = {2005}
}
@article{Dagum1998,
author = {Dagum, L. and Menon, R.},
doi = {10.1109/99.660313},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Dagum, Menon - 1998 - OpenMP an industry standard API for shared-memory programming.pdf:pdf},
issn = {10709924},
journal = {IEEE Computational Science and Engineering},
keywords = {openmp},
mendeley-tags = {openmp},
number = {1},
pages = {46--55},
title = {{OpenMP: an industry standard API for shared-memory programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=660313},
volume = {5},
year = {1998}
}
@inproceedings{Lattner2011,
abstract = {This talk introduces LLVM, giving a brief sense for its library based design. It then dives into Clang to describe the end-user benefits of LLVM compiler technology, finally wrapping up with mentions of Clang Static Analyzer, LLDB, libc++ and the LLVM MC projects.},
author = {Lattner, Chris},
booktitle = {Free and Open Source Developers European Meeting (FOSDEM)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lattner - 2011 - LLVM and Clang Advancing Compiler Technology.pdf:pdf},
title = {{LLVM and Clang: Advancing Compiler Technology}},
year = {2011}
}
@inproceedings{Cruz2010,
abstract = {Process mapping is a technique widely used in parallel machines to provide performance gains by improving the use of resources such as interconnections and cache me- mory hierarchy. The problem to find the best mapping is considered NP-Hard and, in shared memory environments, there is the additional difficulty to find the communication pattern, which is implicit and occurs through memory ac- cesses. In this context, this work aims to improve the perfor- mance of parallel applications that use shared memory. For that, it was developed a method for analysis of the shared memory which identifies the mapping without requiring any previous knowledge of the application behavior. Applicati- ons from the NAS Parallel Benchmarks (NPB) were used in these experiments, showing performance gains of up to 42\% compared to the native scheduler of the operating system.},
author = {Cruz, Eduardo H M and Alves, Marco A Z and Navaux, Philippe O A},
booktitle = {Symposium on Computing Systems (WSCAD-SCC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz, Alves, Navaux - 2010 - Process Mapping Based on Memory Access Traces.pdf:pdf},
keywords = {Benchmark,Mapping},
mendeley-tags = {Benchmark,Mapping},
pages = {72--79},
title = {{Process Mapping Based on Memory Access Traces}},
year = {2010}
}
@inproceedings{Nikolopoulos2000,
abstract = {This paper presents algorithms for improving the performance of parallel programs on multiprogrammed shared-memory NUMA multiprocessors, via the use of user-level dynamic page migration. The idea that drives the algorithms is that a page migration engine can perform accurate and timely page migrations in a multiprogrammed system if it can correlate page reference information with scheduling information obtained from the operating system. The necessary page migrations can be performed as a response to scheduling events that break the implicit association between threads and their memory affinity sets. We present two algorithms that use feedback from the kernel scheduler to aggressively migrate pages upon thread migrations. The first algorithm exploits the iterative nature of parallel programs, while the second targets generic codes without making assumptions on their structure. Performance evaluation on an SGI Origin2000 shows that our page migration algorithms provide substantial improvements in throughput of up to 264\% compared to the native IRIX 6.5.5 page placement and migration schemes},
author = {Nikolopoulos, D.S. and Papatheodorou, T.S. and Polychronopoulos, C.D. and Labarta, J. and Ayguade, E.},
booktitle = {International Conference on Parallel Processing (ICPP)},
doi = {10.1109/ICPP.2000.876083},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Nikolopoulos et al. - 2000 - User-level dynamic page migration for multiprogrammed shared-memory multiprocessors.pdf:pdf},
isbn = {0-7695-0768-9},
issn = {0190-3918},
pages = {95--103},
title = {{User-level dynamic page migration for multiprogrammed shared-memory multiprocessors}},
year = {2000}
}
@inproceedings{Kassick2011,
abstract = {This paper presents the use of trace-based performance visualization of a large scale atmospheric model, the Ocean-Land-Atmosphere Model (OLAM). The trace was obtained with the libRastro library, and the visualization was done with Paj\'{e}. The use of visualization aimed to analyze OLAM’s performance and to identify its bottlenecks. Especially, we are interested in the model’s I/O operations, since it was proved to be the main issue for the model’s performance. We show that most of the time spent in the output routine is spent in the close operation. With this information, we delayed this operation until the next output phase, obtaining improved I/O performance.},
author = {Kassick, Rodrigo Virote and Boito, Francieli Zanon and Diener, Matthias and Navaux, Philippe O. A. and Denneulin, Yves and Schepke, Claudio and Maillard, Nicolas and Osthoff, Carla and Grunmann, Pablo and Dias, Pedro and Panetta, Jairo},
booktitle = {Workshop on Architecture and Multi-Core Applications (WAMCA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kassick et al. - 2011 - Trace-based Visualization as a Tool to Understand Applications' IO Performance in Multi-Core Machines.pdf:pdf},
pages = {5--11},
title = {{Trace-based Visualization as a Tool to Understand Applications' I/O Performance in Multi-Core Machines}},
year = {2011}
}
@article{Bolosky1991a,
author = {Bolosky, William J. and Scott, Michael L. and Fitzgerald, Robert P. and Fowler, Robert J. and Cox, Alan L.},
doi = {10.1145/106975.106994},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bolosky et al. - 1991 - NUMA policies and their relation to memory architecture.pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {2},
pages = {212--221},
title = {{NUMA policies and their relation to memory architecture}},
volume = {19},
year = {1991}
}
@inproceedings{Pearce2012,
abstract = {Load balance is critical for performance in large parallel applica- tions. An imbalance on today’s fastest supercomputers can force hundreds of thousands of cores to idle, and on future exascale ma- chines this cost will increase by over a factor of a thousand. Im- proving load balance requires a detailed understanding of the amount of computational load per process and an application’s simulated domain, but no existing metrics sufficiently account for both fac- tors. Current load balance mechanisms are often integrated into applications and make implicit assumptions about the load. Some strategies place the burden of providing accurate load information, including the decision on when to balance, on the application. Ex- isting application-independent mechanisms simply measure the ap- plication load without any knowledge of application elements, which limits them to identifying imbalance without correcting it. Our novel load model couples abstract application information with scalable measurements to derive accurate and actionable load metrics. Using these metrics, we develop a cost model for cor- recting load imbalance. Our model enables comparisons of the ef- fectiveness of load balancing algorithms in any specific imbalance scenario. Our model correctly selects the algorithm that achieves the lowest runtime in up to 96\% of the cases, and can achieve a 19\% gain over selecting a single balancing algorithm for all cases.},
author = {Pearce, Olga and Gamblin, Todd and de Supinski, Bronis R. and Schulz, Martin and Amato, Nancy M.},
booktitle = {ACM International Conference on Supercomputing (ICS)},
doi = {10.1145/2304576.2304601},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pearce et al. - 2012 - Quantifying the effectiveness of load balance algorithms.pdf:pdf},
isbn = {9781450313162},
keywords = {framework,load balance,modeling,performance,simulation},
pages = {185--194},
title = {{Quantifying the effectiveness of load balance algorithms}},
year = {2012}
}
@inproceedings{Majo2012,
author = {Majo, Zoltan and Gross, Thomas R.},
booktitle = {International Symposium on Code Generation and Optimization (CGO)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Majo, Gross - 2012 - Matching memory access patterns and data placement for NUMA systems.pdf:pdf},
pages = {230--241},
title = {{Matching memory access patterns and data placement for NUMA systems}},
year = {2012}
}
@inproceedings{Villavieja2011,
author = {Villavieja, Carlos and Karakostas, Vasileios and Vilanova, Lluis and Etsion, Yoav and Ramirez, Alex and Mendelson, Avi and Navarro, Nacho and Cristal, Adrian and Unsal, Osman S.},
booktitle = {2011 International Conference on Parallel Architectures and Compilation Techniques},
doi = {10.1109/PACT.2011.65},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Villavieja et al. - 2011 - DiDi Mitigating the Performance Impact of TLB Shootdowns Using a Shared TLB Directory.pdf:pdf},
isbn = {978-1-4577-1794-9},
month = oct,
publisher = {Ieee},
title = {{DiDi: Mitigating the Performance Impact of TLB Shootdowns Using a Shared TLB Directory}},
year = {2011}
}
@misc{Intel2013,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2013 - Intel 64 and IA-32 Architectures Software Developer's Manual.pdf:pdf},
number = {September},
title = {{Intel 64 and IA-32 Architectures Software Developer's Manual}},
year = {2013}
}
@inproceedings{Goglin2009,
annote = {just implementation of next-touch},
author = {Goglin, Brice and Furmento, Nathalie},
booktitle = {IEEE International Symposium on Parallel \& Distributed Processing (IPDPS)},
doi = {10.1109/IPDPS.2009.5161101},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Goglin, Furmento - 2009 - Enabling High-Performance Memory Migration for Multithreaded Applications on Linux.pdf:pdf},
isbn = {978-1-4244-3751-1},
keywords = {affinity,lazy migration,linux,memory migration,multithreaded applications,next-touch,numa},
mendeley-tags = {next-touch},
title = {{Enabling High-Performance Memory Migration for Multithreaded Applications on Linux}},
year = {2009}
}
@phdthesis{Aslot2002,
author = {Aslot, Vishal},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Aslot - 2002 - Performance characterization of the SPEC OMP benchmarks.pdf:pdf},
keywords = {omp,openmp,spec},
mendeley-tags = {omp,openmp,spec},
school = {Purdue University},
title = {{Performance characterization of the SPEC OMP benchmarks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.5591\&amp;rep=rep1\&amp;type=pdf},
year = {2002}
}
@techreport{Held2010,
author = {Held, Jim},
booktitle = {Agenda},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Held - 2010 - Single-chip Cloud Computer.pdf:pdf},
title = {{Single-chip Cloud Computer}},
year = {2010}
}
@misc{Corbet,
author = {Corbet, Jonathan},
keywords = {kernel,numa balance,numa balancing},
mendeley-tags = {kernel,numa balance,numa balancing},
title = {{Toward better NUMA scheduling}},
url = {http://lwn.net/Articles/486858/},
year = {2012}
}
@article{Radojkovic2012,
abstract = {The introduction of multithreaded processors comprised of a large number of cores with many shared resources makes thread scheduling, and in particular optimal assignment of running threads to processor hardware contexts to become one of the most promising ways to improve the system performance. However, finding optimal thread assignments for workloads running in state-of- the-art multicore/multithreaded processors is an NP-complete problem. In this paper, we propose BlackBox scheduler, a systematic method for thread assignment of multithreaded network applications running on multicore/multithreaded processors. The method requires minimum information about the target processor architecture and no data about the hardware requirements of the applications under study. The proposed method is evaluated with an industrial case study for a set of multithreaded network applications running on the UltraSPARC T2 processor. In most of the experiments, the proposed thread assignment method detected the best actual thread assignment in the evaluation sample. The method improved the system performance from 5\% to 48\% with respect to load balancing algorithms used in state-of-the-art OSs, and up to 60\% with respect to a naive thread assignment.},
author = {Radojkovi\'{c}, Petar and Cakarevi\'{c}, Vladimir and Verd\'{u}, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Radojkovi\'{c} et al. - 2013 - Thread Assignment of Multithreaded Network Applications in MulticoreMultithreaded Processors.pdf:pdf;:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Radojkovi\'{c} et al. - 2013 - Thread Assignment of Multithreaded Network Applications in MulticoreMultithreaded Processors(2).pdf:pdf},
journal = {IEEE Transactions on Parallel and Distributed Systems (TPDS)},
number = {12},
pages = {2513--2525},
title = {{Thread Assignment of Multithreaded Network Applications in Multicore/Multithreaded Processors}},
volume = {24},
year = {2013}
}
@article{Azimi2009,
abstract = {Multicore processors contain new hardware characteristics that are different from previous generation single-core systems or traditional SMP (symmetric multiprocessing) multiprocessor systems. These new characteristics provide new performance opportunities and challenges. In this paper, we show how hardware performance monitors can be used to provide a fine-grained, closely-coupled feedback loop to dynamic optimizations done by a multicore-aware operating system. These multicore optimizations are possible due to the advanced capabilities of hardware performance monitoring units currently found in commodity processors, such as execution pipeline stall breakdown and data address sampling. We demonstrate three case studies on how a multicore-aware operating system can use these online capabilities for (1) determining cache partition sizes, which helps reduce contention in the shared cache among applications, (2) detecting memory regions with bad cache usage, which helps in isolating these regions to reduce cache pollution, and (3) detecting sharing among threads, which helps in clustering threads to improve locality. Using realistic applications from standard benchmark suites, the following performance improvements were achieved: (1) up to 27\% improvement in IPC (instructions-per-cycle) due to cache partition sizing; (2) up to 10\% reduction in cache miss rates due to reduced cache pollution, resulting in up to 7\% improvement in IPC; and (3) up to 70\% reduction in remote cache accesses due to thread clustering, resulting in up to 7\% application-level improvement.},
author = {Azimi, Reza and Tam, David K. and Soares, Livio and Stumm, Michael},
doi = {10.1145/1531793.1531803},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Azimi et al. - 2009 - Enhancing Operating System Support for Multicore Processors by Using Hardware Performance Monitoring.pdf:pdf},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
month = apr,
number = {2},
pages = {56--65},
title = {{Enhancing Operating System Support for Multicore Processors by Using Hardware Performance Monitoring}},
volume = {43},
year = {2009}
}
@article{Coteus2011,
abstract = {To satisfy the economic drive for ever more powerful computers to handle scientific and business applications, new technologies are needed to overcome the limitations of current approaches. New memory technologies will address the need for greater amounts of data in close proximity to the processors. Three-dimensional silicon integration will allow more cache and function to be integrated with the processor while allowing more than 1,000 times higher bandwidth communications at low power per channel using local interconnects between Si die layers and between die stacks. Integrated silicon nanophotonics will provide low-power and high-bandwidth optical interconnections between different parts of the system on a chip, board, and rack levels. Highly efficient power delivery and advanced liquid cooling will reduce the electrical demand and facility costs. A combination of these technologies will likely be required to build exascale systems that meet the combined challenges of a practical power constraint on the order of 20 MW with sufficient reliability and at a reasonable cost.},
author = {Coteus, P. W. and Knickerbocker, J. U. and Lam, C. H. and Vlasov, Y. a.},
doi = {10.1147/JRD.2011.2163967},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Coteus et al. - 2011 - Technologies for exascale systems.pdf:pdf},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = sep,
number = {5},
pages = {14:1--14:12},
title = {{Technologies for exascale systems}},
volume = {55},
year = {2011}
}
@inproceedings{Ribeiro2010,
author = {Ribeiro, CP and Castro, M and Mehaut, Jean-Francois and Carrissimi, Alexandre},
booktitle = {International Conference on High Performance Computing for Computational Science (VECPAR)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro et al. - 2010 - Improving memory affinity of geophysics applications on NUMA platforms using Minas.pdf:pdf},
keywords = {minas},
mendeley-tags = {minas},
pages = {279--292},
title = {{Improving memory affinity of geophysics applications on NUMA platforms using Minas}},
year = {2010}
}
@inproceedings{Natarajan2013,
abstract = {Recent years have seen a large volume of propos- als on managing the shared last-level cache (LLC) of chip- multiprocessors (CMPs). However, most of these proposals pri- marily focus on reducing the amount of destructive interference between competing independent threads of multi-programmed workloads.While very few of these studies evaluate the proposed policies on shared memory multi-threaded applications, they do not improve constructive cross-thread sharing of data in the LLC. In this paper, we characterize a set of multi-threaded applica- tions drawn from the PARSEC, SPEC OMP, and SPLASH-2 suites with the goal of introducing sharing-awareness in LLC replacement policies. We motivate our characterization study by quantifying the potential contributions of the shared and the private blocks toward the overall volume of the LLC hits in these applications and show that the shared blocks are more important than the private blocks. Next, we characterize the amount of sharing-awareness enjoyed by recent proposals compared to the optimal policy. We design and evaluate a generic oracle that can be used in conjunction with any existing policy to quantify the potential improvement that can come from introducing sharing- awareness. The oracle analysis shows that introducing sharing- awareness reduces the number of LLC misses incurred by the least-recently-used (LRU) policy by 6\% and 10\% on average for a 4MB and 8MB LLC respectively. A realistic implementation of this oracle requires the LLC controller to have the capability to accurately predict, at the time a block is filled into the LLC, whether the block will be shared during its residency in the LLC. We explore the feasibility of designing such a predictor based on the address of the fill and the program counter of the instruction that triggers the fill. Our sharing behavior predictability study of two history-based fill-time predictors that use block addresses and program counters concludes that achieving acceptable levels of accuracy with such predictors will require other architectural and/or high-level program semantic features that have strong correlations with active sharing phases of the LLC blocks.},
author = {Natarajan, Ragavendra and Chaudhuri, Mainak},
booktitle = {2013 IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2013.6704665},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Natarajan, Chaudhuri - 2013 - Characterizing Multi-threaded Applications for Designing Sharing-aware Last-level Cache Replacement Polici.pdf:pdf},
isbn = {978-1-4799-0555-3},
title = {{Characterizing Multi-threaded Applications for Designing Sharing-aware Last-level Cache Replacement Policies}},
year = {2013}
}
@article{Freitas2009,
author = {Freitas, S. R. and Longo, K. M. and {Silva Dias}, M. a. F. and Chatfield, R. and {Silva Dias}, P. and Artaxo, P. and Andreae, M. O. and Grell, G. and Rodrigues, L. F. and Fazenda, A. and Panetta, J.},
doi = {10.5194/acp-9-2843-2009},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Freitas et al. - 2009 - The Coupled Aerosol and Tracer Transport model to the Brazilian developments on the Regional Atmospheric Mode(2).pdf:pdf},
issn = {1680-7324},
journal = {Atmospheric Chemistry and Physics},
keywords = {brams},
mendeley-tags = {brams},
month = apr,
number = {8},
pages = {2843--2861},
title = {{The Coupled Aerosol and Tracer Transport model to the Brazilian developments on the Regional Atmospheric Modeling System (CATT-BRAMS) – Part 1: Model description and evaluation}},
volume = {9},
year = {2009}
}
@article{Bellosa1996,
author = {Bellosa, Frank and Steckermeier, Martin},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bellosa, Steckermeier - 1996 - The performance implications of locality information usage in shared-memory multiprocessors.pdf:pdf},
journal = {Journal of Parallel and Distributed Computing},
number = {1},
pages = {113--121},
title = {{The performance implications of locality information usage in shared-memory multiprocessors}},
volume = {37},
year = {1996}
}
@inproceedings{Jaleel2010,
author = {Jaleel, Aamer and Borch, Eric},
booktitle = {\ldots (MICRO), 2010 43rd \ldots},
doi = {10.1109/MICRO.2010.52},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Jaleel, Borch - 2010 - Achieving non-inclusive cache performance with inclusive caches Temporal locality aware (TLA) cache management po.pdf:pdf},
isbn = {978-1-4244-9071-4},
keywords = {exclusion,inclusion,non-inclusion,replacement},
month = dec,
pages = {151--162},
publisher = {Ieee},
title = {{Achieving non-inclusive cache performance with inclusive caches: Temporal locality aware (TLA) cache management policies}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5695533 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5695533},
year = {2010}
}
@inproceedings{Osiakwan1990,
abstract = {There are efficient sequential algorithms that use linear programming (LP) for computing maximum weight matchings. Finding a deterministic parallel algorithm for computing maximum weight matchings in complete graphs has been an open problem for some time. Since LP is known to be P-complete, then, by the parallel computation thesis, it is unlikely that there exists an NC algorithm that uses LP to solve the maximum weight matching problem. The authors present an LP-based parallel algorithm for maximum weight matching in a complete weighted graph. The algorithm is designed for the EREW PRAM model of parallel computation, and runs in O(n3/p+n2logn) time for p⩽n, where p is the number of processors and n is the number of vertices in the graph. This algorithm provides an optimal speedup with respect to the O(n 3) sequential LP-based solution of Gabow (1974) or Lawler (1976), for p⩽n/log n. This is the first deterministic optimal speedup parallel algorithm designed for the maximum weight matching problem on complete graphs.},
author = {Osiakwan, C.N.K. and Akl, S.G.},
booktitle = {IEEE Symposium on Parallel and Distributed Processing (SPDP)},
doi = {10.1109/SPDP.1990.143503},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Osiakwan, Akl - 1990 - The Maximum Weight Perfect Matching Problem for Complete Weighted Graphs is in PC.pdf:pdf},
isbn = {0-8186-2087-0},
keywords = {maximum weight matching,optimal speedup,parallel algorithm},
pages = {880--887},
title = {{The Maximum Weight Perfect Matching Problem for Complete Weighted Graphs is in PC}},
year = {1990}
}
@inproceedings{Cascaval2005,
abstract = {With the growing awareness that individual hardware cores will not continue to produce the same level of per- formance improvement, there is a need to develop an in- tegrated approach to performance optimization. In this paper we present a paradigm for Continuous Program Optimization (CPO), whereby automatic agentsmonitor and optimize application and system performance. The monitoring data is used to analyze and create models of application and system behavior. Using this analysis,we describe how CPO agents can improve the performance of both the application and the underlying system. Using the CPO paradigm, we implemented cooperating page size optimization agents that automatically opti- mize large page usage. An offline agent uses vertically integrated performance data to produce a page size ben- efit analysis for different categories of data structures within an application. We show how an online CPO agent can use the results of the predictive analysis to au- tomatically improve application performance. We val- idate that the predictions made by the CPO agent re- flect the actual performance gains of up to 60\% across a range of scientific applications including the SPEC- cpu2000 floating point benchmarks and two large high performance computing (HPC) applications.},
author = {Cascaval, C. and Duesterwald, E. and Sweeney, P.F. and Wisniewski, R.W.},
booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
doi = {10.1109/PACT.2005.32},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cascaval et al. - 2005 - Multiple page size modeling and optimization.pdf:pdf},
isbn = {0-7695-2429-X},
pages = {339--349},
title = {{Multiple page size modeling and optimization}},
year = {2005}
}
@inproceedings{Roloff2012,
abstract = {Using the Cloud Computing paradigm for High- Performance Computing (HPC) is currently a hot topic in the research community and the industry. The attractiveness of Cloud Computing for HPC is the capability to run large applications on powerful, scalable hardware without needing to actually own or maintain this hardware. Most current research focuses on running HPC applications on the Amazon Cloud Computing platform, which is relatively easy because it supports environments that are similar to existing HPC solutions, such as clusters and supercomputers. In this paper, we evaluate the possibility of using Microsoft Windows Azure as a platform for HPC applications. Since most HPC applications are based on the Unix programming model, their source code has to be ported to the Windows programming model in addition to porting it to the Azure platform. We outline the challenges we encountered during porting applications and their resolutions. Furthermore, we introduce a metric to measure the efficiency of Cloud Comput- ing platforms in terms of performance and price.We compared the performance and efficiency of running these benchmarks on a real machine, an Amazon EC2 instance and a Windows Azure instance. Results show that the performance of Azure is close to the performance of running on real machines, and that it is a viable alternative for running HPC applications when compared to other Cloud Computing solutions.},
author = {Roloff, Eduardo and Birck, Francis and Diener, Matthias and Carissimi, Alexandre and Navaux, Philippe O. A.},
booktitle = {IEEE International Conference on Cloud Computing (CLOUD)},
doi = {10.1109/CLOUD.2012.47},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Roloff et al. - 2012 - Evaluating High Performance Computing on the Windows Azure Platform.pdf:pdf},
isbn = {9780769547558},
pages = {803--810},
title = {{Evaluating High Performance Computing on the Windows Azure Platform}},
year = {2012}
}
@inproceedings{Bhatele2012,
annote = {Rubik},
author = {Bhatele, Abhinav and Gamblin, Todd and Langer, Steven H. and Bremer, Peer-Timo and Draeger, Erik W. and Hamann, Bernd and Isaacs, Katherine E. and Landge, Aaditya G. and Levine, Joshua a. and Pascucci, Valerio and Schulz, Martin and Still, Charles H.},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
doi = {10.1109/SC.2012.75},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bhatele et al. - 2012 - Mapping applications with collectives over sub-communicators on torus networks.pdf:pdf},
isbn = {978-1-4673-0806-9},
keywords = {mpi,optimize bandwidth,rubik},
mendeley-tags = {mpi,optimize bandwidth,rubik},
month = nov,
title = {{Mapping applications with collectives over sub-communicators on torus networks}},
year = {2012}
}
@techreport{Intel2007,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2007 - Dual-Core Intel® Itanium® Processor 9000 and 9100 Series.pdf:pdf},
keywords = {itanic,itanium,montecito},
mendeley-tags = {itanic,itanium,montecito},
number = {October},
title = {{Dual-Core Intel® Itanium® Processor 9000 and 9100 Series}},
year = {2007}
}
@misc{Chan1998,
abstract = {The MPE extensions provide a number of useful facilites for MPI programmers. These include several profiling libraries to collect information on MPI programs, in- cluding logfiles for post-mortum visualization and real-time animation. Also included are routines to provide simple X window system graphics to parallel programs. MPE may be used with any implemenation of MPI.},
author = {Chan, Anthony and Gropp, William and Lusk, Ewing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chan, Gropp, Lusk - 1998 - User's Guide for MPE Extensions for MPI Programs.pdf:pdf},
title = {{User's Guide for MPE Extensions for MPI Programs}},
year = {1998}
}
@article{Verghese1996a,
abstract = {The dominant architecture for the next generation of shared-memory$\backslash$nmultiprocessors is CC-NUMA (cache-coherent non-uniform memory architecture).$\backslash$nThese machines are attractive as compute servers because they provide$\backslash$ntransparent access to local and remote memory. However, the access$\backslash$nlatency to remote memory is 3 to 5 times the latency to local memory.$\backslash$nCC-NOW machines provide the benefits of cache coherence to networks$\backslash$nof workstations, at the cost of even higher remote access latency.$\backslash$nGiven the large remote access latencies of these architectures, data$\backslash$nlocality is potentially the most important performance issue. Using$\backslash$nrealistic workloads, we study the performance improvements provided$\backslash$nby OS supported dynamic page migration and replication. Analyzing$\backslash$nour kernel-based implementation, we provide a detailed breakdown$\backslash$nof the costs. We show that sampling of cache misses can be used to$\backslash$nreduce cost without compromising performance, and that TLB misses$\backslash$nmay not be a consistent approximation for cache misses. Finally,$\backslash$nour experiments show that dynamic page migration and replication$\backslash$ncan substantially increase application performance, as much as 30\%,$\backslash$nand reduce contention for resources in the NUMA memory system.},
author = {Verghese, Ben and Devine, Scott and Gupta, Anoop and Rosenblum, Mendel},
doi = {10.1145/248209.237205},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Verghese et al. - 1996 - Operating system support for improving data locality on CC-NUMA compute servers.pdf:pdf},
isbn = {0-89791-767-7},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
number = {9},
pages = {279--289},
title = {{Operating system support for improving data locality on CC-NUMA compute servers}},
volume = {31},
year = {1996}
}
@inproceedings{Marchetti1995,
abstract = {The cost of a cache miss depends heavily on the location of the main memory that backs the missing line. For certain applications, this cost is a major factor in overall performance. We report on the utility of OS-based page placement as a mechanism to increase the frequency with which cache fills access local memory in distributed shared memory multiprocessors. Even with the very simple policy of first-use placement, we find significant improvements over round-robin placement for many applications on both hardware- and software-coherent systems. For most of our applications, first-use placement allows 35 to 75 percent of cache fills to be performed locally, resulting in performance improvements of up to 40 percent with respect to round-robin placement. We were surprised to find no performance advantage in more sophisticated policies, including page migration and page replication. In fact, in many cases the performance of our applications suffered under these policies},
author = {Marchetti, M. and Kontothanassis, L. and Bianchini, R. and Scott, M.L.},
booktitle = {International Parallel Processing Symposium (IPPS)},
doi = {10.1109/IPPS.1995.395974},
file = {:Users/mdiener/Downloads/1995\_IPPS.pdf:pdf},
isbn = {0-8186-7074-6},
issn = {10636374},
keywords = {first-touch},
mendeley-tags = {first-touch},
pages = {480--485},
title = {{Using simple page placement policies to reduce the cost of cache fills in coherent shared-memory systems}},
year = {1995}
}
@inproceedings{Henrique,
abstract = {In parallel programs, the tasks of a given applica- tion must cooperate in order to accomplish the required com- putation. However, the communication time between the tasks may be different depending on which core they are executing and how the memory hierarchy and interconnection are used. The problem is even more important in multi-core machines with NUMA characteristics, since the remote access imposes high overhead, making them more sensitive to thread and data mapping. In this context, process mapping is a technique that provides performance gains by improving the use of resources such as interconnections, main memory and cache memory. The problem of detecting the best mapping is considered NP- Hard. Furthermore, in shared memory environments, there is an additional difficulty of finding the communication pattern, which is implicit and occurs through memory accesses. This work aims to provide a method for static mapping for NUMA architectures which does not require any prior knowledge of the application. Different metrics were adopted and an heuristic method based on the Edmonds matching algorithm was used to obtain the mapping. In order to evaluate our proposal, we use the NAS Parallel Benchmarks (NPB) and two modern multi- core NUMA machines. Results show performance gains of up to 75\% compared to the native scheduler and memory allocator of the operating system.},
author = {Cruz, Eduardo Henrique Molina and Alves, Marco Antonio Zanata and Carissimi, Alexandre and Navaux, Philippe Olivier Alexandre and Ribeiro, Christiane Pousa and M\'{e}haut, Jean-Fran\c{c}ois},
booktitle = {IEEE International Symposium on Parallel and Distributed Processing Workshops and PhD Forum},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz et al. - 2011 - Using Memory Access Traces to Map Threads and Data on Hierarchical Multi-core Platforms.pdf:pdf},
keywords = {-process map,high performance computing,memory affinity,parallel architec-,tures},
pages = {551--558},
title = {{Using Memory Access Traces to Map Threads and Data on Hierarchical Multi-core Platforms}},
year = {2011}
}
@inproceedings{Tam2007,
abstract = {The major chip manufacturers have all introduced chip mul- tiprocessing (CMP) and simultaneousmultithreading (SMT) technology into their processing units. As a result, even low-end computing systems and game consoles have become shared memory multiprocessors with L1 and L2 cache shar- ing within a chip. Mid- and large-scale systems will have multiple processing chips and hence consist of an SMP- CMP-SMT configuration with non-uniform data sharing over- heads. Current operating system schedulers are not aware of these new cache organizations, and as a result, distribute threads across processors in a way that causes many unnec- essary, long-latency cross-chip cache accesses. In this paper we describe the design and implementation of a scheme to schedule threads based on sharing patterns detected online using features of standard performance mon- itoring units (PMUs) available in today’s processing units. The primary advantage of using the PMU infrastructure is that it is fine-grained (down to the cache line) and has rel- atively low overhead. We have implemented our scheme in Linux running on an 8-way Power5 SMP-CMP-SMT multi- processor. For commercial multithreaded server workloads (VolanoMark, SPECjbb, and RUBiS), we are able to demon- strate reductions in cross-chip cache accesses of up to 70\%. These reductions lead to application-reported performance improvements of up to 7\%.},
author = {Tam, David and Azimi, Reza and Stumm, Michael},
booktitle = {ACM SIGOPS/EuroSys European Conference on Computer Systems},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Tam, Azimi, Stumm - 2007 - Thread Clustering Sharing-Aware Scheduling on SMP-CMP-SMT Multiprocessors.pdf:pdf},
keywords = {affinity scheduling,cache behavior,cache locality,cmp,detecting sharing,hard-,hardware performance counters,multithreading,performance,ware performance monitors},
pages = {47--58},
title = {{Thread Clustering: Sharing-Aware Scheduling on SMP-CMP-SMT Multiprocessors}},
year = {2007}
}
@phdthesis{Pilla2014,
author = {Pilla, La\'{e}rcio Lima},
booktitle = {hal.inria.fr},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pilla - 2014 - Topology-Aware Load Balancing for Performance Portability over Parallel High Performance Systems.pdf:pdf},
number = {April},
title = {{Topology-Aware Load Balancing for Performance Portability over Parallel High Performance Systems}},
year = {2014}
}
@article{Chapman2002,
abstract = {OpenMP is emerging as a viable high-level programming model for shared memory parallel systems. It was conceived to enable easy, portable application development on this range of systems, and it has also been implemented on cache-coherent Non-Uniform Memory Access (ccNUMA) architectures. Unfortunately, it is hard to obtain high performance on the latter architecture, particularly when large numbers of threads are involved. In this paper, we discuss the difficulties faced when writing OpenMP programs for ccNUMA systems, and explain how the vendors have attempted to overcome them.We focus on one such system, the SGI Origin 2000, and perform a variety of experiments designed to illustrate the impact of the vendor’s efforts. We compare codes written in a standard, loop-level parallel style under OpenMP with alternative versions written in a Single Program Multiple Data (SPMD) fashion, also realized via OpenMP, and show that the latter consistently provides superior performance. A carefully chosen set of language extensions can help us translate programs from the former style to the latter (or to compile directly, but in a similar manner). Syntax for these extensions can be borrowed from HPF, and some aspects of HPF compiler technology can help the translation process. It is our expectation that an extended language, ifwell compiled, would improve the attractiveness of OpenMP as a language for high-performance computation on an important class of modern architectures.},
author = {Chapman, B. and Bregier, F. and Patil, A. and Prabhakar, A.},
doi = {10.1002/cpe.646},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chapman et al. - 2002 - Achieving performance under OpenMP on ccNUMA and software distributed shared memory systems.pdf:pdf},
issn = {1532-0626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {ccnuma architectures,data,data distribution,locality,openmp,restructuring,shared memory parallel programming,software distributed shared memory},
month = jul,
number = {8-9},
pages = {713--739},
title = {{Achieving performance under OpenMP on ccNUMA and software distributed shared memory systems}},
volume = {14},
year = {2002}
}
@inproceedings{Shalf2010,
abstract = {High Performance Computing architectures are expected to change dramatically in the next decade as power and cooling constraints limit increases in microprocessor clock speeds. Consequently computer companies are dramati- cally increasing on-chip parallelism to improve performance. The traditional doubling of clock speeds every 18-24 months is being replaced by a doubling of cores or other parallelism mechanisms. During the next decade the amount of parallelism on a single microprocessor will rival the number of nodes in early massively parallel supercomputers that were built in the 1980s. Applications and algorithms will need to change and adapt as node architectures evolve. In par- ticular, they will need to manage locality to achieve performance. A key element of the strategy as we move forward is the co-design of applications, architectures and programming environments. There is an unprecedented opportunity for application and algorithm developers to influence the direction of future architec- tures so that they meet DOE mission needs. This article will describe the tech- nology challenges on the road to exascale, their underlying causes, and their effect on the future of HPC system design.},
annote = {contains energy consumption of memory accesses.},
author = {Shalf, John and Dosanjh, Sudip and Morrison, John},
booktitle = {High Performance Computing for Computational Science (VECPAR)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Shalf, Dosanjh, Morrison - 2010 - Exascale computing technology challenges.pdf:pdf},
keywords = {codesign,exascale,hpc},
title = {{Exascale computing technology challenges}},
year = {2010}
}
@inproceedings{Bienia2009,
abstract = {The second version of the Princeton Application Repository for Shared-Memory Computers (PARSEC) has been released. PAR- SEC is a benchmark suite for Chip-Multiprocessors (CMPs) that focuses on emerging applications. It includes a diverse set of work- loads from different domains such as interactive animation or sys- tems applications that mimic large-scale commercial workloads. The next version of PARSEC features several improved and one new workload. It also supports an additional parallelization model. Many patches and changeswere includedwhich simplify the use of PARSEC in practice. The benchmarks of the new suite have higher scalability and cover a larger number of emerging applications. In this paper we discuss the major changes in detail and provide the information necessary to interpret results obtained with PARSEC 2.0 correctly.},
author = {Bienia, Christian and Li, Kai},
booktitle = {Annual Workshop on Modeling, Benchmarking and Simulation},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bienia, Li - 2009 - Parsec 2.0 A new benchmark suite for chip-multiprocessors.pdf:pdf},
keywords = {benchmark suite,multithreading,performance measurement,shared-memory computers},
title = {{PARSEC 2.0: A New Benchmark Suite for Chip-Multiprocessors}},
year = {2009}
}
@inproceedings{Devine2006,
author = {Devine, K D and Boman, E G and Heaphy, R T and Bisseling, R H and Catalyurek, U V},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2006.1639359},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Devine et al. - 2006 - Parallel hypergraph partitioning for scientific computing.pdf:pdf},
isbn = {1-4244-0054-6},
pages = {124--133},
title = {{Parallel hypergraph partitioning for scientific computing}},
year = {2006}
}
@inproceedings{Yardimci2006,
address = {New York, New York, USA},
author = {Yardimci, Efe and Franz, Michael},
booktitle = {Proceedings of the 3rd conference on Computing frontiers - CF '06},
doi = {10.1145/1128022.1128040},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Yardimci, Franz - 2006 - Dynamic parallelization and mapping of binary executables on hierarchical platforms.pdf:pdf},
isbn = {1595933026},
keywords = {continuous optimization,dynamic parallelization},
pages = {127},
publisher = {ACM Press},
title = {{Dynamic parallelization and mapping of binary executables on hierarchical platforms}},
url = {http://portal.acm.org/citation.cfm?doid=1128022.1128040},
year = {2006}
}
@article{Saito2006,
author = {Saito, Hideki and Gaertner, Greg and Jones, Wesley and Eigenmann, Rudolf and Iwashita, Hidetoshi and Lieberman, Ron and van Waveren, Matthijs and Whitney, Brian},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Saito et al. - 2006 - Large System Performance of SPEC OMP2001 Benchmarks.pdf:pdf},
journal = {High Performance Computing},
keywords = {benchmarks,computing,high-performance,openmp,performance evaluation,spec omp2001},
pages = {370--379},
title = {{Large System Performance of SPEC OMP2001 Benchmarks}},
url = {http://www.springerlink.com/index/k8vnpv9juhwfq0xl.pdf},
volume = {2327},
year = {2006}
}
@inproceedings{Butko2012,
abstract = {Design space exploration (DSE) of complex embedded systems that combine a number of CPUs, dedicated hardware and software is a tedious task for which a broad range of approaches exists, from the use of high-level models to hardware prototyping. Each of these entails different simulation speed/accuracy tradeoffs, and thereby enables exploring a certain subset of the design space in a given time. Some simulation frameworks devoted to CPU-centric systems have been developed over the past decade, that either feature near real-time simulation speed or moderate to high speed with quasi-cycle level accuracy, often by means of instruction-set simulators or binary translation techniques. This paper presents an evaluation in term of accuracy in modeling real systems using the GEM5 simulator that belong to the first class. Performance figures of a wide range of benchmarks (e.g. in domains such as scientific computing and media applications) are captured and compared to results obtained on real hardware.},
author = {Butko, Anastasiia and Garibotti, Rafael},
booktitle = {International Workshop on Reconfigurable Communication-centric Systems-on-Chip (ReCoSoC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Butko, Garibotti - 2012 - Accuracy evaluation of GEM5 simulator system.pdf:pdf},
keywords = {embedded system,gem5,modeling and full-system},
mendeley-tags = {gem5},
pages = {1--7},
title = {{Accuracy evaluation of GEM5 simulator system}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6322869},
year = {2012}
}
@inproceedings{Rashti2011,
abstract = {MPI standard offers a set of topology-aware interfaces that can be used to construct graph and Cartesian topologies for MPI applications. These interfaces have been mostly used for topology construction and not for performance improvement. To optimize the performance, in this paper we use graph embedding and node/network architecture discovery modules to match the communication topology of the applications to the physical topology of multi-core clusters with multi-level networks. Micro-benchmark results show considerable improvement in communication performance when using weighted and network-aware mapping. We also show that the implementation can improve communication and execution time of the applications.},
author = {Rashti, Mohammad Javad and Green, Jonathan and Balaji, Pavan and Afsahi, Ahmad and Gropp, William},
booktitle = {Recent Advances in the Message Passing Interface},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Rashti et al. - 2011 - Multi-core and Network Aware MPI Topology Functions.pdf:pdf},
keywords = {mpi,multi-core,network,physical topology,virtual topology},
title = {{Multi-core and Network Aware MPI Topology Functions}},
year = {2011}
}
@article{Kim2002,
abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. In- creases in on-chip communication delays will make the hit time of large on-chip caches a function of a line’s physical location within the cache. Consequently, cache access times will become a contin- uum of latencies rather than a single discrete latency. This non- uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow im- portant data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy– while using less silicon area–by 13\%, and comes within 13\% of an ideal, minimal hit latency solution.},
author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
doi = {10.1145/635506.605420},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Burger, Keckler - 2002 - An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches.pdf:pdf},
isbn = {1581135742},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {nuca},
mendeley-tags = {nuca},
month = dec,
number = {5},
pages = {211},
title = {{An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches}},
volume = {30},
year = {2002}
}
@misc{Frumkin2003,
abstract = {Data movement across computational grids and across memory hierarchy of individual grid machines is known to be a limiting factor for application involving large data sets. In this paper we introduce the Data Cube Operator on an Arithmetic Data Set which we call Arithmetic Data Cube (ADC). We propose to use the ADC to benchmark grid capabilities to handle large distributed data sets. The ADC stresses all levels of grid memory by producing 2d views of an Arithmetic Data Set of d-tuples described by a small number of integer parameters. We control data intensity of the ADC by controlling the sizes of the views through choice of the tuple parameters.},
author = {Frumkin, Michael A. and Shabanov, Leonid},
booktitle = {National Aeronautics and Space \ldots},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Frumkin, Shabanov - 2003 - Arithmetic Data Cube as a Data Intensive Benchmark.pdf:pdf},
keywords = {data cube,dc,nas},
mendeley-tags = {data cube,dc,nas},
publisher = {National Aeronautics and Space Administration},
title = {{Arithmetic Data Cube as a Data Intensive Benchmark}},
url = {http://www.nas.nasa.gov/assets/pdf/techreports/2003/nas-03-005.pdf},
year = {2003}
}
@techreport{Drepper2007,
abstract = {As CPU cores become both faster and more numerous, the limiting factor for most programs is now, and will be for some time, memory access. Hardware designers have come up with ever more sophisticated memory handling and acceleration techniquessuch as CPU cachesbut these cannot work optimally without some help from the programmer. Unfortunately, neither the structure nor the cost of using the memory subsystem of a computer or the caches on CPUs is well understood by most programmers. This paper explains the structure of memory subsys- tems in use on modern commodity hardware, illustrating why CPU caches were developed, how they work, and what programs should do to achieve optimal performance by utilizing them.},
author = {Drepper, Ulrich},
booktitle = {Changes},
doi = {10.1.1.91.957},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Drepper - 2007 - What Every Programmer Should Know About Memory.pdf:pdf},
institution = {Red Hat, Inc.},
issn = {0361526X},
number = {4},
pages = {114},
publisher = {Citeseer},
title = {{What Every Programmer Should Know About Memory}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.957\&rep=rep1\&type=pdf},
volume = {3},
year = {2007}
}
@article{Bailey1991,
abstract = {A new set of benchmarks has been developed for the performance evaluation of highly parallel supercomputers. These benchmarks con- sist of ve $\backslash$parallel kernel" benchmarks and three $\backslash$simulated appli- cation" benchmarks. Together they mimic the computation and data movement characteristics of large scale computational uid dynamics applications. The principal distinguishing feature of these benchmarks is their $\backslash$pencil and paper" speci cation | all details of these benchmarks are speci ed only algorithmically. In this way many of the di cul- ties associated with conventional benchmarking approaches on highly parallel systems are avoided.},
author = {Bailey, D and Barszcz, E and Barton, J and Browning, D and Carter, R and Dagum, L and Fineberg, S and Frederickson, P and Lasinski, T and Schreiber, R and Simon, H and Venkatakrishnan, V and Weeratunga, S},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bailey et al. - 1991 - The NAS Parallel Benchmarks.pdf:pdf},
journal = {International Journal of High Performance Computing Applications},
keywords = {nas},
mendeley-tags = {nas},
number = {3},
pages = {66--73},
title = {{The NAS Parallel Benchmarks}},
volume = {5},
year = {1991}
}
@phdthesis{Bhatele2010,
author = {Bhatele, Abhinav},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bhatele - 2010 - Automating Topology Aware Mapping for Supercomputers.pdf:pdf},
title = {{Automating Topology Aware Mapping for Supercomputers}},
year = {2010}
}
@techreport{Chandramouli2004,
author = {Chandramouli, Badrish and Iyer, Sita},
booktitle = {Department Of Computer Science, Duke},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chandramouli, Iyer - 2004 - A performance study of snoopy and directory based cache-coherence protocols.pdf:pdf},
institution = {Duke University},
keywords = {cache-coherence protocols,directory,multi- processors,snoopy},
pages = {1--8},
title = {{A performance study of snoopy and directory based cache-coherence protocols}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.180\&amp;rep=rep1\&amp;type=pdf},
year = {2004}
}
@inproceedings{Piccoli2014,
author = {Piccoli, Guilherme and Santos, Henrique N. and Rodrigues, Raphael E. and Pousa, Christiane and Borin, Edson and {Quint\~{a}o Pereira}, Fernando M. and Magno, Fernando},
booktitle = {International Conference on Parallel Architectures and Compilation (PACT)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Piccoli et al. - 2014 - Compiler support for selective page migration in NUMA architectures.pdf:pdf},
keywords = {data placement,memory topology,numa,static analysis},
pages = {369--380},
title = {{Compiler support for selective page migration in NUMA architectures}},
year = {2014}
}
@inproceedings{Bull2002,
author = {Bull, J. Mark and Johnson, Chris},
booktitle = {European Workshop on OpenMP (EWOMP)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bull, Johnson - 2002 - Data distribution, migration and replication on a cc-NUMA architecture.pdf:pdf},
pages = {1--5},
title = {{Data distribution, migration and replication on a cc-NUMA architecture}},
year = {2002}
}
@book{ARM2005,
author = {ARM},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/ARM - 2005 - ARM Architecture Reference Manual.pdf:pdf},
title = {{ARM Architecture Reference Manual}},
year = {2005}
}
@article{Bolosky1992,
abstract = {In recent years, much effort has been devoted to analyzing the performance of distributed memory systems for multiprocessors. Such systems usually consist of a set of memories or caches, some device such as a bus or switch to connect the memories and processors, and a policy for determining when to put which addressable objects in which memories. In attempting to evaluate such systems, it has generally proven difficult to separate the performance implications of the hardware architecture from those of the policy that controls the hardware (whether implemented in software or hardware). The use of off-line optimal analysis to achieve this separation is described. Using a trace-driven dynamic programming algorithm, the policy decisions that would maximize overall memory system performance for a given program execution are computed. The result allows us to eliminate the artifacts of any arbitrarily chosen policy when evaluating hardware performance, and provides a baseline against which to compare the performance of particular, realizable, policies. This technique is illustrated in the context of software-controlled page migration and replication, and its applicability to other forms of multiprocessor memory management is argued.},
author = {Bolosky, William J. and Scott, Michael L.},
doi = {10.1016/0743-7315(92)90051-N},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bolosky, Scott - 1991 - Evaluation of multiprocessor memory systems using off-line optimal behavior.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing (JPDC)},
number = {4},
pages = {382--398},
title = {{Evaluation of multiprocessor memory systems using off-line optimal behavior}},
volume = {15},
year = {1992}
}
@inproceedings{Lachaize2012,
author = {Lachaize, Renaud and Lepers, Baptiste and Qu\'{e}ma, Vivien},
booktitle = {Proceedings of the 2012 USENIX Conference on Annual Technical Conference},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lachaize, Lepers, Qu\'{e}ma - 2012 - MemProf A Memory Profiler for NUMA Multicore Systems.pdf:pdf},
title = {{MemProf: A Memory Profiler for NUMA Multicore Systems}},
year = {2012}
}
@inproceedings{Zhou2009,
abstract = {Resource sharing can cause unfair and unpredictable performance of concurrently executing applications in Chip-Multiprocessors (CMP). The shared last-level cache is one of themost important shared resources because off-chip request latencymay take a significant part of total execution cycles for data intensive applications. Instead of enforcing performance fairness directly, prior work addressing fair- ness issue of cache sharing mainly focuses on the fairness metrics of cache miss numbers or miss rates. However, be- cause of the variation of cache miss penalty, fairness on cache miss cannot guarantee performance fairness. Cache sharing managementwhich directly addresses performance fairness is needed for CMP systems. This paper introduces a model to analyze the perfor- mance impact of cache sharing, and proposes a mechanism of cache sharing management to provide performance fair- ness for concurrently executing applications. The proposed mechanism monitors the actual penalty of all cache misses and dynamically estimates the cache misses with dedicated caches when the applications are actually running with a shared cache. The estimated relative slowdown for each core from dedicated environment to shared environment is used to guide cache sharing in order to guarantee perfor- mance fairness. The experiment results show that the pro- posed mechanism always improves the performance fair- ness metric, and can provide no worse throughput than the scenario without any management mechanism.},
author = {Zhou, Xing and Chen, Wenguang and Zheng, Weimin},
booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
doi = {10.1109/PACT.2009.40},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Chen, Zheng - 2009 - Cache Sharing Management for Performance Fairness in Chip Multiprocessors.pdf:pdf},
isbn = {978-0-7695-3771-9},
month = sep,
pages = {384--393},
title = {{Cache Sharing Management for Performance Fairness in Chip Multiprocessors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5260507},
year = {2009}
}
@article{Chishti2005,
author = {Chishti, Zeshan and Powell, Michael D. and Vijaykumar, T. N.},
doi = {10.1145/1080695.1070001},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chishti, Powell, Vijaykumar - 2005 - Optimizing Replication, Communication, and Capacity Allocation in CMPs.pdf:pdf},
isbn = {076952270X},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
month = may,
number = {2},
pages = {357--368},
title = {{Optimizing Replication, Communication, and Capacity Allocation in CMPs}},
volume = {33},
year = {2005}
}
@article{Wang2009,
abstract = {The efficient mapping of program parallelism to multi-core proces- sors is highly dependent on the underlying architecture. This pa- per proposes a portable and automatic compiler-based approach to mapping such parallelism using machine learning. It develops two predictors: a data sensitive and a data insensitive predictor to select the best mapping for parallel programs. They predict the number of threads and the scheduling policy for any given program using a model learnt off-line. By using low-cost profiling runs, they pre- dict the mapping for a new unseen program across multiple input data sets. We evaluate our approach by selecting parallelism map- ping configurations for OpenMP programs on two representative but different multi-core platforms (the Intel Xeon and the Cell pro- cessors). Performance of our technique is stable across programs and architectures. On average, it delivers above 96\% performance of the maximum available on both platforms. It achieve, on aver- age, a 37\% (up to 17.5 times) performance improvement over the OpenMP runtime default scheme on the Cell platform. Compared to two recent prediction models, our predictors achieve better per- formance with a significant lower profiling cost.},
author = {Wang, Zheng and O'Boyle, MFP},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Wang, O'Boyle - 2009 - Mapping parallelism to multi-cores a machine learning based approach.pdf:pdf},
isbn = {9781605583976},
journal = {ACM Sigplan Notices},
keywords = {artificial neural networks,chine learning,compiler optimization,ma-,performance modeling,support vector machine},
number = {4},
pages = {75--84},
title = {{Mapping parallelism to multi-cores: a machine learning based approach}},
url = {http://dl.acm.org/citation.cfm?id=1594835.1504189},
volume = {44},
year = {2009}
}
@article{Bach2010,
author = {Bach, Moshe and Charney, Mark and Cohn, Robert and Demikhovsky, Elena and Devor, Tevi and Hazelwood, Kim and Jaleel, Aamer and Luk, Chi-Keung and Lyons, Gail and Patil, Harish and Tal, Ady},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bach et al. - 2010 - Analyzing Parallel Programs with Pin.pdf:pdf},
journal = {IEEE Computer},
number = {3},
pages = {34--41},
publisher = {IEEE},
title = {{Analyzing Parallel Programs with Pin}},
volume = {43},
year = {2010}
}
@inproceedings{Busse2013,
abstract = {Since the advent of multi-core processors, different multi-core sys- tem and in particular processor architectures have emerged exhibit- ing individual advantages and disadvantages. One of the main dis- tinguishing factors among these architectures is their varying de- gree and type of resource sharing among individual cores. On the one hand, resource sharing is necessary for the cores to commu- nicate, while on the other hand resource sharing is often used for economic reasons. Depending on the degree and type of resource sharing, the impact on performance depends on the workload ap- plied and can vary to a large extend. In this paper, we investigate the impact of different kinds of re- source interdependencies found in current processors on the per- formance of scheduling strategies using a set of benchmarks. Our results show that the architecture has a major impact on the perfor- mance of a process placement strategy. However, they also point out that simple strategies taking only a few basic architectural char- acteristics into account fall short. Thus, new holistic scheduling strategies are needed that take more characteristics into account.},
author = {Busse, Anselm and Sch\"{o}nherr, Jan H. and Diener, Matthias and M\"{u}hl, Gero and Richling, Jan},
booktitle = {ACM Symposium on Applied Computing (SAC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Busse et al. - 2013 - Analyzing Resource Interdependencies in Multi-Core Architectures to Improve Scheduling Decisions.pdf:pdf},
isbn = {9781450316569},
title = {{Analyzing Resource Interdependencies in Multi-Core Architectures to Improve Scheduling Decisions}},
year = {2013}
}
@inproceedings{Riesen2006,
abstract = {Parallel applications have message-passing patterns that are important to understand. Network topology, rout- ing decisions, and connection and buffer management need to match the communication patterns of an application for it to run efficiently and scale well. These patterns are not eas- ily discerned from the source code of an application, and even when the data is available it is not easy to categorize it appropriately such that meaningful knowledge emerges. We describe a novel system to gather the information we need to discover an application’s communication pattern. We create five categories that help us analyze that data and explain how information from each category can be useful in the design of networking hardware and software. We use the NAS parallel benchmarks as examples on how to apply our techniques.},
author = {Riesen, Rolf},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Riesen - 2006 - Communication Patterns.pdf:pdf},
isbn = {1424400546},
title = {{Communication Patterns}},
year = {2006}
}
@inproceedings{Gustavson1989,
author = {Gustavson, D.B.},
booktitle = {COMPCON Spring'89. Thirty-Fourth IEEE Computer Society International Conference: Intellectual Leverage, Digest of Papers.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/James et al. - 1990 - Distributed-Directory Scheme Scalable Coherent Interface.pdf:pdf},
keywords = {directory,distributed directory},
mendeley-tags = {directory,distributed directory},
number = {415},
pages = {536--538},
publisher = {IEEE},
title = {{Scalable coherent interface}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=301989},
year = {1989}
}
@article{Lenoski1990,
abstract = {DASH is a scalable shared-memory multiprocessor currently being developed at Stanford's Computer Systems Laboratory. The architecture consists of powerful processing nodes, each with a portion of the shared-memory, connected to a scalable interconnection network. A key feature of DASH is its distributed directory-based cache coherence protocol. Unlike traditional snoopy coherence protocols, the DASH protocol does not rely on broadcast; instead it uses point-to-point messages sent between the processors and memories to keep caches consistent. Furthermore, the DASH system does not contain any single serialization or control point. While these features provide the basis for scalability, they also force a reevaluation of many fundamental issues involved in the design of a protocol. These include the issues of correctness, performance and protocol complexity. In this paper, we present the design of the DASH coherence protocol and discuss how it addresses the above issues. We also discuss our strategy for verifying the correctness of the protocol and briefly compare our protocol to the IEEE Scalable Coherent Interface protocol.},
author = {Lenoski, Daniel and Laudon, James and Gharachorloo, Kourosh and Gupta, Anoop and Hennessy, John},
doi = {10.1145/325164.325132},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lenoski et al. - 1990 - The directory-based cache coherence protocol for the DASH multiprocessor.pdf:pdf},
institution = {Stanford University},
isbn = {0897913663},
issn = {01635964},
journal = {Complexity},
number = {3a},
pages = {148--159},
publisher = {ACM},
series = {ISCA '90},
title = {{The directory-based cache coherence protocol for the DASH multiprocessor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=134520},
volume = {18},
year = {1990}
}
@inproceedings{Conway2007,
author = {Conway, Pat},
booktitle = {Micro, IEEE},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Conway - 2007 - The AMD Opteron northbridge architecture.pdf:pdf},
keywords = {amd,hypertransport},
pages = {10--21},
title = {{The AMD Opteron northbridge architecture}},
year = {2007}
}
@inproceedings{Lee2009,
abstract = {Scientific computing algorithms on parallel computing environments are popularly used to simulate scientific and engineering phenomena rather than physical experimenta- tions. The performance of these applications on parallel computing environments depends on the communication de- lay between processors. To reduce the delay, communi- cation patterns have been studied by many research sci- entists. The communication characteristics enables us to better understand the performance behaviors of scientific applications and allows us to predict the performance of large scale applications using a model of smaller version application programs. In this paper, we analyzed the com- munication behavior using NAS-MPI benchmark programs which have been used to represent scientific and engineer- ing workloads. The experimental results show that the com- munication patterns such as communication timing, sizes of messages and destinations could be used to predict the per- formance.},
author = {Lee, Ingyu},
booktitle = {IEEE Southeastcon},
doi = {10.1109/SECON.2009.5174068},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lee - 2009 - Characterizing communication patterns of NAS-MPI benchmark programs.pdf:pdf},
isbn = {978-1-4244-3976-8},
title = {{Characterizing communication patterns of NAS-MPI benchmark programs}},
year = {2009}
}
@inproceedings{Dashti2013,
abstract = {NUMA systems are characterized by Non-Uniform Memory Ac- cess times, where accessing data in a remote node takes longer than a local access. NUMA hardware has been built since the late 80’s, and the operating systems designed for it were optimized for ac- cess locality. They co-located memory pages with the threads that accessed them, so as to avoid the cost of remote accesses. Con- trary to older systems, modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main concern for performance, as we discovered in this work. In- stead, congestion on memory controllers and interconnects, caused by memory traffic from data-intensive applications, hurts perfor- mance a lot more. Because of that, memory placement algorithms must be redesigned to target traffic congestion. This requires an arsenal of techniques that go beyond optimizing locality. In this paper we describe Carrefour, an algorithm that addresses this goal. We implemented Carrefour in Linux and obtained performance im- provements of up to 3.6× relative to the default kernel, as well as significant improvements compared to NUMA-aware patchsets available for Linux. Carrefour never hurts performance by more than 4\% when memory placement cannot be improved.We present the design of Carrefour, the challenges of implementing it on mod- ern hardware, and draw insights about hardware support that would help optimize system software on future NUMA systems.},
author = {Dashti, Mohammad and Fedorova, Alexandra and Funston, Justin and Gaud, Fabien and Lachaize, Renaud and Lepers, Baptiste and Qu\'{e}ma, Vivien and Roth, Mark},
booktitle = {Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Dashti et al. - 2013 - Traffic Management A Holistic Approach to Memory Placement on NUMA Systems.pdf:pdf},
keywords = {interleave,multicore,numa,operating systems,scheduling},
mendeley-tags = {interleave},
pages = {381--393},
title = {{Traffic Management: A Holistic Approach to Memory Placement on NUMA Systems}},
year = {2013}
}
@article{Martin2012,
abstract = {SHARED MEMORY IS the dominant low-level communication paradigm in today’s mainstream multicore processors. In a shared-memory system, the (processor) cores communicate via loads and stores to a shared address space. The cores use caches to reduce the average memory latency and memory traffic. Caches are thus beneficial, but private caches lead to the possibility of cache incoherence. The mainstream solution is to provide shared memory and prevent incoherence through a hardware cache coherence protocol, making caches functionally invisible to software. The incoherence problem and basic hardware coherence solution are outlined in the sidebar, “The Problem of Incoherence,” page 86. Cache-coherent shared memory is provided by mainstream servers, desktops, laptops, and mobile devices and is available from all major vendors, including AMD, ARM, IBM, Intel, and Oracle (Sun).},
author = {Martin, Milo M. K. and Hill, Mark D. and Sorin, Daniel J.},
doi = {10.1145/2209249.2209269},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Martin, Hill, Sorin - 2012 - Why On-Chip Cache Coherence is Here to Stay.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = jul,
number = {7},
pages = {78},
title = {{Why On-Chip Cache Coherence is Here to Stay}},
volume = {55},
year = {2012}
}
@inproceedings{Castro2012,
author = {Castro, M\'{a}rcio and G\'{o}es, Luis F.W. and Fernandes, Luiz G. and M\'{e}haut, Jean-Fran\c{c}ois},
booktitle = {Euro-Par Parallel Processing},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Castro et al. - 2012 - Dynamic Thread Mapping Based on Machine Learning for Transactional Memory Applications.pdf:pdf},
keywords = {dynamic thread mapping,machine learning,transactional memory},
pages = {465--476},
title = {{Dynamic Thread Mapping Based on Machine Learning for Transactional Memory Applications}},
year = {2012}
}
@misc{PLPA2009,
author = {{The Open MPI project}},
booktitle = {2009},
title = {{Portable Linux Processor Affinity (PLPA)}},
url = {https://www.open-mpi.org/projects/plpa/}
}
@phdthesis{Majo2014,
author = {Maj\'{o}, Zolt\'{a}n},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Maj\'{o} - 2014 - Modeling Memory System Performance of NUMA Multicore-Multiprocessors.pdf:pdf},
title = {{Modeling Memory System Performance of NUMA Multicore-Multiprocessors}},
year = {2014}
}
@book{Buttlar1996,
author = {Buttlar, Dick and Farrell, Jacqueline},
title = {{Pthreads programming: A POSIX standard for better multiprocessing}},
year = {1996}
}
@phdthesis{Alves2014,
author = {Alves, Marco Antonio Zanata},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Alves - 2014 - Increasing Energy Efficiency of Processor Caches via Line Usage Predictors.pdf:pdf},
number = {March},
title = {{Increasing Energy Efficiency of Processor Caches via Line Usage Predictors}},
year = {2014}
}
@techreport{Intel2009,
author = {Intel},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Intel - 2009 - Intel ® Xeon ® Processor 5500 Series.pdf:pdf},
keywords = {nehalem},
mendeley-tags = {nehalem},
title = {{Intel® Xeon® Processor 5500 Series}},
url = {http://www.intel.com/assets/PDF/datasheet/318589.pdf},
year = {2009}
}
@incollection{Koppler1999,
abstract = {This paper introduces an infrastructure for parallel mesh computations running on distributed-memory computers. The infrastruc- ture consists of the mesh partitioning algorithm GARP and the domain- specific communication library GRAPHlib. Unlike existing algorithms, GARP exploits geometrical properties of the mesh shape in order to produce shape-adequate rectilinear partitions. The structure of such par- titions is exploited by GRAPHlib using an optimized message ordering strategy. We describe the concepts behind GARP and GRAPHlib and show that for meshes with particular shapes our infrastructure provides better utilization of the parallel computer than solutions using existing partitioning algorithms and communication libraries.},
author = {Koppler, Rainer},
booktitle = {Parallel Computation},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Koppler - 1999 - Geometry-Aided Rectilinear Partitioning of Unstructured Meshes.pdf:pdf},
keywords = {NP mapping},
mendeley-tags = {NP mapping},
pages = {450--459},
title = {{Geometry-Aided Rectilinear Partitioning of Unstructured Meshes}},
url = {http://www.springerlink.com/content/6wv5cd3wkbbu498p/},
year = {1999}
}
@inproceedings{Villa2008,
address = {New York, New York, USA},
author = {Villa, Oreste and Palermo, Gianluca and Silvano, Cristina},
booktitle = {Proceedings of the 2008 international conference on Compilers, architectures and synthesis for embedded systems - CASES '08},
doi = {10.1145/1450095.1450110},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Villa, Palermo, Silvano - 2008 - Efficiency and scalability of barrier synchronization on NoC based many-core architectures.pdf:pdf},
isbn = {9781605584690},
keywords = {barrier,efficiency,manycore,multicore,noc,scalability,synchronization},
pages = {81--90},
publisher = {ACM Press},
title = {{Efficiency and scalability of barrier synchronization on NoC based many-core architectures}},
url = {http://portal.acm.org/citation.cfm?doid=1450095.1450110},
year = {2008}
}
@techreport{McCalpin1995,
author = {McCalpin, John D.},
title = {{Sustainable Memory Bandwidth in Current High Performance Computers}},
url = {http://www.cs.virginia.edu/~mccalpin/papers/bandwidth/bandwidth.html},
year = {1995}
}
@inproceedings{Johnson2012,
author = {Johnson, Matthew and McCraw, Heike and Moore, Shirley and Mucci, Phil and Nelson, John and Terpstra, Dan and Weaver, Vince and Mohan, Tushar},
booktitle = {International Conference on Parallel Processing Workshops (ICPPW)},
doi = {10.1109/ICPPW.2012.29},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - 2012 - PAPI-V Performance Monitoring for Virtual Machines.pdf:pdf},
isbn = {978-1-4673-2509-7},
keywords = {analsis,due to lack of,performance,performance counters,performance monitoring,support for counter,typically not been available,virutal machines},
month = sep,
pages = {194--199},
title = {{PAPI-V: Performance Monitoring for Virtual Machines}},
year = {2012}
}
@inproceedings{Liu2014,
abstract = {Almost all of today’s microprocessors contain memory con- trollers and directly attach to memory. Modern multiproces- sor systems support non-uniform memory access (NUMA): it is faster for a microprocessor to access memory that is directly attached than it is to access memory attached to an- other processor.Without careful distribution of computation and data, a multithreaded program running on such a sys- tem may have high average memory access latency. To use multiprocessor systems efficiently, programmers need per- formance tools to guide the design of NUMA-aware codes. To address this need, we enhanced the HPCToolkit perfor- mance tools to support measurement and analysis of per- formance problems on multiprocessor systems with mul- tiple NUMA domains. With these extensions, HPCToolkit helps pinpoint, quantify, and analyze NUMA bottlenecks in executions of multithreaded programs. It computes derived metrics to assess the severity of bottlenecks, analyzes mem- ory accesses, and provides a wealth of information to guide NUMA optimization, including information about how to distribute data to reduce access latency and minimize con- tention. This paper describes the design and implementation of our extensions to HPCToolkit.We demonstrate their util- ity by describing case studies in which we use these capabil- ities to diagnose NUMA bottlenecks in four multithreaded applications.},
author = {Liu, Xu and Mellor-Crummey, John},
booktitle = {ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
doi = {10.1145/2555243.2555271},
isbn = {9781450326568},
pages = {259--272},
publisher = {ACM Press},
title = {{A tool to analyze the performance of multithreaded programs on NUMA architectures}},
year = {2014}
}
@inproceedings{Suh2002,
abstract = {We propose a low overhead, online memory monitoring scheme utilizing a set of novel hardware counters. The counters indicate the marginal gain in cache hits as the size of the cache is increased, which gives the cache miss-rate as a function of cache size. Using the counters, we describe a scheme that enables an accurate estimate of the isolated miss-rates of each process as a function of cache size under the standard LRU replacement policy. This information can be used to schedule jobs or to partition the cache to minimize the overall miss-rate. The data collected by the monitors can also be used by an analytical model of cache and memory behavior to produce a more accurate overall miss-rate for the collection of processes sharing a cache in both time and space. This overall miss-rate can be used to improve scheduling and partitioning schemes.},
author = {Suh, G. Edward and Devadas, Srinivas and Rudolph, Larry},
booktitle = {International Symposium on High-Performance Computer Architecture},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Suh, Devadas, Rudolph - 2002 - A new memory monitoring scheme for memory-aware scheduling and partitioning.pdf:pdf},
pages = {117--128},
publisher = {IEEE},
title = {{A new memory monitoring scheme for memory-aware scheduling and partitioning}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=995703},
year = {2002}
}
@article{Zhai2011,
abstract = {Communication patterns of parallel applications are important to optimize application performance and design better communication subsystems. Communication patterns can be extracted from communication traces. However, existing approaches to generate communication traces need to execute the entire parallel applications on full-scale systems that are time consuming and expensive. We propose a novel technique, called Fact, which can perform FAst Communication Traces collection for large-scale parallel applications on small-scale systems. Our idea is to reduce the original program to obtain a program slice through static analysis, and to execute the program slice to acquire the communication traces. Our idea is based on an observation that most computation and message contents in parallel applications are not relevant to their spatial and volume communication attributes, and therefore can be removed for the purpose of communication trace collection. We have implemented Fact and evaluated it with NPB programs and Sweep3D. The results show that Fact can reduce resource consumptions by two orders of magnitude in most cases.},
author = {Zhai, Jidong and Sheng, Tianwei and He, Jiangzhou},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Zhai, Sheng, He - 2011 - Efficiently Acquiring Communication Traces for Large-Scale Parallel Applications.pdf:pdf},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {communication pattern,mpi,nas},
mendeley-tags = {communication pattern,mpi,nas},
number = {11},
pages = {1862--1870},
title = {{Efficiently Acquiring Communication Traces for Large-Scale Parallel Applications}},
volume = {22},
year = {2011}
}
@techreport{Asanovic2006,
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David A and Yelick, Katherine A},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
pages = {56},
title = {{The Landscape of Parallel Computing Research: A View from Berkeley}},
year = {2006}
}
@inproceedings{Karypis1996,
author = {Karypis, George and Kumar, V},
booktitle = {International Parallel Processing Symposium (IPPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Karypis, Kumar - 1996 - Parallel Multilevel Graph Partitioning.pdf:pdf},
keywords = {parmetis},
mendeley-tags = {parmetis},
title = {{Parallel Multilevel Graph Partitioning}},
year = {1996}
}
@article{Drake2003,
abstract = {We present a linear time approximation algorithm with a performance ratio of 1/2 for finding a maximum weight matching in an arbitrary graph. Such a result is already known and is due to Preis [STACS’99, Lecture Notes in Comput. Sci., Vol. 1563, 1999, pp. 259–269]. Our algorithm uses a new approach which is much simpler than the one given by Preis and needs no amortized analysis for its running time.},
author = {Drake, Doratha E and Hougardy, Stefan},
doi = {10.1016/S0020-0190(02)00393-9},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Drake, Hougardy - 2003 - A simple approximation algorithm for the weighted matching problem.pdf:pdf},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {analysis of algorithms,approximation algorithms,drake,edmonds,graph algorithms,hourgardy,mapping,matching,maximum weight matching},
mendeley-tags = {drake,edmonds,hourgardy,mapping,matching},
month = feb,
number = {4},
pages = {211--213},
title = {{A simple approximation algorithm for the weighted matching problem}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020019002003939},
volume = {85},
year = {2003}
}
@article{Lorenzo-Castillo2013,
author = {Lorenzo-Castillo, Juan a. and Pichel, Juan C. and Rivera, Francisco F. and Pena, Tom\'{a}s F. and Cabaleiro, Jos\'{e} C.},
doi = {10.1007/s11227-013-0872-4},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Lorenzo-Castillo et al. - 2013 - A flexible and dynamic page migration infrastructure based on hardware counters.pdf:pdf},
isbn = {1122701308724},
issn = {0920-8542},
journal = {The Journal of Supercomputing},
keywords = {a,c,cabaleiro,centro de investigaci\'{o}n en,citius,compostela,f,hardware counters,j,lorenzo-castillo,numa,page migration,pena,pichel,rivera,santiago de compostela,spain,t,tecnolox\'{\i}as da informaci\'{o}n,university of santiago de},
month = jan,
number = {2},
pages = {930--948},
title = {{A flexible and dynamic page migration infrastructure based on hardware counters}},
url = {http://link.springer.com/10.1007/s11227-013-0872-4},
volume = {65},
year = {2013}
}
@article{Mercier2009,
abstract = {This paper presents a method to efficiently place MPI pro- cesses on multicore machines. Since MPI implementations often feature efficient supports for both shared-memory and network communication, an adequate placement policy is a crucial step to improve applications performance. As a case study, we show the results obtained for several NAS computing kernels and explain how the policy influences overall performance. In particular, we found out that a policy merely increasing the intranode communication ratio is not enough and that cache utiliza- tion is also an influential factor. A more sophisticated policy (eg. one taking into account the architecture’s memory structure) is required to observe performance improvements.},
author = {Mercier, Guillaume and Clet-Ortega, J},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Mercier, Clet-Ortega - 2009 - Towards an efficient process placement policy for mpi applications in multicore environments.pdf:pdf},
isbn = {9783642037702},
journal = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
keywords = {message-passing,mpi,multicore architectures,process placement},
mendeley-tags = {mpi},
title = {{Towards an efficient process placement policy for mpi applications in multicore environments}},
year = {2009}
}
@inproceedings{McVoy1996,
author = {McVoy, LW and Staelin, Carl},
booktitle = {USENIX Annual Technical Conference},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/McVoy, Staelin - 1996 - lmbench Portable Tools for Performance Analysis.pdf:pdf},
pages = {23--38},
title = {{Lmbench: Portable Tools for Performance Analysis.}},
year = {1996}
}
@techreport{VanderWijngaart2003,
abstract = {We describe an extension of the NAS Parallel Benchmarks (NPB) suite that involves solving the application benchmarks LU, BT and SP on collections of loosely coupled discretization meshes. The solutions on the meshes are updated independently, but after each time step they exchange boundary value information. This strategy, which is common among structured- mesh production flow solver codes in use at NASA Ames and elsewhere, provides relatively easily exploitable coarse-grain parallelism between meshes. Since the individual application benchmarks also allow fine-grain parallelism themselves, this NPB extension, named NPBMulti- Zone (NPB-MZ), is a good candidate for testing hybrid and multi-level parallelization tools and strategies.},
author = {{Van der Wijngaart}, Rob F. and Jin, Haoqiang},
booktitle = {NASA Ames Research Center, Tech. Rep. NAS-03-010},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Van der Wijngaart, Jin - 2003 - NAS Parallel Benchmarks, Multi-Zone Versions.pdf:pdf},
title = {{NAS Parallel Benchmarks, Multi-Zone Versions}},
year = {2003}
}
@inproceedings{Che2009,
author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Lee, Sang-Ha and Skadron, Kevin},
booktitle = {IEEE International Symposium on Workload Characterization (IISWC)},
doi = {10.1109/IISWC.2009.5306797},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Che et al. - 2009 - Rodinia A benchmark suite for heterogeneous computing.pdf:pdf},
isbn = {978-1-4244-5156-2},
pages = {44--54},
title = {{Rodinia: A benchmark suite for heterogeneous computing}},
year = {2009}
}
@inproceedings{Pilla2012,
abstract = {Multi-core compute nodes with non-uniform mem- ory access (NUMA) are now a common architecture in the assembly of large-scale parallel machines. On these machines, in addition to the network communication costs, the memory access costs within a compute node are also asymmetric. Ignoring this can lead to an increase in the data movement costs. Therefore, to fully exploit the potential of these nodes and reduce data access costs, it becomes crucial to have a complete view of the machine topology (i.e. the compute node topology and the interconnection network among the nodes). Furthermore, the parallel application behavior has an important role in determining how to utilize the machine efficiently. In this paper, we propose a hierarchical load balancing approach to improve the performance of applica- tions on parallel multi-core systems. We introduce NUCOLB, a topology-aware load balancer that focuses on redistributing work while reducing communication costs among and within compute nodes. NUCOLB takes the asymmetric memory access costs present on NUMA multi-core compute nodes, the interconnection network overheads, and the application communication patterns into account in its balancing decisions. We have implemented NUCOLB using the CHARM++ parallel runtime system and evaluated its performance. Results show that our load balancer improves performance up to 20\% when compared to state-of-the- art load balancers on three different NUMA parallel machines.},
author = {Pilla, Laercio L. and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Mei, Chao and Bhatele, Abhinav and Navaux, Philippe O.a. and Broquedis, Francois and Mehaut, Jean-Francois and Kale, Laxmikant V.},
booktitle = {International Conference on Parallel Processing},
doi = {10.1109/ICPP.2012.9},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Pilla et al. - 2012 - A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems.pdf:pdf},
isbn = {978-1-4673-2508-0},
keywords = {-load balancing,cluster,mem-,multi-core,non-uniform memory access,ory affinity,topology},
month = sep,
pages = {118--127},
publisher = {Ieee},
title = {{A Hierarchical Approach for Load Balancing on Parallel Multi-core Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6337573},
year = {2012}
}
@article{Popek1973,
abstract = {Virtual machine systems have been implemented on a limited number of third generation computer systems, e.g. CP-67 on the IBM 360/67. From previous empirical studies, it is known that certain third generation computer systems, e.g. the DEC PDP-10, cannot support a virtual machine system. In this paper, model of a third-generation-like computer system is developed. Formal techniques are used to derive precise sufficient conditions to test whether such an architecture can support virtual machines.},
author = {Popek, Gerald J. and Goldberg, Robert P.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Popek, Goldberg - 1973 - Formal requirements for virtualizable third generation architectures.pdf:pdf},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
number = {4},
pages = {121},
title = {{Formal requirements for virtualizable third generation architectures}},
volume = {7},
year = {1973}
}
@article{Huh2005,
address = {New York, New York, USA},
author = {Huh, Jaehyuk and Kim, Changkyu and Shafi, Hazim and Zhang, Lixin and Burger, Doug and Keckler, Stephen W.},
doi = {10.1145/1088149.1088154},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Huh et al. - 2005 - A NUCA substrate for flexible CMP cache sharing.pdf:pdf},
isbn = {1595931678},
journal = {Proceedings of the 19th annual international conference on Supercomputing - ICS '05},
keywords = {cache sharing,chip-multiprocessor,non-,uniform cache architecture},
pages = {31},
publisher = {ACM Press},
title = {{A NUCA substrate for flexible CMP cache sharing}},
url = {http://portal.acm.org/citation.cfm?doid=1088149.1088154},
year = {2005}
}
@article{Wong2008,
abstract = {The Operating System scheduler is designed to allocate the CPU resources appropriately to all processes. The Linux Completely Fair Scheduler (CFS) design ensures fairness among tasks using the thread fair scheduling algorithm. This algorithm ensures allocation of resources based on the number of threads in the system and not within executing programs. This can lead to fairness issue in a multi-threaded environment as the Linux scheduler tends to favor programs with higher number of threads. We illustrate the issue of fairness through experimental evaluation thus exposing the weakness of the current allocation scheme where software developers could take advantage by spawning many additional threads in order to obtain more CPU resources. A novel algorithm is proposed as a solution towards achieving better fairness in the Linux scheduler. The algorithm is based on weight readjustment of the threads created in the same process to significantly reduce the unfair allocation of CPU resources in multi-threaded environments. The algorithm was implemented and evaluated. It demonstrated promising results towards solving the raised fairness issue. We conclude this paper highlighting the limitations of the proposed approach and the future work in the stated direction.},
author = {Wong, Chee Siang and Tan, Ian and Kumari, Rosalind Deena and Wey, Fun},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Wong et al. - 2008 - Towards achieving fairness in the Linux scheduler.pdf:pdf},
journal = {ACM SIGOPS Operating Systems Review},
month = jul,
number = {5},
pages = {34--43},
title = {{Towards achieving fairness in the Linux scheduler}},
volume = {42},
year = {2008}
}
@inproceedings{Cruz2012,
abstract = {The communication latency between the cores in multiprocessor architectures differs depending on the memory hierarchy and the interconnections. With the increase of the number of cores per chip and the number of threads per core, this difference between the communication latencies is increasing. Therefore, it is important to map the threads of parallel applications taking into account the communication between them. In parallel applications based on the shared memory paradigm, the communication is implicit and occurs through accesses to shared variables. For this reason, it is difficult to detect the communication pattern between the threads. Traditional approaches use simulation to monitor the memory accesses performed by the application, requiring modifications to the source code and drastically increasing the overhead. In this paper, we introduce a new light-weight mechanism to detect the communication pattern of threads using the Translation Lookaside Buffer (TLB). Our mechanism relies entirely on hardware features, which makes the thread map- ping transparent to the programmer and allows it to be performed dynamically by the operating system. Moreover, no time consuming task, such as simulation, is required. We evaluated our mechanism with the NAS Parallel Benchmarks (NPB) and achieved an accurate representation of the communication patterns. Using the detected communication patterns, we generated thread mappings using a heuristic method based on the Edmonds graph matching algorithm. Running the applications with these mappings resulted in performance improvements of up to 15.3\%, reducing the number of cache misses by up to 31.1\%.},
author = {Cruz, Eduardo H. M. and Diener, Matthias and Navaux, Philippe O. A.},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2012.56},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz, Diener, Navaux - 2012 - Using the Translation Lookaside Buffer to Map Threads in Parallel Applications Based on Shared Memory.pdf:pdf},
keywords = {cache misses,interconnec-,memory,parallel applications,shared,thread mapping,tions,tlb,translation lookaside buffer},
pages = {532--543},
title = {{Using the Translation Lookaside Buffer to Map Threads in Parallel Applications Based on Shared Memory}},
year = {2012}
}
@inproceedings{Cruz2015,
author = {Cruz, Eduardo H M and Diener, Matthias and Pilla, La\'{e}rcio L. and Navaux, Philippe O A},
booktitle = {International Conference on Parallel, Distributed, and Network-Based Processing (PDP)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz et al. - 2015 - An Efficient Algorithm for Communication-Based Task Mapping.pdf:pdf},
keywords = {communication,hardware topol-,memory hierarchy,ogy,task mapping},
pages = {207--214},
title = {{An Efficient Algorithm for Communication-Based Task Mapping}},
year = {2015}
}
@inproceedings{Dormanns1995,
abstract = {We consider the problem of mapping large scale FEM graphs to highly parallel distributed memory computers. Typically, these programs show a low-dimensional grid-like communication structure. We argue that conventional domain decomposition methods that are usually employed today are not well suited for future highly parallel computers as they do not take into account the interconnection structure of the parallel computer resulting in a large communication overhead. Therefore we propose a new mapping heuristic which performs both, partitioning of the solution domain and processor allocation in one integrated step. Our procedure is based on the ability of Kohonen neural networks to exploit topological similarities of an input space and a grid-like structured network: to complete a neighbourhood preserving mapping between the set of discretization points and the parallel computer},
author = {Dormanns, Marcus and Heiss, Hans-Ulrich},
booktitle = {Parallel and Distributed Processing, \ldots},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Dormanns, Heiss - 1995 - Partitioning and Mapping of Large FEM-graphs by Self-Organization.pdf:pdf},
pages = {227--235},
title = {{Partitioning and Mapping of Large FEM-graphs by Self-Organization}},
year = {1995}
}
@inproceedings{Long1989,
author = {Long, Douglas L. and Clarke, Lori A.},
booktitle = {International Conference on Software Engineering},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Long, Clarke - 1989 - Task Interaction Graphs for Concurrency Analysis.pdf:pdf},
keywords = {tig},
mendeley-tags = {tig},
pages = {44--52},
title = {{Task Interaction Graphs for Concurrency Analysis}},
year = {1989}
}
@inproceedings{Drosinos2004,
author = {Drosinos, Nikolaos and Koziris, Nectarios},
booktitle = {IEEE International Parallel \& Distributed Processing Symposium (IPDPS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Drosinos, Koziris - 2004 - Performance Comparison of Pure MPI vs Hybrid MPI-OpenMP Parallelization Models on SMP Clusters.pdf:pdf},
isbn = {0769521320},
keywords = {hybrid},
mendeley-tags = {hybrid},
number = {C},
title = {{Performance Comparison of Pure MPI vs Hybrid MPI-OpenMP Parallelization Models on SMP Clusters}},
year = {2004}
}
@inproceedings{Chodnekar1997,
abstract = {The interconnection network (ICN) is a vital component of a parallel machine and is often the limiting factor in the performance of several parallel applications. While ICN performance evaluation has been a widely researched topic, there have been very few studies that have used real applications to drive this research. In this paper we develop a framework for characterizing the communication properties of parallel applications. Message generation frequency, spatial distribution of messages and message length are the three attributes that quantify any communication. We develop a methodology to quantify these attributes, in particular the first two attributes. We employ two strategies, namely dynamic and static, in our methodology. In the former, the applications are executed on an execution-driven simulator called SPASM, while in the latter they are executed on a parallel machine, IBM SP2. We gather communication events from these executions and feed them to a 2-D mesh network simulator. The log of the network activity is then analyzed using a statistical analysis package (SAS) to find the message inter-arrival time distribution and spatial distribution via regression analysis. Five shared memory applications and two message passing applications are analyzed to quantify their communication workloads. It is shown that it is possible to express the message generation and spatial distribution of an application in terms of commonly used distributions. These distributions can be used in the analysis of ICNs for developing realistic performance models},
author = {Chodnekar, Sucheta and Srinivasan, Viji and Vaidya, Aniruddha S. and Sivasubramaniam, Anand and Das, Chita R.},
booktitle = {International Symposium on High Performance Computer Architecture (HPCA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chodnekar et al. - 1997 - Towards a communication characterization methodology for parallel applications.pdf:pdf},
title = {{Towards a communication characterization methodology for parallel applications}},
year = {1997}
}
@inproceedings{Kumar2004,
author = {Kumar, Rakesh and Tullsen, Dean M. and Ranganathan, Parthasarathy and Jouppi, Norman P. and Farkas, Keith I.},
booktitle = {Proceedings of the 31st International Symposium on Computer Architecture},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2004 - Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload Performance.pdf:pdf},
title = {{Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload Performance}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.89\&rep=rep1\&type=pdf},
year = {2004}
}
@inproceedings{Rabenseifner2009,
author = {Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
booktitle = {Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
doi = {10.1109/PDP.2009.43},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Rabenseifner, Hager, Jost - 2009 - Hybrid MPIOpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes.pdf:pdf},
isbn = {978-0-7695-3544-9},
keywords = {hybrid},
mendeley-tags = {hybrid},
number = {c},
pages = {427--436},
title = {{Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes}},
year = {2009}
}
@inproceedings{Antony2006,
abstract = {Modern shared memory multiprocessor systems commonly have non-uniform memory access (NUMA) with asymmetric memory bandwidth and latency characteristics. Operating systems now provide application programmer interfaces allowing the user to perform specific thread and memory placement. To date, however, there have been rel- atively few detailed assessments of the importance of memory/thread placement for complex applications. This paper outlines a framework for performing memory and thread placement experiments on Solaris and Linux. Thread binding and lo- cation specific memory allocation and its verification is discussed and contrasted. Using the framework, the performance characteristics of serial versions of lmbench, Stream and various BLAS libraries (ATLAS, GOTO, ACML on Opteron/Linux and Sunperf on Opteron, UltraSPARC/Solaris) are measured on two different hardware platforms (UltraSPARC/FirePlane and Opteron/HyperTransport). A simple model describing performance as a function of memory distribution is proposed and assessed for both the Opteron and UltraSPARC.},
author = {Antony, Joseph and Janes, P and Rendell, A},
booktitle = {High Performance Computing (HiPC)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Antony, Janes, Rendell - 2006 - Exploring Thread and Memory Placement on NUMA Architectures Solaris and Linux, UltraSPARCFirePlane and O.pdf:pdf},
pages = {338--352},
title = {{Exploring Thread and Memory Placement on NUMA Architectures: Solaris and Linux, UltraSPARC/FirePlane and Opteron/HyperTransport}},
year = {2006}
}
@inproceedings{Fazenda2006,
abstract = {Efective portability of production codes is a goal that has been pursued over decades with modest success. This work presents a technique that enhances efficiency of a single code fiagment (adveclion) qf a production weather forecast code on both a veclor machine and an IA-32-based machine. The technique combines vector instructions with cache reuse and small memory foolprinl, using vector length us the single performance parameler. Performance results compare favorably with current performance levels for production weather forecast codes.},
author = {Fazenda, Alvaro and Enari, Eduardo and Rodrigues, Luiz and Panetta, Jairo},
booktitle = {2006 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)},
doi = {10.1109/SBAC-PAD.2006.29},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Fazenda et al. - 2006 - Towards Production Code Effective Portability among Vector Machines and Microprocessor-Based Architectures.pdf:pdf},
isbn = {0-7695-2704-3},
issn = {1550-6533},
month = oct,
pages = {11--20},
publisher = {Ieee},
title = {{Towards Production Code Effective Portability among Vector Machines and Microprocessor-Based Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4032411},
year = {2006}
}
@inproceedings{paje,
author = {Augerat, P. and Martin, C. and Stein, B.},
booktitle = {Euromicro Workshop on Parallel, Distributed and Network-based Processing},
doi = {10.1109/EMPDP.2002.994255},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Augerat, Martin, Stein - 2002 - Scalable monitoring and configuration tools for grids and clusters.pdf:pdf},
isbn = {0-7695-1444-8},
pages = {147--153},
publisher = {IEEE Comput. Soc},
title = {{Scalable monitoring and configuration tools for grids and clusters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=994255},
year = {2002}
}
@inproceedings{Faraj2002,
author = {Faraj, AA and Yuan, X},
booktitle = {Parallel and Distributed Computing and Systems (PDCS)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Faraj, Yuan - 2002 - Communication characteristics in the NAS parallel benchmarks.pdf:pdf},
keywords = {characteristics,communication,compiled communication,nas parallel benchmarks},
title = {{Communication characteristics in the NAS parallel benchmarks}},
year = {2002}
}
@inproceedings{Agarwal2000,
author = {Agarwal, Vikas and Hrishikesh, M. S. and Keckler, Stephen W. and Burger, Doug},
booktitle = {International Symposium on Computer Architecture (ISCA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Agarwal et al. - 2000 - Clock rate versus IPC The end of the road for conventional microarchitectures.pdf:pdf},
keywords = {wire-delay},
mendeley-tags = {wire-delay},
title = {{Clock rate versus IPC: The end of the road for conventional microarchitectures}},
year = {2000}
}
@inproceedings{Wang2012,
abstract = {With the shift to chip multiprocessors, managing shared resources has become a critical issue in realizing their full potential. Previous research has shown that thread mapping is a powerful tool for resource management. However, the difficulty of simultaneously managing multiple hardware resources and the varying nature of the workloads have impeded the efficiency of thread mapping algorithms. To overcome the difficulties of simultaneously managing multiple resources with thread mapping, the interaction between various microarchitectural resources and thread characteristics must be well understood. This paper presents an in-depth analysis of PARSEC bench- marks running under different thread mappings to investigate the interaction of various thread mappings with microarchitectural resources including, L1 I/D-caches, I/D TLBs, L2 caches, hardware prefetchers, off-chip memory interconnects, branch predictors, memory disambiguation units and the cores. For each resource, the analysis provides guidelines for how to improve its utilization when mapping threads with different characteristics. We also analyze how the relative importance of the resources varies depending on the workloads. Our experiments show that when only memory resources are considered, thread mapping improves an applica- tion’s performance by as much as 14\% over the default Linux scheduler. In contrast, when both memory and processor resources are considered the mapping algorithm achieves performance improvements by as much as 28\%. Additionally, we demonstrate that thread mapping should consider L2 caches, prefetchers and off-chip memory interconnects as one resource, and we present a new metric called L2-misses-memory-latency-product (L2MP) for evaluating their aggregated performance impact.},
author = {Wang, Wei and Dey, Tanima and Mars, Jason and Tang, Lingjia and Davidson, Jack W and Soffa, Mary Lou},
booktitle = {IEEE International Symposium on Performance Analysis of Systems \& Software (ISPASS)},
doi = {10.1109/ISPASS.2012.6189222},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2012 - Performance Analysis of Thread Mappings with a Holistic View of the Hardware Resources.pdf:pdf},
isbn = {978-1-4673-1146-5},
keywords = {parsec},
mendeley-tags = {parsec},
title = {{Performance Analysis of Thread Mappings with a Holistic View of the Hardware Resources}},
year = {2012}
}
@inproceedings{Yu2011,
author = {Yu, Yulong and Wang, Yuxin and Guo, He and He, Xubin},
booktitle = {IEEE International Conference on Networking, Architecture, and Storage},
doi = {10.1109/NAS.2011.30},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2011 - Hybrid Co-scheduling Optimizations for Concurrent Applications in Virtualized Environments.pdf:pdf},
isbn = {978-1-4577-1172-5},
keywords = {-concurrent applications,boost,credit scheduler,hybrid co-scheduling,partial co-scheduling,virtualization},
month = jul,
pages = {20--29},
publisher = {Ieee},
title = {{Hybrid Co-scheduling Optimizations for Concurrent Applications in Virtualized Environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6005443},
year = {2011}
}
@article{Cruz2014,
author = {Cruz, Eduardo Henrique Molina and Diener, Matthias and Alves, Marco Antonio Zanata and Navaux, Philippe Olivier Alexandre},
doi = {10.1016/j.jpdc.2013.11.006},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Cruz et al. - 2014 - Dynamic thread mapping of shared memory applications by exploiting cache coherence protocols.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing (JPDC)},
keywords = {cache coherence protocols,communication pattern,computing,of parallel and distributed,parallel applications,preprint submitted to journal,shared memory,thread communication,thread mapping},
month = mar,
number = {3},
pages = {2215--2228},
title = {{Dynamic thread mapping of shared memory applications by exploiting cache coherence protocols}},
volume = {74},
year = {2014}
}
@inproceedings{Bolosky1989,
abstract = {Multiprocessors with non-uniform memory access times introduce the problem of placing data near the processes that use them, in order to improve performance. We have implemented an automatic page placement strat- egy in the Mach operating system on the IBM ACE multiprocessor workstation. Our experience indicates that even very simple automatic strategies can produce nearly optimal page placement. It also suggests that the greatest leverage for further performance improvement lies in reducing false sharing, which occurs when the same page contains objects that would best be placed in different memories.},
address = {New York},
author = {Bolosky, William J and Fitzgerald, Robert P and Scott, Michael L},
booktitle = {ACM Symposium on Operating Systems Principles},
doi = {10.1145/74850.74854},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Bolosky, Fitzgerald, Scott - 1989 - Simple But Effective Techniques for NUMA Memory Management.pdf:pdf},
publisher = {ACM},
title = {{Simple But Effective Techniques for NUMA Memory Management}},
url = {http://dl.acm.org/citation.cfm?id=74854},
year = {1989}
}
@article{Browne2000,
abstract = {The purpose of the PAPI project is to specify a standard application programming interface (API) for accessing hardware performance counters available on most modern microprocessors. These counters exist as a small set of registers that count events, which are occurrences of specific signals and states related to the processor’s function. Monitoring these events facilitates correlation between the structure of source/object code and the efficiency of the mapping of that code to the underlying architecture. This correlation has a variety of uses in performance analysis including hand tuning, compiler optimization, debugging, benchmarking, monitoring and performance modeling. In addition, it is hoped that this information will prove useful in the development of new compilation technology as well as in steering architectural development towards alleviating commonly occurring bottlenecks in high performance computing.},
author = {Browne, S. and Dongarra, J. and Garner, N. and Ho, G. and Mucci, P.},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Browne et al. - 2000 - A Portable Programming Interface for Performance Evaluation on Modern Processors.pdf:pdf},
journal = {International Journal of High Performance Computing Applications},
keywords = {papi},
mendeley-tags = {papi},
number = {3},
pages = {189--204},
title = {{A Portable Programming Interface for Performance Evaluation on Modern Processors}},
volume = {14},
year = {2000}
}
@inproceedings{Roloff2012,
abstract = {High-Performance Computing (HPC) in the cloud has reached the mainstream and is currently a hot topic in the research community and the industry. The attractiveness of cloud for HPC is the capability to run large applications on powerful, scalable hardware without needing to actually own or maintain this hardware. In this paper, we conduct a detailed comparison of HPC applications running on three cloud providers, Amazon EC2, Microsoft Azure and Rackspace. We analyze three important characteristics of HPC, deployment facilities, performance and cost efficiency and compare them to a cluster of machines. For the experiments, we used the well-known NAS parallel benchmarks as an example of general scientific HPC applications to examine the computational and communication performance. Our results show that HPC applications can run efficiently on the cloud. However, care must be taken when choosing the provider, as the differences between them are large. The best cloud provider depends on the type and behavior of the application, as well as the intended usage scenario. Furthermore, our results show that HPC in the cloud can have a higher performance and cost efficiency than a traditional cluster, up to 27\% and 41\%, respectively.},
author = {Roloff, Eduardo and Diener, Matthias and Carissimi, Alexandre and Navaux, Philippe O. A.},
booktitle = {IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Roloff et al. - 2012 - High Performance Computing in the Cloud Deployment, Performance and Cost Efficiency.pdf:pdf},
isbn = {9781467345101},
pages = {371--378},
title = {{High Performance Computing in the Cloud: Deployment, Performance and Cost Efficiency}},
year = {2012}
}
@inproceedings{DeMelo2010,
author = {de Melo, Arnaldo Carvalho},
booktitle = {Linux Kongress},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/de Melo - 2010 - The New Linux 'perf' Tools.pdf:pdf},
title = {{The New Linux 'perf' Tools}},
year = {2010}
}
@inproceedings{Luk2005,
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evalu- ation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to pro- vide easy-to-use, portable, transparent, and efficient instrumenta- tion. Instrumentation tools (called Pintools) are written in C/C++ using Pin’s rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level with- out the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different archi- tectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application’s original, unin- strumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses sev- eral techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instru- mentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin’s versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly avail- able for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium R V , and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 down- loads from its website.},
author = {Luk, CK and Cohn, Robert and Muth, Robert and Patil, Harish},
booktitle = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Luk et al. - 2005 - Pin Building Customized Program Analysis Tools with Dynamic Instrumentation.pdf:pdf},
keywords = {and eventually detach,collect profiles,dynamic com-,instrumentation,program analysis tools,strument it,the application},
pages = {190--200},
title = {{Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation}},
year = {2005}
}
@article{Borkar2011,
author = {Borkar, Shekhar and Chien, Andrew A.},
doi = {10.1145/1941487},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Borkar, Chien - 2011 - The Future of Microprocessors.pdf:pdf},
journal = {Communications of the ACM},
number = {5},
pages = {67--77},
title = {{The Future of Microprocessors}},
volume = {54},
year = {2011}
}
@techreport{Thoziyoor2008,
abstract = {CACTI 5.1 is a version of CACTI 5 fixing a number of small bugs in CACTI 5.0. CACTI 5 is the latest major revision of the CACTI tool for modeling the dynamic power, access time, area, and leakage power of caches and other memories. CACTI 5 includes a number of major improvements over CACTI 4. First, as fabrication technologies enter the deep-submicron era, device and process parameter scaling has become non-linear. To better model this, the base technology modeling in CACTI 5 has been changed from simple linear scaling of the original CACTI 0.8 micron technology to models based on the ITRS roadmap. Second, embedded DRAM technology has become available from some vendors, and there is interest in 3D stacking of commodity DRAM with modern chip multiprocessors. As another major enhancement, CACTI 5 adds modeling support of DRAM memories. Third, to support the significant technology modeling changes above and to enable fair comparisons of SRAM and DRAM technology, the CACTI code base has been extensively rewritten to become more modular. At the same time, various circuit assumptions have been updated to be more relevant to modern design practice. Finally, numerous bug fixes and small feature additions have been made. For example, the cache organization assumed by CACTI is now output graphically to assist users in understanding the output generated by CACTI.},
author = {Thoziyoor, Shyamkumar and Muralimanohar, Naveen and Ahn, Jung Ho and Jouppi, Norman P and Alto, Palo},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Thoziyoor et al. - 2008 - Cacti 5.1.pdf:pdf},
keywords = {DRAM,access time,area,cache,memory,power},
title = {{Cacti 5.1}},
year = {2008}
}
@inproceedings{Li2010,
author = {Li, Yong and Abousamra, Ahmed and Melhem, Rami and Jones, Alex K},
booktitle = {Parallel Architectures and Compilation Techniques (PACT)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2010 - Compiler-assisted Data Distribution for Chip Multiprocessors Categories and Subject Descriptors.pdf:pdf},
keywords = {compiler-assisted caching,data distribution,partitioning},
pages = {501--512},
title = {{Compiler-assisted Data Distribution for Chip Multiprocessors Categories and Subject Descriptors}},
year = {2010}
}
@article{Chandra1994,
abstract = {Several cache-coherent shared-memory multiprocessors have been developed that are scalable and offer a very tight coupling between the processing resources. They are therefore quite attractive for use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management, however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of OS scheduling and page migration policies on the performance of such compute servers. Our experiments are done on the Stanford DASH, a distributed-memory cache-coherent multiprocessor. We show that for our multiprogramming workloads consisting of sequential jobs, the traditional Unix scheduling policy does very poorly. In contrast, a policy incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to two-fold performance improvement. For our workloads consisting of multiple parallel applications, we compare space-sharing policies that divide the processors among the applications to time-slicing policies such as standard Unix or gang scheduling. We show that space-sharing policies can achieve better processor utilization due to the operating point effect, but time-slicing policies benefit strongly from user-level data distribution. Our initial experience with automatic page migration suggests that policies based only on TLB miss information can be quite effective, and useful for addressing the data distribution problems of space-sharing schedulers.},
author = {Chandra, Rohit and Devine, Scott and Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
doi = {10.1145/381792.195485},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Chandra et al. - 1994 - Scheduling and page migration for multiprocessor compute servers.pdf:pdf},
isbn = {0-89791-660-3},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
number = {5},
pages = {12--24},
title = {{Scheduling and page migration for multiprocessor compute servers}},
volume = {28},
year = {1994}
}
@inproceedings{Weyers2014,
abstract = {The available memory bandwidth of existing high performance computing platforms turns out as being more and more the limitation to various applications. Therefore, modern microarchitectures integrate the memory controller on the processor chip, which leads to a non-uniform memory access behavior of such systems. This access behavior in turn entails major chal- lenges in the development of shared memory parallel applications. An improperly implemented memory access functionality results in a bad ratio between local and remote memory access, and causes low performance on such architectures. To address this problem, the developers of such applications rely on tools to make these kinds of performance problems visible. This work presents a new tool for the visualization of performance data of the non-uniform memory access behavior. Because of the visual design of the tool, the developer is able to judge the severity of remote memory access in a time-dependent simulation, which is currently not possible using existing tools.},
author = {Weyers, Benjamin and Terboven, Christian and Schmidl, Dirk and Herber, Joachim and Kuhlen, Torsten W and M\"{u}ller, S Matthias and Hentschel, Bernd},
booktitle = {Workshop on Visual Performance Analysis (VPA)},
file = {:Users/mdiener/Library/Application Support/Mendeley Desktop/Downloaded/Weyers et al. - 2014 - Visualization of Memory Access Behavior on Hierarchical NUMA Architectures.pdf:pdf},
pages = {42--49},
title = {{Visualization of Memory Access Behavior on Hierarchical NUMA Architectures}},
year = {2014}
}
