%!TEX encoding=UTF-8 Unicode
\section{Expriments}
\label{sec:expe}
In this section, we show how to analyse and optimize NUMA memory behaviour using
\TABARNAC~ through two benchmarks. Finally we discuss our tool overhead.
\subsection{Setup}
\label{sec:expe-setup}
machines used, becnhmarks, tools, mechanism used \ldots

\DB{fill the tabular}
\begin{table}
    \centering
    \resizebox{\linewidth}{!}
    {
        \begin{tabular}{l|c|c|l|c|c|l}
            Machine & \#Numa nodes & \#Core & CPU & Cache (Mib) & Mem (GiB) & OS\\
            \hline
            Turing & 4 & 64 & ?? & ?? & ?? &  ubuntu ??\\
            \hline
            Naskapi & ?? &?? &?? &?? & ?? &debian ??\\
        \end{tabular}
    }
    \caption{Experimental machine description}
    \label{tab:machines}
\end{table}

\subsection{Analysis}
\label{sec:expe-analysis}

\subsubsection{IS}
The first benchmark we studied is \emph{IS} from the  NAS Parallel benchmarks OpenMp
\cite{Feng04Unstructured}. This applications computes a bucket sort using
mainly three arrays.

\begin{figure}[htb]
    \centering

    \subfigure[\texttt{key\_buff2}]{
        \includegraphics[height=.27\textheight]  {is_w_kb2_orig}
        \label{fig:is-behaviour-orig-kb2}
    }
    \subfigure[\texttt{key\_array}]{
        \includegraphics[height=.27\textheight]  {is_w_kba_orig}
        \label{fig:is-behaviour-orig-kba}
    }
    \subfigure[\texttt{key\_buff1}]{
        \includegraphics[height=.27\textheight]  {is_w_kb1_orig}
        \label{fig:is-behaviour-orig-kb1}
    }
    \caption{Original memory access distribution for the main structure of
        \emph{IS} with $4$ threads.}
    \label{fig:is-behaviour-orig}
\end{figure}

Figure \ref{fig:is-behaviour-orig} shows the access distributions for the
three main structures of \emph{IS} class W with $4$ threads. We can see that
each of these structure have a different access pattern: \texttt{key\_array}
(fig \ref{fig:is-behaviour-orig-kba}) access distribution shows that every
threads works on a different part of the structures which allows automated
tools to do efficient data/thread mapping on it. However \texttt{key\_buff2}
(fig \ref{fig:is-behaviour-orig-kb2}) is completely shared by every threads,
but the most interesting access distribution is the one of \texttt{key\_buff1}
(fig \ref{fig:is-behaviour-orig-kb1}). Indeed the access repartition seems to
follow a nice Gaussian, which means a few pages are more used than all the
others. Finding a good NUMA balance with such a distribution is difficult and
almost impossible for automated tools.


\lstinputlisting[caption=\emph{IS} code responsible for the
Gaussain distribution of access, label=lst:is]{code/is.c}

Using this knowledge, we can look at \emph{IS} code and identify the source of the
Gaussian pattern, indeed all the access to \texttt{key\_buff1} are linear
excepts the one shown in listing \ref{lst:is}\footnote{
    The code have been slightly modified to make it more readable. In the
    original version the arrays \texttt{key\_buff1} (resp \texttt{key\_buff2})
    are accessed via a generic pointer called \texttt{key\_buff\_ptr} (resp
    \texttt{key\_buff\_ptr2}). More over some comments have been removed as
    they are not necessary here.
}  line \ref{lst:is-gaus}-\ref{lst:is-gaus-end} which depends on the values of
\texttt{key\_buff2}. The comments above the OpenMp loop explains that the
cyclic distribution will result in an unbalanced work distribution. Still we can easily design a cyclic
distribution aware of the Gaussian pattern which provides both a good
distribution of access among the thread and a strong locality. The idea is to
split the loop in two half and give one part of each half to each threads in a
round robin way. We can do that only by modifying line \ref{lst:is-cyclic} as
shown in listing \ref{lst:is-modif}.
\begin{lstlisting}[caption=One line optimization for \emph{IS}, label=lst:is-modif]
#pragma omp for schedule(static,NUM_BUCKETS/(2*omp_get_max_threads()))
\end{lstlisting}

\begin{figure}[htb]
    \centering

    \subfigure[\texttt{key\_buff2}]{
        \includegraphics[height=.27\textheight] {is_w_kb2_modif}
        \label{fig:is-behaviour-modif-kb2}
    }
    \subfigure[\texttt{key\_array}]{
        \includegraphics[height=.27\textheight] {is_w_kba_modif}
        \label{fig:is-behaviour-modif-kba}
    }

    \subfigure[\texttt{key\_buff1}]{
        \includegraphics[height=.27\textheight] {is_w_kb1_modif}
        \label{fig:is-behaviour-modif-kb1}
    }
    \caption{Memory access distribution for the main structure of
        \emph{IS} with $4$ threads after our modifications.}
    \label{fig:is-behaviour-modif}
\end{figure}

With this extremely simple code modification we obtain the access distribution
shown in figure \ref{fig:is-behaviour-modif}. We can see that the Gaussian
access of \texttt{key\_buff1} is now distributed over the threads. Each page
of this structure is almost used by only one thread. Moreover
\texttt{key\_buff2} access distribution have also changed, we can see that
each thread use mostly one part of the array.

The main point of our code modification is to improve the affinity between
thread and memory, therefore we need to pin each thread on a core to keep them
near to their data. To do so we use \texttt{GOMP\_CPU\_AFFINITY}. \TABARNAC~
also shows us that the first touch is always done by the thread actually using
the data for IS, therefore we do not need to map the data on NUMA nodes.
\DB{need to put a plot ?}

We then compare the execution time of \emph{IS.D} for the three scheduling
methods \emph{Dynamic}, \emph{Cyclic} with a step of $1$ and \TABARNAC:
cyclic with the distribution proposed. For each method we compare the
execution time on the bare operation system \emph{OS}, the execution time with
interleave policy \emph{interleave} and with thread affinity \emph{affinity}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.9\linewidth]{is_exectime.pdf}
    \caption{Speedup for \emph{IS} in class D compared to the default}
\label{fig:is-res}
\end{figure}

Figure\ref{fig:is-res} show the speedup of \emph{IS} in class D compared to
the default version (\emph{dynamic}) for each scheduling method, and for each
optimization technique. The first thing to notice is that with the
\emph{dynamic} scheduling, both \emph{interleave} and \emph{affinity} slows
the application down. Indeed simple optimization policy are hardly efficient
for non NUMA conscious code.

The cyclic policy, proposed in the original code already provides $13\%$
speedup, while in the developers clearly explain that it should be slower than
dynamic. We can see that if interleave is not suitable for this method, adding
some affinity provide one more percent of speedup. Indeed with the cyclic
distribution, if all threads are still using all the structure
\texttt{key\_buff1}, each thread have a more regular pattern than with the
dynamic distribution resulting on a better memory bandwith.

The \TABARNAC~ version provides already $17\%$ speedup with the
\emph{OS} version. This version only improves data locality without providing
any sort of mapping, still memory and caches are designed to be more efficient
for such accesses. For this scheduling method, the \emph{interleave} policy is
irrelevant as each thread works on contiguous chunks of each structures.
Finally, adding some affinity to keep thread as close as possible to their
data allows us to get more than $20\%$ speedup.

\DB{small cncl: 1line opti + 1 line aff => $20\% $ speedup}

\subsubsection{Stream Cluster}
same methodo as above

